<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-03-15T19:08:24+08:00</updated><id>http://localhost:4000/</id><title type="html">Leslie</title><subtitle>離於人境 獨面暗埜 惡龍新生</subtitle><author><name>Leslie</name></author><entry><title type="html"></title><link href="http://localhost:4000/2018/03/15/2017-10-01-%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86(BD)/" rel="alternate" type="text/html" title="" /><published>2018-03-15T19:08:24+08:00</published><updated>2018-03-15T19:08:24+08:00</updated><id>http://localhost:4000/2018/03/15/2017-10-01-%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86(BD)</id><content type="html" xml:base="http://localhost:4000/2018/03/15/2017-10-01-%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86(BD)/">&lt;h2 id=&quot;海量数据处理&quot;&gt;海量数据处理&lt;/h2&gt;

&lt;p&gt;所谓海量数据，就是数据量太大，要么在短时间内无法计算出结果，要么数据太大，无法一次性装入内存。&lt;/p&gt;

&lt;p&gt;针对时间，我们可以使用巧妙的算法搭配合适的数据结构，如bitmap/堆/trie树等&lt;br /&gt;
针对空间，就一个办法，大而化小，分而治之。常采用hash映射&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hash映射/分而治之&lt;/li&gt;
  &lt;li&gt;Bitmap&lt;/li&gt;
  &lt;li&gt;Bloom filter(布隆过滤器)&lt;/li&gt;
  &lt;li&gt;双层桶划分&lt;/li&gt;
  &lt;li&gt;Trie树&lt;/li&gt;
  &lt;li&gt;数据库索引&lt;/li&gt;
  &lt;li&gt;倒排索引(Inverted Index)&lt;/li&gt;
  &lt;li&gt;外排序&lt;/li&gt;
  &lt;li&gt;simhash算法&lt;/li&gt;
  &lt;li&gt;分布处理之Mapreduce&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;估算&quot;&gt;估算&lt;/h3&gt;

&lt;p&gt;在处理海量问题之前，我们往往要先估算下数据量，能否一次性载入内存？如果不能，应该用什么方式拆分成小块以后映射进内存？每次拆分的大小多少合适？以及在不同方案下，大概需要的内存空间和计算时间。&lt;/p&gt;

&lt;p&gt;比如,我们来了解下以下常见问题&lt;code class=&quot;highlighter-rouge&quot;&gt;时间&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;空间&lt;/code&gt; 估算 :&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;8位的电话号码，最多有99 999 999个
IP地址
1G内存，2^32 ,差不多40亿，40亿Byte*8 = 320亿 bit

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;海量处理问题常用的分析解决问题的思路是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;分而治之/Hash映射 + hash统计/trie树/红黑树/二叉搜索树 + 堆排序/快速排序/归并排序&lt;/li&gt;
  &lt;li&gt;双层桶划分&lt;/li&gt;
  &lt;li&gt;Bloom filter 、Bitmap&lt;/li&gt;
  &lt;li&gt;Trie树/数据库/倒排索引&lt;/li&gt;
  &lt;li&gt;外排序&lt;/li&gt;
  &lt;li&gt;分布处理之 Hadoop/Mapreduce&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Leslie</name></author></entry><entry><title type="html"></title><link href="http://localhost:4000/2018/03/15/2017-10-01-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95/" rel="alternate" type="text/html" title="" /><published>2018-03-15T19:08:24+08:00</published><updated>2018-03-15T19:08:24+08:00</updated><id>http://localhost:4000/2018/03/15/2017-10-01-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95</id><content type="html" xml:base="http://localhost:4000/2018/03/15/2017-10-01-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95/">&lt;h2 id=&quot;数据库索引&quot;&gt;数据库索引&lt;/h2&gt;

&lt;p&gt;索引使用的数据结构多是B树或B+树。B树和B+树广泛应用于文件存储系统和数据库系统中，mysql使用的是B+树，oracle使用的是B树，Mysql也支持多种索引类型，如b-tree 索引，哈希索引，全文索引等。&lt;/p&gt;

&lt;p&gt;一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。&lt;/p&gt;

&lt;h3 id=&quot;磁盘数据查找过程&quot;&gt;磁盘数据查找过程&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\assets\images\disk_search.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;盘面：每一个盘片都有2个上下盘面，每个盘面都可以存储数据&lt;/p&gt;

&lt;p&gt;柱面：所有盘面上的同一磁道构成一个圆柱，叫做柱面。磁盘读写按柱面进行;
只在同一柱面所有的磁头全部读/写完毕后磁头才转移到下一柱面，因为选取磁头只需通过电子切换即可，而选取柱面则必须通过机械切换。电子切换相当快，比在机械上磁头向邻近磁道移动快得多，所以，数据的读/写按柱面进行，而不按盘面进行。也就是说，一个磁道写满数据后，就在同一柱面的下一个盘面来写，一个柱面写满后，才移到下一个扇区开始写数据。读数据也按照这种方式进行，这样就提高了硬盘的读/写效率。&lt;/p&gt;

&lt;p&gt;磁道：磁盘在格式化时被划分出许多同心圆，这些同心圆轨迹叫做磁道 track。磁道从外向内从0开始编号。&lt;/p&gt;

&lt;p&gt;扇区：信息以脉冲串的形式记录在这些轨迹中，这些同心圆不是连续记录数据，而是被划分成一段段圆弧，每段圆弧叫做一个扇区，扇区从“1”开始编号 。扇区也叫块号。&lt;/p&gt;

&lt;p&gt;磁盘在物理上划分为柱面, 磁道，扇区。想要读取扇区的数据，需要将磁头放到这个扇区上方:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;先找到柱面，也就是寻道。磁头是不能动的，但可以沿着磁盘半径方向运动，耗时记为寻道事件 t(seek)&lt;/li&gt;
  &lt;li&gt;将目标扇区旋转到磁头下，这个过程耗时是旋转时间t(r)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一个磁盘扇区数据读取的时间t = t(seek)+t(r)+t(数据传输) , 在数据库查找数据时，查找时间与访问的磁盘盘块成正比，内存处理时间可以忽略不计。&lt;/p&gt;

&lt;h3 id=&quot;b树&quot;&gt;B树&lt;/h3&gt;

&lt;p&gt;2-3树：一个节点最多有2个key，红黑树就是2-3树的一种实现。&lt;/p&gt;

&lt;p&gt;B树又叫多路平衡查找树。B树可以看做是对2-3树的扩展，允许每个节点有M-1个key，并以升序排列，这里的M就是B树的阶。&lt;/p&gt;

&lt;p&gt;B树的度d(d&amp;gt;=2) ，有一些特征：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;根节点至少有2个子节点&lt;/li&gt;
  &lt;li&gt;所有的叶节点具有相同的深度 h，也就是树高&lt;/li&gt;
  &lt;li&gt;每个叶子节点至少包含一个key和2个指针，最多2d-1个key和2d个指针，叶节点的指针都是null。每个节点的关键字个数在【d-1,2d-1】之间&lt;/li&gt;
  &lt;li&gt;每个非叶子节点，key和指针互相间隔，节点两端是指针，因此节点中指针个数=key的个数+1&lt;/li&gt;
  &lt;li&gt;每个指针要么是null，要么指向另一个节点&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果某个指针在节点node最左边且不为null，则其指向节点的所有key小于v(key1)，其中v(key1)为node的第一个key的值。
如果某个指针在节点node最右边且不为null，则其指向节点的所有key大于v(keym)，其中v(keym)为node的最后一个key的值。
如果某个指针在节点node的左右相邻key分别是keyi和keyi+1且不为null，则其指向节点的所有key小于v(keyi+1)且大于v(keyi)。&lt;/p&gt;

&lt;p&gt;使用数据结构表示如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;typedef struct Item{
     int key;
     Data data;
}

#define m 3 //B树的阶

typedef struct BTNode{
    int degree; //B树的度
    int keynums; //每个节点key的个数
     Item  items[m];
     struct BTNode *p[m];
}BTNode,* BTree;

typedef struct{
     BTNode *pt; //指向找到的节点
     int i; // 节点中关键字的序号 (0,m-1)
     int tag; //1:查找成功，0：查找失败
}Result;

Status btree_insert(root,target);
Status btree_delete(root,target);
Result btree_find(root,target);

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;建立索引&quot;&gt;建立索引&lt;/h3&gt;

&lt;p&gt;当为一张空表创建索引时，数据库系统将为你分配一个索引页，该索引页在你插入数据前一直是空的。此页此时既是根结点，也是叶结点。每当你往表中插入一行数据，数据库系统即向此根结点中插入一行索引记录。&lt;/p&gt;

&lt;p&gt;插入和删除新的数据记录都会破坏B-Tree的性质，因此在插入删除时，需要对树进行一个分裂、合并、转移等操作以保持B-Tree性质&lt;/p&gt;

&lt;h3 id=&quot;查找操作&quot;&gt;查找操作&lt;/h3&gt;

&lt;p&gt;从root节点出发，对每个节点，找到等于target的key，则查找成功；或者找到大于target的最小k[i], 到 k[i] 左指针指向的子节点继续查找，直到页节点，如果找不到，说明关键字target不在B树中。&lt;/p&gt;

&lt;p&gt;分析下时间复杂度：&lt;/p&gt;

&lt;p&gt;对于一个度为d的B-Tree,每个节点的索引key个数是d-1, 索引key个数为N，树高h上限是：&lt;/p&gt;

&lt;p&gt;2d^h-1=N ==&amp;gt; h=logd^((N+1) /2) ？？？&lt;/p&gt;

&lt;p&gt;因此，检索一个key，查找节点的个数的复杂度是O(logd^N)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;比如d=2，N=1,000,000 (1百万)，h差不多20个
d=3,N=1,000,000 (1百万) ,h差不多13个(3^11=1,594,323)
d=4,N=1,000,000 (1百万) ,h差不多10个
d=5,N=1,000,000 (1百万) ,h差不多9个 (5^9 = 1,953,125)
d=6,N=1,000,000 (1百万) ,h差不多8个(6^8 = 1,679,616)
d=7,N=1,000,000 (1百万) ,h差不多8个
d=8,N=1,000,000 (1百万) ,h差不多7个
d=9,N=1,000,000 (1百万) ,h差不多7个
d=10,N=1,000,000 (1百万) ,h差不多6个
d=100时，h差不多3个
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;数据库系统在设计时，通常将一个节点的大小设为一个页大小(通常4k)，这样保证一个节点在物理上也存储在一个页里，加上计算机存储分配都是按页对其，这样保证一个节点只需要一次I/O.&lt;/p&gt;

&lt;p&gt;实际应用中，d都是比较大，通常超过100，因此1百万的数据通常最多访问3个节点，也就是3次I/O, 因此使用B树作为索引结构查询效率非常高。&lt;/p&gt;

&lt;h3 id=&quot;插入数据&quot;&gt;插入数据&lt;/h3&gt;

&lt;p&gt;插入数据时，需要更新索引，索引中也要添加一条记录。索引中添加一条记录的过程是：&lt;/p&gt;

&lt;p&gt;沿着搜索的路径从root一直到叶节点&lt;/p&gt;

&lt;p&gt;每个节点的关键字个数在【d-1,2d-1】之间，当节点的关键字个数是2t-1时，再加入target就违反了B树定义，需要对该节点进行分裂：已中间节点为界，分成2个包含d-1个关键字的子节点（另外还有一个分界关键字，2*(d-1)+1=2d-1），同时把该分界关键字提升到该叶子的父节点中，如果这导致父节点关键字个数超过2d-1,就继续向上分裂，直到根节点。&lt;/p&gt;

&lt;p&gt;如下演示动画，往度d=2的B树中插入：&lt;code class=&quot;highlighter-rouge&quot;&gt; 6 10 4 14 5 11 15 3 2 12 1 7 8 8 6 3 6 21 5 15 15 6 32 23 45 65 7 8 6 5 4&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\images\btree_insert.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;b树和b树的区别&quot;&gt;B树和B+树的区别&lt;/h3&gt;

&lt;p&gt;B树和B+树的区别在于：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;B+树的非叶子节点只包含导航信息，不包含实际记录的信息，这可以保证一个固定大小节点可以放入更多个关键字，也就是更大的度d，从而树高h可以更小，从而相比B树有更优秀的查询效率&lt;/li&gt;
  &lt;li&gt;所有的叶子节点和相邻的节点使用链表方式相连，便于区间查找和遍历&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Leslie</name></author></entry><entry><title type="html"></title><link href="http://localhost:4000/2018/03/15/2017-10-01-%E5%A4%96%E6%8E%92%E5%BA%8F/" rel="alternate" type="text/html" title="" /><published>2018-03-15T19:08:24+08:00</published><updated>2018-03-15T19:08:24+08:00</updated><id>http://localhost:4000/2018/03/15/2017-10-01-%E5%A4%96%E6%8E%92%E5%BA%8F</id><content type="html" xml:base="http://localhost:4000/2018/03/15/2017-10-01-%E5%A4%96%E6%8E%92%E5%BA%8F/">&lt;p&gt;对磁盘文件的排序。将待处理的数据不能一次装入内存，先读入部分数据排序后输出到临时文件，采用「排序-归并」的策略。在归并阶段将这些临时文件组合为一个大的有序文件，也即排序结果。&lt;/p&gt;

&lt;h2 id=&quot;外排序&quot;&gt;外排序&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;多路归并&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;最小堆&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;比如，要对900 MB的数据进行排序，但机器上只有100 MB的可用内存时，外归并排序按如下方法操作：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;读入100 MB的数据至内存中，用某种常规方式（如快速排序、堆排序、归并排序等方法）在内存中完成排序。&lt;/li&gt;
  &lt;li&gt;将排序完成的数据写入磁盘。&lt;/li&gt;
  &lt;li&gt;重复步骤1和2直到所有的数据都存入了不同的100 MB的块（临时文件）中。在这个例子中，有900 MB数据，单个临时文件大小为100 MB，所以会产生9个临时文件。&lt;/li&gt;
  &lt;li&gt;读入每个临时文件（顺串）的前10 MB（ = 100 MB / (9块 + 1)）的数据放入内存中的输入缓冲区，最后的10 MB作为输出缓冲区。（实践中，将输入缓冲适当调小，而适当增大输出缓冲区能获得更好的效果。）&lt;/li&gt;
  &lt;li&gt;执行&lt;code class=&quot;highlighter-rouge&quot;&gt;九路归并&lt;/code&gt;算法，将结果输出到输出缓冲区。一旦输出缓冲区满，将缓冲区中的数据写出至目标文件，清空缓冲区。一旦9个输入缓冲区中的一个变空，就从这个缓冲区关联的文件，读入下一个10M数据，除非这个文件已读完。这是“外归并排序”能在主存外完成排序的关键步骤 – 因为“归并算法”(merge algorithm)对每一个大块只是顺序地做一轮访问(进行归并)，每个大块不用完全载入主存。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;为了增加每一个有序的临时文件的长度，可以采用&lt;code class=&quot;highlighter-rouge&quot;&gt;置换选择排序&lt;/code&gt;（Replacement selection sorting）。它可以产生大于内存大小的顺串。具体方法是在内存中使用一个&lt;code class=&quot;highlighter-rouge&quot;&gt;最小堆&lt;/code&gt;进行排序，设该最小堆的大小为M。算法描述如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;初始时将输入文件读入内存，建立最小堆。&lt;/li&gt;
  &lt;li&gt;将堆顶元素输出至输出缓冲区。然后读入下一个记录：
    &lt;ul&gt;
      &lt;li&gt;若该元素的关键码值不小于刚输出的关键码值，将其作为堆顶元素并调整堆，使之满足堆的性质；&lt;/li&gt;
      &lt;li&gt;否则将新元素放入堆底位置，将堆的大小减1。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;重复第2步，直至堆大小变为0。&lt;/li&gt;
  &lt;li&gt;此时一个顺串已经产生。将堆中的所有元素建堆，开始生成下一个顺串。[3]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;此方法能生成平均长度为2M的顺串，可以进一步减少访问外部存储器的次数，节约时间，提高算法效率。&lt;/p&gt;</content><author><name>Leslie</name></author></entry><entry><title type="html"></title><link href="http://localhost:4000/2018/03/15/2017-10-01-%E5%8F%8C%E5%B1%82%E6%A1%B6%E5%88%92%E5%88%86/" rel="alternate" type="text/html" title="" /><published>2018-03-15T19:08:24+08:00</published><updated>2018-03-15T19:08:24+08:00</updated><id>http://localhost:4000/2018/03/15/2017-10-01-%E5%8F%8C%E5%B1%82%E6%A1%B6%E5%88%92%E5%88%86</id><content type="html" xml:base="http://localhost:4000/2018/03/15/2017-10-01-%E5%8F%8C%E5%B1%82%E6%A1%B6%E5%88%92%E5%88%86/">&lt;p&gt;双层桶不是一种数据结构，只是一种算法思维。分而治之思想。&lt;/p&gt;
&lt;h2 id=&quot;双层桶划分&quot;&gt;双层桶划分&lt;/h2&gt;

&lt;p&gt;当我们有一大推数据需要处理时，局限于各种资源限制(主要说内存)不能一次处理完成，这是需要将一大堆数据分成多个小段数据。通过处理各个小段数据完成最终任务。&lt;/p&gt;

&lt;p&gt;双层这里是虚指，并不是一定把数据分成2份，也可能多份。比如下面几个问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。&lt;/li&gt;
  &lt;li&gt;5亿个int找它们的中位数&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;第一个问题，2.5亿(2^32=4,294,967,296)个数,我们将这2^32个数分到2^8=256个区域(文件中)。每个文件中的平均数字个数差不多 2^24个(1千7百万个)。
0~2^24 第一个文件，2^24~2^25第二个文件&lt;/p&gt;

&lt;p&gt;假设32位机，装下这些数字需要的内存是 2^24*4=2^26=64MB,也可以不用将文件一次性读入内存而是采用流式读取。&lt;/p&gt;

&lt;p&gt;然后对每个文件使用bitmap处理，每2bit(2-bitmap)表示一个整数，00表示整数未出现，01表示出现一次，10表示出现两次及其以上。这样，每个文件2^24个数字，最大数2^32/(8/2)=2^30=1GB内存&lt;/p&gt;

&lt;p&gt;这个问题倒是更新是bitmap的应用，没有很好体现双层桶分治的优势。&lt;/p&gt;

&lt;p&gt;第二个问题，首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。&lt;/p&gt;

&lt;p&gt;适用问题领域是：&lt;code class=&quot;highlighter-rouge&quot;&gt;top-k，中位数，不重复或重复的数字&lt;/code&gt;&lt;/p&gt;</content><author><name>Leslie</name></author></entry><entry><title type="html"></title><link href="http://localhost:4000/2018/03/15/2017-10-01-%E5%88%86%E5%B8%83%E5%A4%84%E7%90%86%E4%B9%8BMapreduce/" rel="alternate" type="text/html" title="" /><published>2018-03-15T19:08:24+08:00</published><updated>2018-03-15T19:08:24+08:00</updated><id>http://localhost:4000/2018/03/15/2017-10-01-%E5%88%86%E5%B8%83%E5%A4%84%E7%90%86%E4%B9%8BMapreduce</id><content type="html" xml:base="http://localhost:4000/2018/03/15/2017-10-01-%E5%88%86%E5%B8%83%E5%A4%84%E7%90%86%E4%B9%8BMapreduce/">&lt;h3 id=&quot;分布处理之mapreduce&quot;&gt;分布处理之Mapreduce&lt;/h3&gt;

&lt;p&gt;MapReduce是Google提出的一个软件架构，用于大规模数据集（大于1TB）的并行运算。概念“Map（映射）”和“Reduce（归纳）”，及他们的主要思想，都是从函数式编程语言借来的，还有从矢量编程语言借来的特性。MapReduce的伟大之处就在于让不熟悉并行编程的程序员也能充分发挥分布式系统的威力。&lt;/p&gt;

&lt;h5 id=&quot;mapreduce工作原理&quot;&gt;Mapreduce工作原理&lt;/h5&gt;

&lt;p&gt;举一个例子：10年内所有论文(当然有很多很多篇)里面出现最多的几个单词。&lt;/p&gt;

&lt;p&gt;我们把论文集分层N份，一台机器跑一个作业。这个方法跑得快，但是有部署成本，需要把程序copy到别的机器，要把论文分N份，且还需要最后把N个运行结果整合起来。这其实就是Mapreduce本质。&lt;/p&gt;

&lt;p&gt;map函数和reduce函数是交给用户实现的，这两个函数定义了任务本身。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;map函数：接受一个键值对（key-value pair），产生一组中间键值对。MapReduce框架会将map函数产生的中间键值对里键相同的值传递给一个reduce函数。Map操作是可以高度并行的。&lt;/li&gt;
  &lt;li&gt;reduce函数：接受一个键，以及相关的一组值，将这组值进行合并产生一组规模更小的值（通常只有一个或零个值）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;hadoop&quot;&gt;Hadoop&lt;/h5&gt;

&lt;p&gt;谷歌技术有”三宝”，GFS、MapReduce和大表（BigTable）。&lt;/p&gt;

&lt;p&gt;Hadoop实际上就是谷歌三宝的开源实现，Hadoop MapReduce对应Google MapReduce，HBase对应BigTable，HDFS对应GFS。HDFS（或GFS）为上层提供高效的非结构化存储服务，HBase（或BigTable）是提供结构化数据服务的分布式数据库，Hadoop MapReduce（或Google MapReduce）是一种并行计算的编程模型，用于作业调度。&lt;/p&gt;

&lt;p&gt;Hadoop 使用java实现。&lt;/p&gt;</content><author><name>Leslie</name></author></entry><entry><title type="html"></title><link href="http://localhost:4000/2018/03/15/2017-10-01-%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95(Inverted-Index)/" rel="alternate" type="text/html" title="" /><published>2018-03-15T19:08:24+08:00</published><updated>2018-03-15T19:08:24+08:00</updated><id>http://localhost:4000/2018/03/15/2017-10-01-%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95(Inverted%20Index)</id><content type="html" xml:base="http://localhost:4000/2018/03/15/2017-10-01-%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95(Inverted-Index)/">&lt;p&gt;也叫反向索引。是文档检索系统中最常用的数据结构。&lt;/p&gt;
&lt;h2 id=&quot;倒排索引inverted-index&quot;&gt;倒排索引(Inverted Index)&lt;/h2&gt;

&lt;p&gt;常规的索引是文档到关键词的映射，如果对应的文档是&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/elastic/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt;就是使用倒排索引(inverted index)的结构来做快速的全文搜索。ElasticSearch 不仅用于全文搜索, 还有非常强大的统计功能 (facets)。&lt;/p&gt;

&lt;p&gt;携程，58，美团的分享中都提到ES构建实时日志系统，帮助定位系统问题。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://es.xiaoleilu.com/index.html&quot;&gt;Elasticsearch权威指南&lt;/a&gt;&lt;/p&gt;</content><author><name>Leslie</name></author></entry><entry><title type="html"></title><link href="http://localhost:4000/2018/03/15/2017-10-01-simhash%E7%AE%97%E6%B3%95/" rel="alternate" type="text/html" title="" /><published>2018-03-15T19:08:24+08:00</published><updated>2018-03-15T19:08:24+08:00</updated><id>http://localhost:4000/2018/03/15/2017-10-01-simhash%E7%AE%97%E6%B3%95</id><content type="html" xml:base="http://localhost:4000/2018/03/15/2017-10-01-simhash%E7%AE%97%E6%B3%95/">&lt;p&gt;simhash是google用来处理海量文本去重的算法。&lt;/p&gt;

&lt;h2 id=&quot;simhash算法&quot;&gt;simhash算法&lt;/h2&gt;

&lt;p&gt;1、分词，把需要判断文本分词形成这个文章的特征单词。最后形成去掉噪音词的单词序列并为每个词加上权重，我们假设权重分为5个级别（1~5）。比如：“ 美国“51区”雇员称内部有9架飞碟，曾看见灰色外星人 ” ==&amp;gt; 分词后为 “ 美国（4） 51区（5） 雇员（3） 称（1） 内部（2） 有（1） 9架（3） 飞碟（5） 曾（1） 看见（3） 灰色（4） 外星人（5）”，括号里是代表单词在整个句子里重要程度，数字越大越重要。&lt;/p&gt;

&lt;p&gt;2、hash，通过hash算法把每个词变成hash值，比如“美国”通过hash算法计算为 100101,“51区”通过hash算法计算为 101011。这样我们的字符串就变成了一串串数字，还记得文章开头说过的吗，要把文章变为数字计算才能提高相似度计算性能，现在是降维过程进行时。&lt;/p&gt;

&lt;p&gt;3、加权，通过 2步骤的hash生成结果，需要按照单词的权重形成加权数字串，比如“美国”的hash值为“100101”，通过加权计算为“4 -4 -4 4 -4 4”；“51区”的hash值为“101011”，通过加权计算为 “ 5 -5 5 -5 5 5”。&lt;/p&gt;

&lt;p&gt;4、合并，把上面各个单词算出来的序列值累加，变成只有一个序列串。比如 “美国”的 “4 -4 -4 4 -4 4”，“51区”的 “ 5 -5 5 -5 5 5”， 把每一位进行累加， “4+5 -4+-5 -4+5 4+-5 -4+5 4+5” ==》 “9 -9 1 -1 1 9”。这里作为示例只算了两个单词的，真实计算需要把所有单词的序列串累加。&lt;/p&gt;

&lt;p&gt;5、降维，把4步算出来的 “9 -9 1 -1 1 9” 变成 0 1 串，形成我们最终的simhash签名。 如果每一位大于0 记为 1，小于0 记为 0。最后算出结果为：“1 0 1 0 1 1”&lt;/p&gt;

&lt;p&gt;simhash计算过程图
&lt;img src=&quot;http://www.lanceyan.com/wp-content/uploads/2013/08/simhas&quot; alt=&quot;simhash&quot; /&gt;&lt;/p&gt;

&lt;p&gt;大家可能会有疑问，经过这么多步骤搞这么麻烦，不就是为了得到个 0 1 字符串吗？我直接把这个文本作为字符串输入，用hash函数生成 0 1 值更简单。其实不是这样的，传统hash函数解决的是生成唯一值，比如 md5、hashmap等。md5是用于生成唯一签名串，只要稍微多加一个字符md5的两个数字看起来相差甚远；hashmap也是用于键值对查找，便于快速插入和查找的数据结构。不过我们主要解决的是文本相似度计算，要比较的是两个文章是否相识，当然我们降维生成了hashcode也是用于这个目的。看到这里估计大家就明白了，我们使用的simhash就算把文章中的字符串变成 01 串也还是可以用于计算相似度的，而传统的hashcode却不行。我们可以来做个测试，两个相差只有一个字符的文本串，“你妈妈喊你回家吃饭哦，回家罗回家罗” 和 “你妈妈叫你回家吃饭啦，回家罗回家罗”。&lt;/p&gt;

&lt;p&gt;通过simhash计算结果为：&lt;/p&gt;

&lt;p&gt;1000010010101101111111100000101011010001001111100001001011001011&lt;/p&gt;

&lt;p&gt;1000010010101101011111100000101011010001001111100001101010001011&lt;/p&gt;

&lt;p&gt;通过 hashcode计算为：&lt;/p&gt;

&lt;p&gt;1111111111111111111111111111111110001000001100110100111011011110&lt;/p&gt;

&lt;p&gt;1010010001111111110010110011101&lt;/p&gt;

&lt;p&gt;大家可以看得出来，相似的文本只有部分 01 串变化了，而普通的hashcode却不能做到，这个就是局部敏感哈希的魅力。&lt;/p&gt;</content><author><name>Leslie</name></author></entry><entry><title type="html"></title><link href="http://localhost:4000/2018/03/15/2017-10-01-Hash%E6%98%A0%E5%B0%84,%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B/" rel="alternate" type="text/html" title="" /><published>2018-03-15T19:08:24+08:00</published><updated>2018-03-15T19:08:24+08:00</updated><id>http://localhost:4000/2018/03/15/2017-10-01-Hash%E6%98%A0%E5%B0%84,%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B</id><content type="html" xml:base="http://localhost:4000/2018/03/15/2017-10-01-Hash%E6%98%A0%E5%B0%84,%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B/">&lt;p&gt;这里的&lt;code class=&quot;highlighter-rouge&quot;&gt;Hash映射&lt;/code&gt;是指通过一种映射散列的方式，将海量数据均匀分布在对应的内存或更小的文件中&lt;/p&gt;

&lt;h2 id=&quot;hash映射分而治之&quot;&gt;Hash映射,分而治之&lt;/h2&gt;

&lt;p&gt;使用hash映射有个最重要的特点是: &lt;code class=&quot;highlighter-rouge&quot;&gt;hash值相同的两个串不一定一样，但是两个一样的字符串hash值一定相等&lt;/code&gt;。哈希函数如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int hash = 0;
for (int i=0;i&amp;lt;s.length();i++){
	hash = (R*hash +s.charAt(i)%M);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;大文件映射成多个小文件。具体操作是，比如要拆分到100(M)个文件：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;对大文件中的每条记录求hash值，然后对M取余数，即 &lt;code class=&quot;highlighter-rouge&quot;&gt;hash(R)%M&lt;/code&gt;，结果为K&lt;/li&gt;
  &lt;li&gt;将记录R按结果K分配到第K个文件，从而完成数据拆分&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这样，两条相同的记录肯定会被分配到同一个文件。&lt;/p&gt;</content><author><name>Leslie</name></author></entry><entry><title type="html"></title><link href="http://localhost:4000/2018/03/15/2017-10-01-Bloom-filter(%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8)/" rel="alternate" type="text/html" title="" /><published>2018-03-15T19:08:24+08:00</published><updated>2018-03-15T19:08:24+08:00</updated><id>http://localhost:4000/2018/03/15/2017-10-01-Bloom%20filter(%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8)</id><content type="html" xml:base="http://localhost:4000/2018/03/15/2017-10-01-Bloom-filter(%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8)/">&lt;p&gt;Bloom Filter是由Bloom在1970年提出的一种多哈希函数映射的快速查找算法。通常应用在一些需要快速判断某个元素是否属于集合，但是并不严格要求100%正确的场合。&lt;/p&gt;

&lt;h2 id=&quot;bloom-filter布隆过滤器&quot;&gt;Bloom filter(布隆过滤器)&lt;/h2&gt;

&lt;h3 id=&quot;bloom-filter-特点&quot;&gt;Bloom filter 特点&lt;/h3&gt;

&lt;p&gt;为了说明Bloom Filter存在的重要意义，举一个实例：假设要你写一个网络蜘蛛（web crawler）。由于网络间的链接错综复杂，蜘蛛在网络间爬行很可能会形成“环”。为了避免形成“环”，就需要知道蜘蛛已经访问过那些URL。给一个URL，怎样知道蜘蛛是否已经访问过呢？稍微想想，就会有如下几种方案：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;将访问过的URL保存到数据库。&lt;/li&gt;
  &lt;li&gt;用HashSet将访问过的URL保存起来。那只需接近O(1)的代价就可以查到一个URL是否被访问过了。&lt;/li&gt;
  &lt;li&gt;URL经过MD5或SHA-1等单向哈希后再保存到HashSet或数据库。&lt;/li&gt;
  &lt;li&gt;BitMap方法。建立一个BitSet，将每个URL经过一个哈希函数映射到某一位。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;方法1~3都是将访问过的URL完整保存，方法4则只标记URL的一个映射位。以上方法在数据量较小的情况下都能完美解决问题，但是当数据量变得非常庞大时问题就来了。&lt;/p&gt;

&lt;p&gt;方法1的缺点：数据量变得非常庞大后关系型数据库查询的效率会变得很低。而且每来一个URL就启动一次数据库查询是不是太小题大做了？&lt;br /&gt;
方法2的缺点：太消耗内存。随着URL的增多，占用的内存会越来越多。就算只有1亿个URL，每个URL只算50个字符，就需要5GB内存。&lt;br /&gt;
方法3：由于字符串经过MD5处理后的信息摘要长度只有128Bit，SHA-1处理后也只有160Bit，因此方法3比方法2节省了好几倍的内存。&lt;br /&gt;
方法4消耗内存是相对较少的，但缺点是单一哈希函数发生冲突的概率太高。还记得数据结构课上学过的Hash表冲突的各种解决方法么？若要降低冲突发生的概率到1%，就要将BitSet的长度设置为URL个数的100倍。&lt;/p&gt;

&lt;p&gt;实质上上面的算法都忽略了一个重要的隐含条件：允许小概率的出错，不一定要100%准确！也就是说少量url实际上没有没网络蜘蛛访问，而将它们错判为已访问的代价是很小的——大不了少抓几个网页呗。&lt;/p&gt;

&lt;h3 id=&quot;bloom-filter-算法&quot;&gt;Bloom filter 算法&lt;/h3&gt;

&lt;p&gt;Bloom filter可以看做是对bitmap的扩展。只是使用多个hash映射函数，从而减低hash发生冲突的概率。算法如下:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;创建 m 位的bitset，初始化为0， 选中k个不同的哈希函数&lt;/li&gt;
  &lt;li&gt;第 i 个hash 函数对字符串str 哈希的结果记为 h(i,str) ,范围是（0，m-1）&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将字符串记录到bitset的过程：对于一个字符串str,分别记录h(1,str),h(2,str)…,h(k,str)。 然后将bitset的h(1,str),h(2,str)…,h(k,str)位置1。也就是将一个str映射到bitset的 k 个二进制位。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;检查字符串是否存在:对于字符串str，分别计算h(1，str)、h(2，str),…,h(k，str)。然后检查BitSet的第h(1，str)、h(2，str),…,h(k，str) 位是否为1，若其中任何一位不为1则可以判定str一定没有被记录过。若全部位都是1，则“认为”字符串str存在。但是若一个字符串对应的Bit全为1，实际上是不能100%的肯定该字符串被Bloom Filter记录过的。（因为有可能该字符串的所有位都刚好是被其他字符串所对应）这种将该字符串划分错的情况，称为false positive 。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;删除字符串:字符串加入了就被不能删除了，因为删除会影响到其他字符串。实在需要删除字符串的可以使用Counting bloomfilter(CBF)。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Bloom Filter 使用了k个哈希函数，每个字符串跟k个bit对应。从而降低了冲突的概率。&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;最优的哈希函数个数位数组m大小&quot;&gt;最优的哈希函数个数，位数组m大小&lt;/h3&gt;

&lt;p&gt;哈希函数的选择对性能的影响应该是很大的，一个好的哈希函数要能近似等概率的将字符串映射到各个Bit。选择k个不同的哈希函数比较麻烦，一种简单的方法是选择一个哈希函数，然后送入k个不同的参数。&lt;/p&gt;

&lt;p&gt;在原始个数位n时，那这里的k应该取多少呢？位数组m大小应该取多少呢？这里有个计算公式:&lt;code class=&quot;highlighter-rouge&quot;&gt;k=(ln2)*(m/n)&lt;/code&gt;, 当满足这个条件时，错误率最小。&lt;/p&gt;

&lt;p&gt;假设错误率为0.01， 此时m 大概是 n 的13倍，k大概是8个。 这里的n是元素记录的个数，m是bit位个数。如果每个元素的长度原大于13，使用Bloom Filter就可以节省内存。&lt;/p&gt;

&lt;h3 id=&quot;错误率估计&quot;&gt;错误率估计&lt;/h3&gt;

&lt;h3 id=&quot;实现示例&quot;&gt;实现示例&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#define SIZE 15*1024*1024
char a[SIZE]; /* 15MB*8 = 120M bit空间 */
memset(a,0,SIZE);

int seeds[] = { 5, 7, 11, 13, 31, 37, 61};

int hashcode(int cap,int seed, string key){
	int hash = 0;
	for (int i=0;i&amp;lt;key.length();i++){
		hash = (seed*hash +key.charAt(i));
	}
	return hash &amp;amp; (cap-1);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;对每个字符串str求哈希就可以使用 &lt;code class=&quot;highlighter-rouge&quot;&gt;hashcode(SIZE*8,seeds[i],str)&lt;/code&gt; ,i 的取值范围就是 （0，k）。&lt;/p&gt;

&lt;h2 id=&quot;bloom-filter应用&quot;&gt;Bloom filter应用&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;拼写检查一类的字典应用&lt;/li&gt;
  &lt;li&gt;数据库系统&lt;/li&gt;
  &lt;li&gt;网络领域（爬虫，web cache sharing）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;p&gt;http://www.cnblogs.com/heaad/archive/2011/01/02/1924195.html&lt;br /&gt;
http://blog.csdn.net/jiaomeng/article/details/1495500  &lt;br /&gt;
http://pages.cs.wisc.edu/~cao/papers/summary-cache/node8.html  &lt;code class=&quot;highlighter-rouge&quot;&gt;哈希函数个数k、位数组大小m&lt;/code&gt; 测试论证&lt;/p&gt;</content><author><name>Leslie</name></author></entry><entry><title type="html">Machine Learning</title><link href="http://localhost:4000/ml/2017/10/30/Machine-Learning/" rel="alternate" type="text/html" title="Machine Learning" /><published>2017-10-30T00:00:00+08:00</published><updated>2017-10-30T00:00:00+08:00</updated><id>http://localhost:4000/ml/2017/10/30/Machine%20Learning</id><content type="html" xml:base="http://localhost:4000/ml/2017/10/30/Machine-Learning/">&lt;p&gt;它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。&lt;/p&gt;

&lt;h2 id=&quot;数学是基础&quot;&gt;数学是基础&lt;/h2&gt;
&lt;p&gt;微积分
概率论和统计学
线性代数（矩阵，向量）
数值数学（数值分析，线性规划，凸优化理论，常见数值优化算法）
实分析和泛函的基础&lt;/p&gt;

&lt;p&gt;《统计学习方法》 李航博士&lt;/p&gt;

&lt;h2 id=&quot;机器学习&quot;&gt;机器学习&lt;/h2&gt;

&lt;p&gt;机器学习 机器学习是近20多年兴起的一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与统计推断学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。&lt;/p&gt;

&lt;p&gt;从大量数据中找到规律和知识，然后用规律和知识做预测和决策。&lt;/p&gt;

&lt;h3 id=&quot;监督学习&quot;&gt;监督学习&lt;/h3&gt;

&lt;p&gt;从给定的训练数据集中学习一个函数，当新数据到来时，可以根据这个函数预测结果。&lt;/p&gt;

&lt;p&gt;Square判断一笔交易是否是欺诈，信用卡咋骗
Airbnb 预测用户网络平台上的违规操作
Airbnb 搜索引擎，搜索结果最大程度可能是用户感兴趣的租房
Airbnb 预测一个房租的最佳定价范围&lt;/p&gt;

&lt;p&gt;线性回归
决策树&lt;/p&gt;

&lt;p&gt;Aerosolve
Scikit-Learn python&lt;/p&gt;

&lt;h3 id=&quot;无监督学习&quot;&gt;无监督学习&lt;/h3&gt;

&lt;p&gt;从一个输入集里选择识别隐藏的有用的信息，比如从生物信息的DNA里找到负责同一个生物功能的DNA群，图像图形处理里的人脸识别。研究方法是聚类分析。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数据处理与可视化：PCA，LDA，MDS&lt;/li&gt;
  &lt;li&gt;聚类算法&lt;/li&gt;
  &lt;li&gt;稀疏编码&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;增强学习&quot;&gt;增强学习&lt;/h3&gt;

&lt;p&gt;通过观察来学习应该如何的动作。&lt;/p&gt;

&lt;p&gt;机器学习偏向数学问题推导，数据挖掘就是抽特征
不要沉迷于数学公式推导，理解如何运用数据
动手实现一些简单的算法，如感知机，k近邻，线性回归
找一个实际案例，从他的算法选择，特征参数选取调整，以及数据管道的建立等系统的学习一下&lt;/p&gt;

&lt;p&gt;基本不会去实现这些基础算法，都有现成的开源工具&lt;/p&gt;

&lt;p&gt;概率图模型（Probabilistic graphical model）
两个核心的机器学习模型：Latent Dirichlet Allocation（LDA） Probabilistic Matrix Factorization（PMF）
统计计算（Statistical computing）	
深度学习（Deep learning）
优化（optimization）
PAC学习理论（PAC Learning）
非参数贝叶斯统计（Non-parametric Bayesian statistics）&lt;/p&gt;

&lt;p&gt;参考链接：http://www.zhihu.com/question/21714701&lt;/p&gt;

&lt;p&gt;分类器&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Naive Bayes&lt;/li&gt;
  &lt;li&gt;Linear Discriminant Analysis&lt;/li&gt;
  &lt;li&gt;Logistic Regression&lt;/li&gt;
  &lt;li&gt;Linear SVM&lt;/li&gt;
  &lt;li&gt;Kernel SVM&lt;/li&gt;
  &lt;li&gt;Adaboost&lt;/li&gt;
  &lt;li&gt;Decision&lt;/li&gt;
  &lt;li&gt;Neural network&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;数据挖掘&quot;&gt;数据挖掘&lt;/h3&gt;

&lt;p&gt;在知乎的描述中。&lt;code class=&quot;highlighter-rouge&quot;&gt;数据挖掘&lt;/code&gt;是指从大量的数据中自动搜索隐藏于其中的有着特殊关系性的信息和知识的过程。&lt;/p&gt;

&lt;h2 id=&quot;入门参考&quot;&gt;入门参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/&quot;&gt;Machine learning&lt;/a&gt; Andrew Ng 在 coursera公开课。最好能完成所有作业。课程讲义http://cs229.stanford.edu/materials.html&lt;/p&gt;

&lt;p&gt;网易的andrew ng公开课， 这个也不错，年代久远一些。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://julyedu.com/course/getDetail?course_id=34#discard&quot;&gt;julyedu 课程大纲&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;《机器学习基石》 https://www.coursera.org/course/ntumlone&lt;/p&gt;

&lt;p&gt;《机器学习技法》 公开课&lt;/p&gt;

&lt;p&gt;《机器学习实战》&lt;/p&gt;

&lt;p&gt;《机器学习：实用案例解析》 本书比较全面系统地介绍了机器学习的方法和技术。全书案例既有分类问题，也有回归问题；既包含监督学习，也涵盖无监督学习。本书讨论的案例从分类讲到回归，然后讨论了聚类、降维、最优化问题等。这些案例包括分类：垃圾邮件识别，排序：智能收件箱，回归模型：预测网页访问量，正则化：文本回归，最优化：密码破解，无监督学习：构建股票市场指数，空间相似度：用投票记录对美国参议员聚类，推荐系统：给用户推荐R语言包，社交网络分析：在Twitter上感兴趣的人，模型比较：给你的问题找到最佳算法。各章对原理的叙述力求概念清晰、表达准确，突出理论联系实际，富有启发性，易于理解。在探索这些案例的过程中用到的基本工具就是R统计编程语言。&lt;/p&gt;

&lt;p&gt;《Machine Learning》 Tom Mitchell 
Simon Haykin的《神经网络与机器学习》&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;第1课 微积分与概率论
Taylor展式、梯度下降和牛顿法初步、Jensen不等式、常见分布与共轭分布&lt;/p&gt;

&lt;p&gt;第2课 数理统计与参数估计
切比雪夫不等式、大数定理、中心极限定理、矩估计、极大似然估计&lt;/p&gt;

&lt;p&gt;第3课 矩阵和线性代数
特征向量、对称矩阵对角化、线性方程&lt;/p&gt;

&lt;p&gt;第4课 凸优化
凸集、凸函数、凸优化、KKT条件&lt;/p&gt;

&lt;p&gt;第5课 回归
最小二乘法、高斯分布、梯度下降、过拟合、Logistic回归
实践示例：线性回归、Logistic回归实现和分析&lt;/p&gt;

&lt;p&gt;第6课 梯度下降算法剖析
自适应学习率、拟牛顿、LBFGS
实践示例：自适应学习率代码实现和参数调试分析&lt;/p&gt;

&lt;p&gt;第7课 最大熵模型
熵、相对熵、信息增益、最大熵模型、IIS&lt;/p&gt;

&lt;p&gt;第8课 聚类
K-means/K-Medoid/密度聚类/谱聚类
实践示例：K-means、谱聚类代码实现和参数调试分析&lt;/p&gt;

&lt;p&gt;第9课 推荐系统
协同过滤、隐语义模型pLSA/SVD、随机游走Random Walk
实践示例：协同过滤代码实现和参数调试分析&lt;/p&gt;

&lt;p&gt;第10课 决策树和随机森林
ID3、C4.5、CART、Bagging、GBDT
实践案例：使用随机森林进行数据分类&lt;/p&gt;

&lt;p&gt;第11课 Adaboost
Adaboost、前向分步算法&lt;/p&gt;

&lt;p&gt;第12课 SVM
线性可分支持向量机、线性支持向量机、非线性支持向量机、SMO
实践案例: 使用SVM进行数据分类&lt;/p&gt;

&lt;p&gt;第13课 贝叶斯网络
朴素贝叶斯、有向分离、马尔科夫模型/HMM/pLSA&lt;/p&gt;

&lt;p&gt;第14课 EM算法
GMM、pLSA、HMM
实践案例：分解男女身高、图像分割&lt;/p&gt;

&lt;p&gt;第15课 主题模型
pLSA、共轭先验分布、LDA
实践案例：使用LDA进行文档聚类&lt;/p&gt;

&lt;p&gt;第16课 采样与变分
MCMC/KL(p||q)与KL(q||p)&lt;/p&gt;

&lt;p&gt;第17课 隐马尔科夫模型HMM
概率计算问题、参数学习问题、状态预测问题
实践案例：使用HMM进行中文分词&lt;/p&gt;

&lt;p&gt;第18课 条件随机场CRF
概率无向图模型、MRF、线性链CRF&lt;/p&gt;

&lt;p&gt;第19课 人工神经网络
BP算法、CNN、RNN&lt;/p&gt;

&lt;p&gt;第20次课 深度学习
实践案例：使用Torch进行图像分类及卷积网络可视化的深度学习实践&lt;/p&gt;

&lt;p&gt;预习&lt;/p&gt;

&lt;p&gt;《高等数学·上下册》；
《概率论与数理统计·浙大版》、《数理统计学简史·陈希孺》；
《矩阵分析与应用·张贤达》；
《凸优化(Convex Optimization) · Stephen Boyd &amp;amp; Lieven Vandenberghe著》；
《统计学习方法·李航》；
《Pattern Recognition And Machine Learning · Christopher M. Bishop著》，简称PRML；&lt;/p&gt;</content><author><name>Leslie</name></author><summary type="html">它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。</summary></entry></feed>
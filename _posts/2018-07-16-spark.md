---
title: Spark核心编程进阶（一）
categories:
- Spark
- Big Data
tags:
- Spark



---

## Spark核心编程进阶-Spark集群架构概览

spark集群原理概述

搞spark，首先第一点，就是说搭建好大数据集群，spark伪分布式集群（一个节点），master和worker都在一个节点上
你得用spark提供的api，编写大数据计算处理程序，java/scala/python，都可以
程序是针对一份儿，或者多份儿，hdfs、本地文件（很少）、hive、mysql中大数据，至少是千万级别以上的
去根据你的业务需要和需求，去进行计算和处理，最后得到想要的结果

这就是搞这个spark大数据

spark程序写完了以后，就要提交到spark集群上面去运行，这就是spark作业（一次代码的运行+一份数据的处理+一次结果的产出）

spark作业是通过spark集群中的多个独立的进程来并行运行的，每个进程都处理一部分数据，从而做到分布式并行计算，才能对大数据进行处理和计算
作业在多个进程中的运行，是通过SparkContext对象来居中调度的，该对象是在咱们的driver进程中的（包含main方法的程序进程）

SparkContext是支持连接多种集群管理器的（包括Spark Standalone、YARN、Mesos）
集群管理器是负责为SparkContext代表的spark application，在集群中分配资源的
这里说的资源是什么？通俗一点，就是分配多个进程，然后每个进程不都有一些cpu core和内存，有了进程、cpu和内存，你的spark作业才能运行啊

这里说的进程具体是什么呢？怎么工作的呢？

SparkContext会要求集群管理器来分配资源，然后集群管理器就会集群节点上，分配一个或多个executor进程
这些进程就会负责运行你自己写的spark作业代码，每个进程处理一部分数据
具体是怎么运行我们的代码的呢？申请到了executor进程之后，SparkContext会发送我们的工程jar包到executor上，这样，executor就有可以执行的代码了
接着SparkContext会将一些task分发到executor上，每个task执行具体的代码，并处理一小片数据
此外要注意的一点是，executor进程，会会负责存储你的spark作业代码计算出来的一些中间数据，或者是最终结果数据



大数据原理闲谈：你要处理大数据，一台机器，内存、cpu处理不了比如百亿级别的数据量，或者要3天3夜才能处理完，那就废了，你的大量数据，说白了，就是垃圾

大数据的技术才出现了，大数据分布式存储，一台机器可能都存储不下，1亿数据；我现在用100台机器，每台机器存储100万数据，那还是可以存储的下的；

然后是大数据计算，比如说spark，你不能用一台机器，放一个程序，吭哧吭哧去计算1亿数据，太慢了；有这么一种技术，能把你的代码分布到多台机器上面去跑，每台机器都运行相同的代码逻辑，但是处理的数据是不同的；相当于有100台机器同时在处理1亿数据，那么就可以大大加快大数据的计算能力和效率，可能30分钟就处理完了


### 第一步

我们写了一段程序，spark代码，里面肯定是有main方法的，比如说是java/scala，我们去运行这个代码，代码一旦跑起来，一定是运行在一个进程里面的

进程会去执行main方法中的代码

进程，程序跑起来的进程，通常来说就是jvm进程，java虚拟机进程，就是我们的driver进程，也就是driver，也可以叫Driver Program

executor进程会运行在多个节点上面

在某一个节点会运行我们的driver进程

sparkcontext，就是我们自己在代码里创建的
负责调度我们的整个spark作业的运行


### 第二步

spark集群管理器，目前来说，支持的有Spark Standalone、YARN、Mesos，但是国内常用的，也就只有Spark Standalone和YARN

SparkContext，会跟集群管理器去申请资源，申请启动一些executor进程

集群管理器主要就负责在各个节点上，给spark作业启动一批executor进程


### 第三步

启动executor进程


### 第四步
driver会将我们的工程jar发送到所有的executor进程中，这样呢，每个进程，就都有了可以执行的，我们写的spark代码

### 第五步

其实driver，sparkcontext对象，会根据我们写的spark代码，创建一批一批的task，最小计算单元；比如说，1000个task，100个executor进程，会将task分发到executor进程中去执行；每个executor进程执行10个task

每个task，都会执行本地jar包中的相同的一段代码，但是，每个task处理不同的数据

比如说，对于hdfs文件，每个task处理一个hdfs block，hdfs block会对应，rdd的一个partition，一个task就处理一个partition

就做到了大量task并行执行，每个task就处理一小片数据，速度很快

![](http://i2.51cto.com/images/blog/201810/03/19a7653500e528878318f3acc874cd89.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



## Spark核心编程进阶-Spark集群架构的几点特别说明

关于spark集群架构的一些说明

1、每个spark application，都有属于自己的executor进程；绝对不可能出现多个spark application共享一个executor进程的
	executor进程，在整个spark application运行的生命周期内，都会一直存在，不会自己消失的
	executor进程，最主要的，就是使用多线程的方式，运行SparkContext分配过来的task，来一批task就执行一批，一批执行完了，再换下一批task执行
	
2、spark application，跟集群管理器之间，是透明的
	不管你是哪个集群管理器，我就知道，我找你就可以申请到executor进程就好了
	所以说，就算在一个能够容纳其他类型的计算作业的集群管理器中，也可以提交spark作业，比如说YARN、Mesos这种
	大公司里，其实一般都是用YARN作为大数据计算作业管理器的，包括mapreduce、hive、storm和spark，统一在yarn上面运行，统一调度和管理公司的系统资源
	
3、driver（其实也就是咱们的main类运行的jvm进程），必须时刻监听着属于它这个spark application的executor进程发来的通信和连接
	而且driver除了监听，自己也得负责调度整个spark作业（你自己写的spark代码）的调度和运行，也得大量跟executor进程通信，给他们分派计算任务
	所以driver在网络环境中的位置，还是很重要的，driver尽量离spark集群得近一些
	
4、上面说了，driver要调度task给executor执行，所以driver最好和spark集群在一片网络内


    1、spark作业跟executor进程之间的关系
    2、spark作业跟cluster manager之间的关系
    3、driver进程跟executor进程之间的关系
    4、driver一定要跟executor所在的集群举例要近

![](http://i2.51cto.com/images/blog/201810/03/d23e6a438ac0d2502c011c0d976eaabc.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


## Spark核心编程进阶-Spark的核心术语讲解


一些重要的spark术语

    Application			spark应用程序，说白了，就是用户基于spark api开发的程序，一定是通过一个有main方法的类执行的，比如java开发spark，就是在eclipse中，建立的一个工程
    
    Application Jar		这个就是把写好的spark工程，打包成一个jar包，其中包括了所有的第三方jar依赖包，
    比如java中，就用maven+assembly插件打包最方便
    
    Driver Program		说白了，就是运行程序中main方法的进程，这就是driver，也叫driver进程
    
    Cluster Manager		集群管理器，就是为每个spark application，在集群中调度和分配资源的组件，
    比如Spark Standalone、YARN、Mesos等
    
    Deploy Mode			部署模式，无论是基于哪种集群管理器，spark作业部署或者运行模式，都分为两种，client和cluster，
    client模式下driver运行在提交spark作业的机器上；cluster模式下，运行在spark集群中
    
    Worker Node			集群中的工作节点，能够运行executor进程，运行作业代码的节点
    
    Executor			集群管理器为application分配的进程，运行在worker节点上，负责执行作业的任务，
    并将数据保存在内存或磁盘中，每个application都有自己的executor
    
    Job					每个spark application，根据你执行了多少次action操作，就会有多少个job
    
    Stage				每个job都会划分为多个stage（阶段），每个stage都会有对应的一批task，分配到executor上去执行
    
    Task				driver发送到executor上执行的计算单元，每个task负责在一个阶段（stage），
    处理一小片数据，计算出对应的结果


deploy mode，分为两种

    1、client模式：主要用于测试
    2、cluster模式：主要用于生产环境

无论是standalone、yarn，都是分为这两种模式的

    standalone client、standalone cluster
    yarn client、yarn cluster

worker node，代表的是一个工作节点，并不一定就是指代的standalone模式下的Worker进程所在的节点

负责启动executor进程，执行task计算任务，实际执行我们写的代码，处理一部分数据

节点，也就是集群中的从节点（slave节点），统一叫做worker node

standalone模式下，基于spark的Master进程和Worker进程组成的集群，Worker进程所在节点，也就是Worker节点

yarn模式下，yarn的nodemanager进程所在的节点，也就叫做worker node，工作节点


我们之前几讲，讲解的原理、术语，都是基于cluster模式来讲解的

client模式，区别就在于driver启动的位置，你在哪台机器上提交spark application，在那台机器上，就会启动driver进程，直接会去启动一个jvm进程，开始执行你的main类

cluster模式，spark application或者叫做spark作业，提交到cluster manager，cluster manager负责在集群中某个节点上，启动driver进程




cluster mode：集群模式，常用的有两种，standalone和yarn

standalone模式，集群，Master进程和Worker进程，组成的集群

yarn模式，集群，ResourceManager进程和NodeManager进程，组成的集群



![](http://i2.51cto.com/images/blog/201810/03/96ba17128db2f078eb75779d0fa9b15a.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



## Spark核心编程进阶-Spark Standalone集群架构


![](http://i2.51cto.com/images/blog/201810/03/33ade3189fe496ac67c82687b2958e20.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


### Spark Standalone集群

集群管理器，cluster manager：Master进程
工作节点：Worker进程

搭建了一套Hadoop集群（HDFS+YARN）

HDFS：NameNode、DataNode、SecondaryNameNode

YARN：ResourceManager、NodeManager

Spark集群（Spark Standalone集群）：Master、worker



### Spark Standalone集群模式与YARN集群模式不同之处就在于：

如果是Spark Standalone模式，甚至你根本不需要YARN集群，甚至连HDFS集群都可以不需要

Spark，Master+Worker集群架构，就足够了，然后就可以编写spark作业，提交作业到Master+Worker集群架构中去运行

一般大公司，Hadoop、Spark、Storm、Hive、MapReduce，都用到了，统一就不搭建Spark集群（Master+Worker压根儿就没有），




YARN集群

直接部署一个spark客户端，部署一个spark安装包（解压缩+配置上hadoop的配置文件目录，spark-env.sh）

就可以提交spark作业给YARN集群去运行



每个task都执行相同的代码片段，执行相同的功能，但是处理的数据是不同的，一个task处理一小片数据

两种deploy mode：

    1、client：你在哪台机器上，用spark-submit提交spark作业，就会在那台机器上启动driver进程
    2、cluster：作业是提交给master，master找一台worker进程，启动driver


## Spark核心编程进阶-单独启动master和worker脚本详解以及启动日志查看


### standalone模式启动集群命令详解

我们之前搭建那个伪分布式spark standalone集群的时候，以及我们最早搭建分布式集群的时候
在启动集群（master进程和worker进程）的时候，大家回忆一下，我们用的是哪个命令，用的是sbin/start-all.sh脚本
这个脚本一旦执行，就会直接在集群（节点，部署了spark安装包）中，启动master进程和所有worker进程

sbin/start-all.sh脚本，其实是用来便捷地快速启动整个spark standalone集群的
但是我们本讲呢，就是对spark standalone集群的启动脚本，作一个更加详细和全面的介绍和讲解

我们除了会用sbin/start-all.sh脚本，直接启动整个集群
还要会用另外两个脚本，单独启动master和worker进程

先讲理论，后面我们来做实验

直接启动master、worker集群，使用sbin/start-all.sh即可

如果你要单独，分别，启动master和worker进程的话
那么必须得先启动master进程，然后再启动worker进程，因为worker进程启动以后，需要向master进程去注册
反过来先启动worker进程，再启动这个master进程，可能会有问题

为什么我们有的时候也需要单独启动master和worker进程呢
因为我们后面会讲的，在单独启动两个进程的时候，是可以通过命令行参数，为进程配置一些独特的参数
比如说监听的端口号、web ui的端口号、使用的cpu和内存
比如你想单独给某个worker节点配置不同的cpu和内存资源的使用限制，那么就可以使用脚本单独启动这个worker进程的时候，通过命令行参数来设置

### 手动启动master进程

需要在某个部署了spark安装包的节点上，使用sbin/start-master.sh启动
master启动之后，启动日志就会打印一行spark://HOST:PORT URL出来，这就是master的URL地址
worker进程就会通过这个URL地址来连接到master进程，并且进行注册
另外，除了worker进程要使用这个URL意外，我们自己在编写spark代码时，也可以给SparkContext的setMaster()方法，传入这个URL地址
然后我们的spark作业，就会使用standalone模式连接master，并提交作业
此外，还可以通过http://MASTER_HOST:8080 URL来访问master集群的监控web ui，那个web ui上，也会显示master的URL地址

剧透一下，后面会做单独启动master和worker进程的实验
启动master的时候，要观察什么呢？要观察一下日志中透露出来的这个master URL
在http://HOST:8080端口，观察一下master的URL地址

### 手动启动worker进程

需要在你希望作为worker node的节点上，在部署了spark安装包的前提下，使用sbin/start-slave.sh <master-spark-URL>在当前节点上启动
如上所述，使用sbin/start-slave.sh时，需要指定master URL
启动worker进程之后，再访问http://MASTER_HOST:8080，在spark集群web ui上，就可以看见新启动的worker节点，包括该节点的cpu和内存资源等信息

一会儿实验，启动worker之后，要在master web ui上，观察一下新启动的worker节点

此外，以下参数是可以在手动启动master和worker的时候指定的
    
    -h HOST, --host HOST			在哪台机器上启动，默认就是本机，这个很少配
    -p PORT, --port PORT			在机器上启动后，使用哪个端口对外提供服务，master默认是7077，worker默认是随机的，也很少配
    --webui-port PORT				web ui的端口，master默认是8080，worker默认是8081，也很少配
    -c CORES, --cores CORES			仅限于worker，总共能让spark作业使用多少个cpu core，默认是当前机器上所有的cpu core
    -m MEM, --memory MEM			仅限于worker，总共能让spark作业使用多少内存，是100M或者1G这样的格式，默认是1g
    -d DIR, --work-dir DIR			仅限于worker，工作目录，默认是SPARK_HOME/work目录
    --properties-file FILE			master和worker加载默认配置文件的地址，默认是conf/spark-defaults.conf，很少配

咱们举个例子，比如说小公司里面，物理集群可能就一套，同一台机器上面，可能要部署Storm的supervisor进程，可能还要同时部署Spark的worker进程
机器，cpu和内存，既要供storm使用，还要供spark使用
这个时候，可能你就需要限制一下worker节点能够使用的cpu和内存的数量

小公司里面，搭建spark集群的机器可能还不太一样，有的机器比如说是有5个g内存，有的机器才1个g内存
那你对于1个g内存的机器，是不是得限制一下内存使用量，比如说500m

### 实验
    
    1、启动master: 日志和web ui，观察master url
    2、启动worker: 观察web ui，是否有新加入的worker节点，以及对应的信息
    3、关闭master和worker
    4、再次单独启动master和worker，给worker限定，就使用500m内存，跟之前看到的worker信息比对一下内存最大使用量





## Spark核心编程进阶-worker节点配置以及spark-evn.sh参数详解


### standalone部署细节以及相关参数详解

这里主要讲解spark部署过程中的一些细节

配置集群中的worker节点

首先第一点，如果你想将某台机器部署成standalone集群架构中的worker节点（会运行worker daemon进程）
那么你就必须在那台机器上部署咱们的spark安装包

配置conf/slaves文件

在conf/salves文件中，哪些机器是作为worker节点的，可以配置你要在哪些机器上启动worker进程

之前已经演示过分布式的spark standalone集群部署了，需要将master和worker进程所在的节点，都部署好spark安装包

默认情况下，没有conf/slaves这个文件，只有一个conf/slaves.template，而且还是空的

此时，就只是在当前主节点上启动一个master进程和一个worker进程，此时就是master进程和worker进程在一个节点上，也就是伪分布式部署

我们升级课程中就是用的伪分布式部署，但是之前第一个大的阶段，用的是分布式的部署方式，就是说，你得去手动将slaves.template拷贝为一份slaves文件

然后需要手动去编辑conf/slaves文件中的内容

此时，在conf/slaves文件中，你可以编辑要作为worker节点的机器，比如说hostname，或者ip地址，都可以，一个机器是一行

配置以后，所有的节点上，spark部署安装包中，都得去拷贝一份这个conf/slaves文件

这里master机器和worker机器之间的访问时通过linux ssh方式进行的，所以要配置多个机器之间的ssh免密码连接

    conf/spark-env.sh

spark-env.sh文件，它的地位，就类似于hadoop中的core-site.xml、hdfs-site.xml等等

应该说是spark中最核心的一份配置文件

这份文件，可以对整个spark的集群部署，各个master和worker进程的相应的行为，进行充分和细节化的配置

    SPARK_MASTER_IP					指定master进程所在的机器的ip地址
    SPARK_MASTER_PORT				指定master监听的端口号（默认是7077）
    SPARK_MASTER_WEBUI_PORT			指定master web ui的端口号（默认是8080）

大家会发现什么，就是这个东东，怎么跟我们上一讲，讲解的单独启动master和worker进程的命令行参数，作用，是一样的

sbin/start-master.sh --port 7078，类似这种方式，貌似可以指定一样的配置属性

我明确告诉大家，这个作用的确是一模一样的

你可以在spark-evn.sh中就去配置好
但是有时呢，可能你会遇到需要临时更改配置，并启动master或worker进程的情况

此时就比较适合，用sbin/start-master.sh这种脚本的命令行参数，来设置这种配置属性

但是通常来说呢，还是推荐在部署的时候，通过spark-env.sh来设定

脚本命令行参数通常用于临时的情况

    SPARK_MASTER_OPTS				设置master的额外参数，使用"-Dx=y"设置各个参数
    比如说export SPARK_MASTER_OPTS="-Dspark.deploy.defaultCores=1"

    参数名											默认值						含义
    
    spark.deploy.retainedApplications				200							在spark web ui上最多显示多少个application的信息
    
    spark.deploy.retainedDrivers					200							在spark web ui上最多显示多少个driver的信息
    
    spark.deploy.spreadOut							true						资源调度策略，spreadOut会尽量将application的executor进程分布在更多worker上，适合基于hdfs文件计算的情况，提升数据本地化概率；非spreadOut会尽量将executor分配到一个worker上，适合计算密集型的作业

    spark.deploy.defaultCores			无限大				每个spark作业最多在standalone集群中使用多少个cpu core，默认是无限大，有多少用多少
    
    spark.deploy.timeout			60 单位秒，一个worker多少时间没有响应之后，master认为worker挂掉了

    SPARK_LOCAL_DIRS				spark的工作目录，包括了shuffle map输出文件，以及持久化到磁盘的RDD等

    SPARK_WORKER_PORT				worker节点的端口号，默认是随机的
    SPARK_WORKER_WEBUI_PORT			worker节点的web ui端口号，默认是8081
    SPARK_WORKER_CORES				worker节点上，允许spark作业使用的最大cpu数量，默认是机器上所有的cpu core
    SPARK_WORKER_MEMORY				worker节点上，允许spark作业使用的最大内存量，格式为1000m，2g等，默认最小是1g内存

就是说，有些master和worker的配置，可以在spark-env.sh中部署时即配置，但是也可以在start-slave.sh脚本启动进程时命令行参数设置

但是命令行参数的优先级比较高，会覆盖掉spark-env.sh中的配置

比如说，上一讲我们的实验，worker的内存默认是1g，但是我们通过--memory 500m，是可以覆盖掉这个属性的

    SPARK_WORKER_INSTANCES			当前机器上的worker进程数量，默认是1，可以设置成多个，但是这时一定要设置SPARK_WORKER_CORES，限制每个worker的cpu数量
    SPARK_WORKER_DIR				spark作业的工作目录，包括了作业的日志等，默认是spark_home/work
    SPARK_WORKER_OPTS				worker的额外参数，使用"-Dx=y"设置各个参数

    参数名											默认值						含义
    spark.worker.cleanup.enabled					false						是否启动自动清理worker工作目录，默认是false
    spark.worker.cleanup.interval					1800						单位秒，自动清理的时间间隔，默认是30分钟
    spark.worker.cleanup.appDataTtl					7 * 24 * 3600				默认将一个spark作业的文件在worker工作目录保留多少时间，默认是7天

    SPARK_DAEMON_MEMORY				分配给master和worker进程自己本身的内存，默认是1g
    SPARK_DAEMON_JAVA_OPTS			设置master和worker自己的jvm参数，使用"-Dx=y"设置各个参数
    SPARK_PUBLISC_DNS				master和worker的公共dns域名，默认是没有的

这里提示一下，大家可以观察一下，咱们的内存使用情况

在没有启动spark集群之前，我们的内存使用是1个多g，启动了spark集群之后，就一下子耗费到2个多g
每次又执行一个作业时，可能会耗费到3个多g左右

所以大家就明白了，为什么之前用分布式的集群，每个worker节点才1个g内存，根本是没有办法使用standalone模式和yarn模式运行作业的

上一节课讲解了单独启动spark standalone集群
这一节课，就给大家列出spark所有的启动和关闭shell脚本

    sbin/start-all.sh				根据配置，在集群中各个节点上，启动一个master进程和多个worker进程
    sbin/stop-all.sh				在集群中停止所有master和worker进程
    sbin/start-master.sh			在本地启动一个master进程
    sbin/stop-master.sh				关闭master进程
    sbin/start-slaves.sh			根据conf/slaves文件中配置的worker节点，启动所有的worker进程
    sbin/stop-slaves.sh				关闭所有worker进程
    sbin/start-slave.sh				在本地启动一个worker进程




## Spark核心编程进阶-实验：local模式提交spark作业

之前讲解过，spark作业运行集群，有两种部署方式，一种是Spark Standalone集群，还有一种是YARN集群+Spark客户端

所以，我们认为，提交spark作业的两种主要方式，就是Spark Standalone和YARN，这两种方式，分别还分为两种模式，分别是client mode和cluster mode

在体验standalone提交模式之前，咱们呢，先得体验一种Spark中最基本的一种提交模式
也就是，所谓的local模式

### local模式的基本原理

local模式下，没有所谓的master+worker这种概念

local模式，相当于，启动一个本地进程，然后在一个进程内，模拟spark集群中作业的运行
一个spark作业，就对应了进程中的一个或多个executor线程
就开始执行，包括作业的调度，task分配

在实际工作当中，local模式，主要用于测试
最常见的就是在我们的开发环境中，比如说eclipse，或者是InteliJ IDEA
直接运行spark程序，此时就可以采用local模式

本次实验，通过在eclipse中使用local来运行，以及在生产机器上（linux机器）上用local模式来运行


wordcount_local.sh

    /usr/local/spark/bin/spark-submit \
    --class cn.spark.study.core.WordCount \
    --num-executors 1 \
    --driver-memory 100m \
    --executor-memory 100m \
    --executor-cores 1 \
    /usr/local/test/spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar \

代码：

    WordCount.java


## Spark核心编程进阶-实验：standalone client模式提交spark作业

local模式下，我们都不会放到生产机器上面去提交

local模式，其实仅仅用于eclipse中运行spark作业，以及打断点，调试spark作业来用

通常，用local模式执行，我们都会手工生成一份数据，来使用

通常情况下来说，部署在测试机器上去，进行测试运行spark作业的时候

都是使用client模式（standalone client模式，或standalone cluster模式）

client模式下，提交作业以后，driver在本机启动，可以实时看到详细的日志信息，方便你追踪和排查错误

### standalone模式下提交spark作业

standalone模式提交，唯一与local区别，就是一定要想办法将master设置成spark://master_ip:port，比如spark://192.168.0.103:7077

### 三种设置master的方式

    1、硬编码: SparkConf.setMaster("spark://IP:PORT")
    2、spark-submit: --master spark://IP:PORT
    3、spark-shell: --master spark://IP:PORT

通常来说，上面三种写法，使用第二种，是最合适

一般我们都是用spark-submit脚本来提交spark application的，很少用spark-shell，spark-shell仅仅用于实验和测试一些特别简单的spark作业

使用spark-submit脚本来提交时，还可以指定两种deploy mode，spark standalone cluster支持client mode和cluster mode

client mode下，你在哪台机器上用spark-submit脚本提交application，driver就会在那台机器上启动

cluster mode下，driver会通过master进程，随机被分配给某一个worker进程来启动

这里我们就知道了，standalone模式，是要在spark-submit中，用--master指定Master进程的URL

其次，使用standalone client模式或cluster模式，是要在spark-submit中，使用--deploy-mode client/cluster来设置

默认，如果你不设置--deploy-mode，那么就是client模式

使用spark-submit脚本来提交application时，application jar是会自动被分发到所有worker节点上去的

对于你的application依赖的额外jar包，可以通过spark-submit脚本中的--jars标识，来指定，可以使用逗号分隔多个jar

比如说，你写spark-sql的时候，有的时候，在作业中，要往mysql中写数据，此时可能会出现找不到mysql驱动jar包的情况

此时，就需要你手动在spark-submit脚本中，使用--jars，加入一些依赖的jar包

### 我们提交standalone client模式的作业

    1、--master和--deploy-mode，来提交作业
    
    2、观察作业执行日志，可以看到去连接spark://192.168.0.103:7077 URL的master
    
    3、观察一些spark web ui，可以看到completed applications一栏中，有我们刚刚运行的作业，比对一下ui上的applicationId和日志中的applicationId
    
    4、重新再执行一次作业，一执行，立即看一下jps查看进程，看看standalone client模式提交作业的时候，当前机器上会启动哪些进程
    
	SparkSubmit: 也就是我们的driver进程，在本机上启动（spark-submit所在的机器）
	
	CoarseGrainedExecutorBackend（内部持有一个Executor对象）: 在你的执行spark作业的worker机器上，肯定会给你的作业分配和启动一个executor进程，具体名字就是那个
    
## Spark核心编程进阶-实验：standalone cluster模式提交spark作业

### standalone cluster模式

通常用于，spark作业部署到生产环境中去使用，是用standalone cluster模式

因为这种模式，会由master在集群中，某个节点上，来启动driver，然后driver会进行频繁的作业调度

此时driver跟集群在一起，那么是性能比较高的

standalone client模式，在spark-submit脚本执行的机器上，会启动driver进程，然后去进行整个作业的调度

通常来说，你的spark-submit脚本能够执行的机器，也就是，作为一个开发人员能够登录的机器，通常不会直接是spark集群部署的机器

因为，随便谁，都登录到spark集群中，某个机器上去，执行一个脚本，这个太扯淡了，没有安全性可言了

所有此时呢，是这样子，用client模式，你的机器，可能与spark集群部署的机器，都不在一个机房，或者距离很远，那么此时通常遥远的频繁的网络通信

会影响你的整个作业的执行性能

此外，standalone cluster模式，还支持监控你的driver进程，并且在driver挂掉的时候，自动重启该进程

要使用这个功能，在spark-submit脚本中，使用--supervise标识即可

这个跟我们的这个作业关系不是太大，主要是用于spark streaming中的HA高可用性，配置driver的高可用

如果想要杀掉反复挂掉的driver进程，使用以下命令即可: bin/spark-class org.apache.spark.deploy.Client kill <master url> <driver ID>  

如果要查看driver id，通过http://<maser url>:8080即可查看到

### 实验观察
    
    1、日志：命令行只会打印一点点内容，没有什么东西; 主要呢，是在web ui上，可以看到日志
    2、web ui: running drivers出现，此时就可以看到在spark集群中执行的driver的信息; completed drivers
    3、进程: SparkSubmit一闪而过，仅仅只是将driver注册到master上，由master来启动driver，马上就停止了; 在Worker上，会启动DriverWrapper进程
    如果能够申请到足够的cpu资源，其实还会在其他worker上，启动CoarseGrainedExecutorBackend进程
    4、可以通过点击running中的application，去查看作业中的每个job、每个stage、每个executor和task的执行信息，4040端口来看的
    5、对于正在运行的driver或application，可以在web ui上，点击kill按钮，给杀掉

首先呢，cluster模式下，driver也是要通过worker来启动的，executor也是要通过worker来启动的

首先，我们可以看到，此时driver已经启动起来了，在web ui上是可以看到的，包括driver ID

然后呢，通过web ui就可以看到，driver在唯一的worker上启动了，已经获取到了一个cpu core了

此时，driver去跟master申请资源，启动一个executor进程，但是问题来了，此时我们的worker进程，就只有一个，而且只有一个cpu core

那么，master的资源调度算法中，始终无法找到还有空闲cpu资源的worker，所以作业一直处于等待，waiting的一个状态

所以，我们的作业在当前1个cpu core下，是无法通过cluster模式来运行的

但是至少我们可以看到，cluster模式的确奏效了，因为已经开始使用cluster模式来执行作业了

包括driver的启动，开始申请executor


## Spark核心编程进阶-standalone模式下的多作业资源调度


### standalone多作业资源调度问题

目前来说，standalone集群对于同时提交上来的多个作业，仅仅支持FIFO调度策略，也就是先入先出

默认情况下，集群对多个作业同时执行的支持是不好的，没有办法同时执行多个作业

因为先提交上来的每一个作业都会尝试使用集群中所有可用的cpu资源，此时相当于就是只能支持作业串行起来，一个一个运行了

此时，如果我们希望能够支持多作业同时运行，那么就需要调整一下资源参数

我们可以设置spark.cores.max参数，来限制每个作业能够使用的最大的cpu core数量，这样先提交上来的作业不会使用所有的cpu资源

后面提交上来的作业就可以获取到资源，也可以同时并行运行了

比如说，咱们集群一共有20个节点，每个节点是8核，160 cpu core

ok，那么，如果你不限制每个作业获取的最大cpu资源大小，而且在你spark-submit的时候，或者说，你就设置了num-executors，total-cores，160

此时，你的作业是会使用所有的cpu core资源的

所以，如果我们可以通过设置全局的一个参数，让每个作业最多只能获取到一部分cpu core资源

那么，后面提交上来的作业，就也可以获取到一部分资源

standalone集群，才可以支持同时执行多个作业

使用SparkConf或spark-submit中的--conf标识，设置参数即可

比如说: 

    SparkConf conf = new SparkConf()
    		.set("spark.cores.max", "10")

通常不建议使用SparkConf，硬编码，来设置一些属性，不够灵活

建议使用spark-submit来设置属性

    --conf spark.cores.max=10
		
此外，还可以直接通过spark-env.sh配置每个application默认能使用的最大cpu数量来进行限制，默认是无限大，此时就不需要每个application都自己手动设置了

在spark-env.sh中配置spark.deploy.defaultCores即可
比如说:

    export SPARK_MASTER_OPTS="-Dspark.deploy.defaultCores=10"


## Spark核心编程进阶-standalone模式下的作业监控与日志记录

### standalone作业监控和日志记录

standalone模式下的作业的监控，很简单，就是通过我们之前给大家看的spark web ui

spark standalone模式，提供了一个web界面来让我们监控集群，并监控所有的作业的运行

web界面上，提供了master和worker的相关信息，默认的话，我们的web界面是运行在master机器上的8080端口

之前已经讲解过，可以通过配置spark-env.sh文件等方式，来配置web ui的端口

### spark web ui

    1、哪些作业在跑
    2、哪些作业跑完了，花了多少时间，使用了多少资源
    3、哪些作业失败了

### application web ui

作业监控这一块儿，其实很重要的一点是，作业的web ui

我们之前给大家讲过，application detail ui，其实就是作业的driver所在的机器的4040端口

可以看到job、stage、task的详细运行信息，shuffle read、shuffle write、gc、运行时间、每个task分配的数据量

定位很多性能问题、troubleshooting等等，task数据分布不允许，那么就是数据倾斜

哪个stage运行的时间最慢，通过之前讲解的stage划分算法，去你的代码里定位到，那个stage对应的是哪一块儿代码，你的那段代码为什么会运行太慢

是不是可以用我们之前讲解的一些性能优化的策略，去优化一下性能

但是有个问题，作业运行完了以后，我们就看不到了

此时跟history server有关，我们后面会讲解，全局的去配置整个spark集群的监控和日志

    history server

### 日志记录

1、系统级别的，spark自己的日志记录

2、我们在程序里面，用log4j，或者System.out.println打印出来的日志

### spark web ui中可以看到

1、看每个application在每个executor上的日志

2、stdout，可以显示我们用System.out.println打印出来的日志，stderr，可以显示我们用System.err.println打印出来的日志

此外，我们自己在spark作业代码中，打出来的日志，比如用System.out.println()等，是打到每个作业在每个节点的工作目录中去的

默认是SPARK_HOME/work目录下

这个目录下，每个作业都有两个文件，一个是stdout，一个是stderr，分别代表了标准输出流和异常输出流



## Spark核心编程进阶-yarn-client模式原理讲解


![](http://i2.51cto.com/images/blog/201810/03/3cb74ea041dac687654cd1403dd05f83.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

## Spark核心编程进阶-yarn-cluster模式原理讲解

![](http://i2.51cto.com/images/blog/201810/03/75c7fa364df6db7a0da21d369d71f5df.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


## Spark核心编程进阶-实验：yarn-client模式提交spark作业


### yarn运行spark作业的大前提

如果想要让spark作业可以运行在yarn上面，那么首先就必须在spark-env.sh文件中，配置HADOOP_CONF_DIR或者YARN_CONF_DIR属性，值为hadoop的配置文件目录

即HADOOP_HOME/etc/hadoop，其中包含了hadoop和yarn所有的配置文件，比如hdfs-site、yarn-site等

spark需要这些配置来读写hdfs，以及连接到yarn resourcemanager上，这个目录中包含的配置文件都会被分发到yarn集群中去的

### 在yarn模式下，也有两种运行模式

yarn-client模式下，driver进程会运行在提交作业的机器上，ApplicationMaster仅仅只是负责为作业向yarn申请资源（executor）而已，driver还是会负责作业调度

yarn-cluster模式下，driver进程会运行在yarn集群的某个工作节点上，作为一个ApplicationMaster进程运行

跟spark standalone模式不同，通常不需要使用--master指定master URL

cluster manager，也就是yarn resourcemanager的地址，会自动从hadoop配置目录中的配置文件中后去

因此，设置--master时，指定为yarn-client或yarn-cluster即可，也就代表了上面说的两种deploy mode了

这里提示一下，与standalone模式类似，yarn-client模式通常建议在测试时使用，方便你直接在提交作业的机器上查看日志

但是作业实际部署到生产环境进行运行的时候，还是使用yarn-cluster模式

使用yarn-cluster模式提交时，使用以下语法即可: 

    ./bin/spark-submit \
    --class path.to.your.Class \
    --master yarn-cluster \
    [options] \
    <app jar> \
    [app options]

比如如下脚本示例:

    $ ./bin/spark-submit --class org.leo.spark.study.WordCount \
        --master yarn-cluster \
        --num-executors 1 \
        --driver-memory 100m \
        --executor-memory 100m \
        --executor-cores 1 \
        --queue hadoop队列 \
        /usr/local/spark-study/spark-study.jar \
    
    --queue，在大公司里面，队列很重要
    
不同的数据部门，或者是不同的大数据项目，共用同一个yarn集群，运行spark作业

推荐一定要用--queue，指定不同的hadoop队列，做项目或部门之间的队列隔离
	
在yarn-cluster模式下运行时，首先在本地机器会启动一个YARN client进程

YARN client进程会连接到resourcemanager上，然后启动一个spark的ApplicationMaster进程

接着我们自己写的main类，会作为一个ApplicationMaster进程的子线程来运行

提交作业的本地机器上，YARN client进程会周期性地跟ApplicationMaster进程，拉取作业运行的进度，并打印在控制台上

一旦我们的作业完成之后，YARN client进程也就会退出了

使用yarn-client模式提交时，使用以下语法即可: ./bin/spark-shell --master yarn-client

添加其他的jar

在spark-submit脚本中，使用--jars命令即可
    
实验中要观察的几个点

    1、日志
    	命令行日志: 会详细打印你的所有的日志
    	web ui看日志: stdout、stderr
    2、web ui，spark://192.168.0.103:8080这种URL了，因为那是standalone集群的监控web ui
    	yarn的web ui上，才可以看到，stdout、stderr
    	http://192.168.0.103:8088/，URL，YARN web ui，来做作业的监控
    	http://driver:4040，通过yarn，进入spark application web ui
    3、进程
    	driver是什么进程
    	ApplicationMaster进程
    	executor进程
	
yarn模式下

我们的工程jar，是要拷贝到hdfs上面去的

而且它的replication，副本数量，默认是跟hadoop中的副本数量一样的

hdfs，一个datanode，没有办法做replication，所以也没有退出safemode
	

























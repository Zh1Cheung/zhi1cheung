---
title: elasticsearch(四)
categories:
- ELK
tags:
- elasticsearch


---

### filter与query深入对比：相关度，性能

1、filter与query示例

    PUT /company/employee/2
    {
      "address": {
        "country": "china",
        "province": "jiangsu",
        "city": "nanjing"
      },
      "name": "tom",
      "age": 30,
      "join_date": "2016-01-01"
    }
    
    PUT /company/employee/3
    {
      "address": {
        "country": "china",
        "province": "shanxi",
        "city": "xian"
      },
      "name": "marry",
      "age": 35,
      "join_date": "2015-01-01"
    }

搜索请求：年龄必须大于等于30，同时join_date必须是2016-01-01

    GET /company/employee/_search
    {
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "join_date": "2016-01-01"
              }
            }
          ],
          "filter": {
            "range": {
              "age": {
                "gte": 30
              }
            }
          }
        }
      }
    }

2、filter与query对比

filter，仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响

query，会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序

一般来说，如果你是在进行搜索，需要将最匹配搜索条件的数据先返回，那么用query；如果你只是要根据一些条件筛选出一部分数据，不关注其排序，那么用filter

除非是你的这些搜索条件，你希望越符合这些搜索条件的document越排在前面返回，那么这些搜索条件要放在query中；如果你不希望一些搜索条件来影响你的document排序，那么就放在filter中即可

3、filter与query性能

filter，不需要计算相关度分数，不需要按照相关度分数进行排序，同时还有内置的自动cache最常使用filter的数据

query，相反，要计算相关度分数，按照分数进行排序，而且无法cache结果


### 常用的各种query搜索语法



1、match all

    GET /_search
    {
        "query": {
            "match_all": {}
        }
    }

2、match

    GET /_search
    {
        "query": { "match": { "title": "my elasticsearch article" }}
    }

3、multi match

    GET /test_index/test_type/_search
    {
      "query": {
        "multi_match": {
          "query": "test",
          "fields": ["test_field", "test_field1"]
        }
      }
    }

4、range query

    GET /company/employee/_search 
    {
      "query": {
        "range": {
          "age": {
            "gte": 30
          }
        }
      }
    }

5、term query

    GET /test_index/test_type/_search 
    {
      "query": {
        "term": {
          "test_field": "test hello"
        }
      }
    }

6、terms query

    GET /_search
    {
        "query": { "terms": { "tag": [ "search", "full_text", "nosql" ] }}
    }



### 多搜索条件组合查询

    GET /website/article/_search
    {
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "title": "elasticsearch"
              }
            }
          ],
          "should": [
            {
              "match": {
                "content": "elasticsearch"
              }
            }
          ],
          "must_not": [
            {
              "match": {
                "author_id": 111
              }
            }
          ]
        }
      }
    }
    
    {
        "bool": {
            "must":     { "match": { "title": "how to make millions" }},
            "must_not": { "match": { "tag":   "spam" }},
            "should": [
                { "match": { "tag": "starred" }}
            ],
            "filter": {
              "range": { "date": { "gte": "2014-01-01" }} 
            }
        }
    }

bool

must，must_not，should，filter

每个子查询都会计算一个document针对它的相关度分数，然后bool综合所有分数，合并为一个分数，当然filter是不会计算分数的

    {
        "bool": {
            "must":     { "match": { "title": "how to make millions" }},
            "must_not": { "match": { "tag":   "spam" }},
            "should": [
                { "match": { "tag": "starred" }}
            ],
            "filter": {
              "bool": { 
                  "must": [
                      { "range": { "date": { "gte": "2014-01-01" }}},
                      { "range": { "price": { "lte": 29.99 }}}
                  ],
                  "must_not": [
                      { "term": { "category": "ebooks" }}
                  ]
              }
            }
        }
    }
    
    GET /company/employee/_search 
    {
      "query": {
        "constant_score": {
          "filter": {
            "range": {
              "age": {
                "gte": 30
              }
            }
          }
        }
      }
    }


### 如何定位不合法的搜索以及其原因


    GET /test_index/test_type/_validate/query?explain
    {
      "query": {
        "math": {
          "test_field": "test"
        }
      }
    }
    
    {
      "valid": false,
      "error": "org.elasticsearch.common.ParsingException: no [query] registered for [math]"
    }
    
    GET /test_index/test_type/_validate/query?explain
    {
      "query": {
        "match": {
          "test_field": "test"
        }
      }
    }
    
    {
      "valid": true,
      "_shards": {
        "total": 1,
        "successful": 1,
        "failed": 0
      },
      "explanations": [
        {
          "index": "test_index",
          "valid": true,
          "explanation": "+test_field:test #(#_type:test_type)"
        }
      ]
    }

一般用在那种特别复杂庞大的搜索下，比如你一下子写了上百行的搜索，这个时候可以先用validate api去验证一下，搜索是否合法



### 如何定制搜索结果的排序规则


1、默认排序规则

默认情况下，是按照_score降序排序的

然而，某些情况下，可能没有有用的_score，比如说filter

    GET /_search
    {
        "query" : {
            "bool" : {
                "filter" : {
                    "term" : {
                        "author_id" : 1
                    }
                }
            }
        }
    }

当然，也可以是constant_score

    GET /_search
    {
        "query" : {
            "constant_score" : {
                "filter" : {
                    "term" : {
                        "author_id" : 1
                    }
                }
            }
        }
    }

2、定制排序规则

    GET /company/employee/_search 
    {
      "query": {
        "constant_score": {
          "filter": {
            "range": {
              "age": {
                "gte": 30
              }
            }
          }
        }
      },
      "sort": [
        {
          "join_date": {
            "order": "asc"
          }
        }
      ]
    }


### 将一个field索引两次来解决字符串排序问题


如果对一个string field进行排序，结果往往不准确，因为分词后是多个单词，再排序就不是我们想要的结果了

通常解决方案是，将一个string field建立两次索引，一个分词，用来进行搜索；一个不分词，用来进行排序

    PUT /website 
    {
      "mappings": {
        "article": {
          "properties": {
            "title": {
              "type": "text",
              "fields": {
                "raw": {
                  "type": "string",
                  "index": "not_analyzed"
                }
              },
              "fielddata": true
            },
            "content": {
              "type": "text"
            },
            "post_date": {
              "type": "date"
            },
            "author_id": {
              "type": "long"
            }
          }
        }
      }
    }
    
    PUT /website/article/1
    {
      "title": "first article",
      "content": "this is my second article",
      "post_date": "2017-01-01",
      "author_id": 110
    }
    
    {
      "took": 2,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 3,
        "max_score": 1,
        "hits": [
          {
            "_index": "website",
            "_type": "article",
            "_id": "2",
            "_score": 1,
            "_source": {
              "title": "first article",
              "content": "this is my first article",
              "post_date": "2017-02-01",
              "author_id": 110
            }
          },
          {
            "_index": "website",
            "_type": "article",
            "_id": "1",
            "_score": 1,
            "_source": {
              "title": "second article",
              "content": "this is my second article",
              "post_date": "2017-01-01",
              "author_id": 110
            }
          },
          {
            "_index": "website",
            "_type": "article",
            "_id": "3",
            "_score": 1,
            "_source": {
              "title": "third article",
              "content": "this is my third article",
              "post_date": "2017-03-01",
              "author_id": 110
            }
          }
        ]
      }
    }

    GET /website/article/_search
    {
      "query": {
        "match_all": {}
      },
      "sort": [
        {
          "title.raw": {
            "order": "desc"
          }
        }
      ]
    }


### 相关度评分TF&IDF算法

1、算法介绍

relevance score算法，简单来说，就是计算出，一个索引中的文本，与搜索文本，他们之间的关联匹配程度

 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.

Elasticsearch使用的是 term frequency/inverse document frequency算法，简称为TF/IDF算法

Term frequency：搜索文本中的各个词条在field文本中出现了多少次，出现次数越多，就越相关

搜索请求：hello world

    doc1：hello you, and world is very good
    doc2：hello, how are you

Inverse document frequency：搜索文本中的各个词条在整个索引的所有文档中出现了多少次，出现的次数越多，就越不相关

搜索请求：hello world

    doc1：hello, today is very good
    doc2：hi world, how are you

比如说，在index中有1万条document，hello这个单词在所有的document中，一共出现了1000次；world这个单词在所有的document中，一共出现了100次

doc2更相关

Field-length norm：field长度，field越长，相关度越弱

搜索请求：hello world

    doc1：{ "title": "hello article", "content": "babaaba 1万个单词" }
    doc2：{ "title": "my article", "content": "blablabala 1万个单词，hi world" }

hello world在整个index中出现的次数是一样多的

doc1更相关，title field更短



2、_score是如何被计算出来的

    GET /test_index/test_type/_search?explain
    {
      "query": {
        "match": {
          "test_field": "test hello"
        }
      }
    }
 

3、分析一个document是如何被匹配上的

    GET /test_index/test_type/6/_explain
    {
      "query": {
        "match": {
          "test_field": "test hello"
        }
      }
    }

### doc value


搜索的时候，要依靠倒排索引；排序的时候，需要依靠正排索引，看到每个document的每个field，然后进行排序，所谓的正排索引，其实就是doc values

在建立索引的时候，一方面会建立倒排索引，以供搜索用；一方面会建立正排索引，也就是doc values，以供排序，聚合，过滤等操作使用

doc values是被保存在磁盘上的，此时如果内存足够，os会自动将其缓存在内存中，性能还是会很高；如果内存不足够，os会将其写入磁盘上


    doc1: hello world you and me
    doc2: hi, world, how are you
    
    word		doc1		doc2
    
    hello		*
    world		*		*
    you		*		*
    and 		*
    me		*
    hi				*
    how				*
    are				*
    
    hello you --> hello, you
    
    hello --> doc1
    you --> doc1,doc2
    
    doc1: hello world you and me
    doc2: hi, world, how are you
    
    sort by age
    
    
    doc1: { "name": "jack", "age": 27 }
    doc2: { "name": "tom", "age": 30 }
    
    document	name		age
    
    doc1		jack		27
    doc2		tom		30	




### query phase


1、query phase

（1）搜索请求发送到某一个coordinate node，构构建一个priority queue，长度以paging操作from和size为准，默认为10

（2）coordinate node将请求转发到所有shard，每个shard本地搜索，并构建一个本地的priority queue

（3）各个shard将自己的priority queue返回给coordinate node，并构建一个全局的priority queue

2、replica shard如何提升搜索吞吐量

一次请求要打到所有shard的一个replica/primary上去，如果每个shard都有多个replica，那么同时并发过来的搜索请求可以同时打到其他的replica上去

![image](https://img2018.cnblogs.com/blog/1279115/201809/1279115-20180928164326271-1211279601.png)



### fetch phase

1、fetch phbase工作流程

（1）coordinate node构建完priority queue之后，就发送mget请求去所有shard上获取对应的document

（2）各个shard将document返回给coordinate node

（3）coordinate node将合并后的document结果返回给client客户端

2、一般搜索，如果不加from和size，就默认搜索前10条，按照_score排序


![image](https://img2018.cnblogs.com/blog/1279115/201809/1279115-20180928164447778-1675369653.png)



### 搜索相关参数以及bouncing results问题


1、preference

决定了哪些shard会被用来执行搜索操作

    _primary, _primary_first, _local, _only_node:xyz, _prefer_node:xyz, _shards:2,3

bouncing results问题，两个document排序，field值相同；不同的shard上，可能排序不同；每次请求轮询打到不同的replica shard上；每次页面上看到的搜索结果的排序都不一样。这就是bouncing result，也就是跳跃的结果。

搜索的时候，是轮询将搜索请求发送到每一个replica shard（primary shard），但是在不同的shard上，可能document的排序不同

解决方案就是将preference设置为一个字符串，比如说user_id，让每个user每次搜索的时候，都使用同一个replica shard去执行，就不会看到bouncing results了

2、timeout，已经讲解过原理了，主要就是限定在一定时间内，将部分获取到的数据直接返回，避免查询耗时过长

3、routing，document文档路由，_id路由，routing=user_id，这样的话可以让同一个user对应的数据到一个shard上去

4、search_type

    default：query_then_fetch
    dfs_query_then_fetch，可以提升revelance sort精准度




### 基于scoll技术滚动搜索大量数据


如果一次性要查出来比如10万条数据，那么性能会很差，此时一般会采取用scoll滚动查询，一批一批的查，直到所有数据都查询完处理完

使用scoll滚动搜索，可以先搜索一批数据，然后下次再搜索一批数据，以此类推，直到搜索出全部的数据来

scoll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索，如果这个期间数据变更，是不会让用户看到的
采用基于_doc进行排序的方式，性能较高

每次发送scroll请求，我们还需要指定一个scoll参数，指定一个时间窗口，每次搜索请求只要在这个时间窗口内能完成就可以了

    GET /test_index/test_type/_search?scroll=1m
    {
      "query": {
        "match_all": {}
      },
      "sort": [ "_doc" ],
      "size": 3
    }
    
    {
      "_scroll_id": "DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACxeFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYBY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAALF8WNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAACxhFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYhY0b25zVFlWWlRqR3ZJajlfc3BXejJ3",
      "took": 5,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 10,
        "max_score": null,
        "hits": [
          {
            "_index": "test_index",
            "_type": "test_type",
            "_id": "8",
            "_score": null,
            "_source": {
              "test_field": "test client 2"
            },
            "sort": [
              0
            ]
          },
          {
            "_index": "test_index",
            "_type": "test_type",
            "_id": "6",
            "_score": null,
            "_source": {
              "test_field": "tes test"
            },
            "sort": [
              0
            ]
          },
          {
            "_index": "test_index",
            "_type": "test_type",
            "_id": "AVp4RN0bhjxldOOnBxaE",
            "_score": null,
            "_source": {
              "test_content": "my test"
            },
            "sort": [
              0
            ]
          }
        ]
      }
    }

获得的结果会有一个scoll_id，下一次再发送scoll请求的时候，必须带上这个scoll_id

    GET /_search/scroll
    {
        "scroll": "1m", 
        "scroll_id" : "DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACxeFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYBY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAALF8WNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAACxhFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYhY0b25zVFlWWlRqR3ZJajlfc3BXejJ3"
    }
    
    11,4,7
    3,2,1
    20

scoll，看起来挺像分页的，但是其实使用场景不一样。分页主要是用来一页一页搜索，给用户看的；scoll主要是用来一批一批检索数据，让系统进行处理的


### 创建、修改以及删除索引



1、为什么我们要手动创建索引？

2、创建索引

创建索引的语法

    PUT /my_index
    {
        "settings": { ... any settings ... },
        "mappings": {
            "type_one": { ... any mappings ... },
            "type_two": { ... any mappings ... },
            ...
        }
    }

创建索引的示例

    PUT /my_index
    {
      "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0
      },
      "mappings": {
        "my_type": {
          "properties": {
            "my_field": {
              "type": "text"
            }
          }
        }
      }
    }

3、修改索引

    PUT /my_index/_settings
    {
        "number_of_replicas": 1
    }

4、删除索引

    DELETE /my_index
    DELETE /index_one,index_two
    DELETE /index_*
    DELETE /_all
    
    elasticsearch.yml
    action.destructive_requires_name: true


### 修改分词器以及定制自己的分词器


1、默认的分词器

standard
    
    standard tokenizer：以单词边界进行切分
    standard token filter：什么都不做
    lowercase token filter：将所有字母转换为小写
    stop token filer（默认被禁用）：移除停用词，比如a the it等等

2、修改分词器的设置

启用english停用词token filter

    PUT /my_index
    {
      "settings": {
        "analysis": {
          "analyzer": {
            "es_std": {
              "type": "standard",
              "stopwords": "_english_"
            }
          }
        }
      }
    }
    
    GET /my_index/_analyze
    {
      "analyzer": "standard", 
      "text": "a dog is in the house"
    }
    
    GET /my_index/_analyze
    {
      "analyzer": "es_std",
      "text":"a dog is in the house"
    }

3、定制化自己的分词器

    PUT /my_index
    {
      "settings": {
        "analysis": {
          "char_filter": {
            "&_to_and": {
              "type": "mapping",
              "mappings": ["&=> and"]
            }
          },
          "filter": {
            "my_stopwords": {
              "type": "stop",
              "stopwords": ["the", "a"]
            }
          },
          "analyzer": {
            "my_analyzer": {
              "type": "custom",
              "char_filter": ["html_strip", "&_to_and"],
              "tokenizer": "standard",
              "filter": ["lowercase", "my_stopwords"]
            }
          }
        }
      }
    }
    
    GET /my_index/_analyze
    {
      "text": "tom&jerry are a friend in the house, <a>, HAHA!!",
      "analyzer": "my_analyzer"
    }
    
    PUT /my_index/_mapping/my_type
    {
      "properties": {
        "content": {
          "type": "text",
          "analyzer": "my_analyzer"
        }
      }
    }



### type底层数据结构



type，是一个index中用来区分类似的数据的，类似的数据，但是可能有不同的fields，而且有不同的属性来控制索引建立、分词器

field的value，在底层的lucene中建立索引的时候，全部是opaque bytes类型，不区分类型的
lucene是没有type的概念的，在document中，实际上将type作为一个document的field来存储，即_type，es通过_type来进行type的过滤和筛选

一个index中的多个type，实际上是放在一起存储的，因此一个index下，不能有多个type重名，而类型或者其他设置不同的，因为那样是无法处理的

    {
       "ecommerce": {
          "mappings": {
             "elactronic_goods": {
                "properties": {
                   "name": {
                      "type": "string",
                   },
                   "price": {
                      "type": "double"
                   },
    	       "service_period": {
    		  "type": "string"
    	       }			
                }
             },
             "fresh_goods": {
                "properties": {
                   "name": {
                      "type": "string",
                   },
                   "price": {
                      "type": "double"
                   },
    	       "eat_period": {
    		  "type": "string"
    	       }
                }
             }
          }
       }
    }
    
    {
      "name": "geli kongtiao",
      "price": 1999.0,
      "service_period": "one year"
    }
    
    {
      "name": "aozhou dalongxia",
      "price": 199.0,
      "eat_period": "one week"
    }

在底层的存储是这样子

    {
       "ecommerce": {
          "mappings": {
            "_type": {
              "type": "string",
              "index": "not_analyzed"
            },
            "name": {
              "type": "string"
            }
            "price": {
              "type": "double"
            }
            "service_period": {
              "type": "string"
            }
            "eat_period": {
              "type": "string"
            }
          }
       }
    }
    
    {
      "_type": "elactronic_goods",
      "name": "geli kongtiao",
      "price": 1999.0,
      "service_period": "one year",
      "eat_period": ""
    }
    
    {
      "_type": "fresh_goods",
      "name": "aozhou dalongxia",
      "price": 199.0,
      "service_period": "",
      "eat_period": "one week"
    }


最佳实践，将类似结构的type放在一个index下，这些type应该有多个field是相同的

假如说，你将两个type的field完全不同，放在一个index下，那么就每条数据都至少有一半的field在底层的lucene中是空值，会有严重的性能问题




### mapping root object


1、root object

就是某个type对应的mapping json，包括了properties，metadata（_id，_source，_type），settings（analyzer），其他settings（比如include_in_all）

    PUT /my_index
    {
      "mappings": {
        "my_type": {
          "properties": {}
        }
      }
    }

2、properties

    type，index，analyzer
    
    PUT /my_index/_mapping/my_type
    {
      "properties": {
        "title": {
          "type": "text"
        }
      }
    }

3、_source

好处

（1）查询的时候，直接可以拿到完整的document，不需要先拿document id，再发送一次请求拿document

（2）partial update基于_source实现

（3）reindex时，直接基于_source实现，不需要从数据库（或者其他外部存储）查询数据再修改

（4）可以基于_source定制返回field

（5）debug query更容易，因为可以直接看到_source

如果不需要上述好处，可以禁用_source

    PUT /my_index/_mapping/my_type2
    {
      "_source": {"enabled": false}
    }

4、_all

将所有field打包在一起，作为一个_all field，建立索引。没指定任何field进行搜索时，就是使用_all field在搜索。
    
    PUT /my_index/_mapping/my_type3
    {
      "_all": {"enabled": false}
    }

也可以在field级别设置include_in_all field，设置是否要将field的值包含在_all field中

    PUT /my_index/_mapping/my_type4
    {
      "properties": {
        "my_field": {
          "type": "text",
          "include_in_all": false
        }
      }
    }

5、标识性metadata

    _index，_type，_id







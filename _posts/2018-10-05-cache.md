---
title: Cache（五）
categories:
- Caching architecture
tags:
- Caching architecture
- Storm

---


## Storm介绍



kafka，zookeeper，lua，我觉得，那些东西的话，主要是讲解基于他们的一些架构，和解决方案的设计还有开发

redis：跟我们的这个topic是很有关系的，大型缓存架构，高并发高性能高可用的缓存架构的底层支持，redis，细致的去讲解，那块redis技术和知识是本套课程的一个重点

数据库+缓存双写，多级缓存架构，大家重点去理解里面的方案设计和架构思想

热数据的处理，缓存雪崩 --> storm，hystrix

对于这两个技术，都是关键性的会去影响你的热数据，缓存雪崩时的系统可用性和稳定性


kafka，消息队列，用起来很简单，而且搞java得一般来说，对消息队列都有一些了解吧，而且到了真实的生产环境中，kafka你是可以换成其他的技术，Active MQ，Rabbit MQ，Rocket MQ

zookeeper，分布式锁，分布式锁，搞java一般也会知道一些，zk去做，redis去做锁也是可以的

lua，大家后面真的是要用到lua，觉得课程里讲解的东西不够，可以自己去网上查一些lua的语法可以了，语法是最最简单的

storm，说句实话，在做热数据这块，如果要做复杂的热数据的统计和分析，亿流量，高并发的场景下，我还真觉得，最合适的技术就是storm，没有其他

缓存架构，热数据先关的架构设计，热数据相关的架构中最重要的唯一的可选技术，storm，好好的去讲一下的

hystrix，分布式系统的高可用性的限流，熔断，降级，等等，一些措施，缓存雪崩的方案，限流的技术


一、Storm到底是什么？

1、mysql，hadoop与storm

mysql：事务性系统，面临海量数据的尴尬
hadoop：离线批处理
storm：实时计算


mysql、hadoop与storm的关系：
![image](http://i2.51cto.com/images/blog/201810/01/992071f939a26db927da9ef4d93c6cb7.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)





2、我们能不能自己搞一套storm？

来一条数据，我理解就算一条，来一条，算一条

坑，海量高并发大数据，高并发的请求数据，分布式的系统，流式处理的分布式系统

如果自己搞一套实时流系统出来，也是可以的，但是。。。。

（1）花费大量的时间在底层技术细节上：如何部署各种中间队列，节点间的通信，容错，资源调配，计算节点的迁移和部署，等等

（2）花费大量的时间在系统的高可用上问题上：如何保证各种节点能够高可用稳定运行

（3）花费大量的时间在系统扩容上：吞吐量需要扩容的时候，你需要花费大量的时间去增加节点，修改配置，测试，等等

5万/s，10万/s，扩容

国内，国产的实时大数据计算系统，唯一做出来的，做得好的，做得影响力特别大，特别牛逼的，就是JStorm，阿里

阿里，技术实力，世界一流，顶尖，国内顶尖，一流

JStorm，clojure编程预压，Java重新写了一遍，Galaxy流式计算的系统，百度，腾讯，也都自己做了，也能做得很好

3、storm的特点是什么？

（1）支撑各种实时类的项目场景：实时处理消息以及更新数据库，基于最基础的实时计算语义和API（实时数据处理领域）；对实时的数据流持续的进行查询或计算，同时将最新的计算结果持续的推送给客户端展示，同样基于最基础的实时计算语义和API（实时数据分析领域）；对耗时的查询进行并行化，基于DRPC，即分布式RPC调用，单表30天数据，并行化，每个进程查询一天数据，最后组装结果

storm做各种实时类的项目都ok

（2）高度的可伸缩性：如果要扩容，直接加机器，调整storm计算作业的并行度就可以了，storm会自动部署更多的进程和线程到其他的机器上去，无缝快速扩容

扩容起来，超方便

（3）数据不丢失的保证：storm的消息可靠机制开启后，可以保证一条数据都不丢

数据不丢失，也不重复计算

（4）超强的健壮性：从历史经验来看，storm比hadoop、spark等大数据类系统，健壮的多的多，因为元数据全部放zookeeper，不在内存中，随便挂都不要紧

特别的健壮，稳定性和可用性很高

（5）使用的便捷性：核心语义非常的简单，开发起来效率很高

用起来很简单，开发API还是很简单的



## Storm集群架构与核心概念



二、Storm的集群架构以及核心概念


1、Storm的集群架构


Nimbus，Supervisor，ZooKeeper，Worker，Executor，Task



![image](http://i2.51cto.com/images/blog/201810/01/a88247d9c305c6c9a4a66d3ff1e06296.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



2、Storm的核心概念


![image](http://i2.51cto.com/images/blog/201810/01/c3d6d72e2bed43a3b13ce287deaf8eeb.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)




Topology，Spout，Bolt，Tuple，Stream

拓扑：务虚的一个概念

Spout：数据源的一个代码组件，就是我们可以实现一个spout接口，写一个java类，在这个spout代码中，我们可以自己尝试去数据源获取数据，比如说从kafka中消费数据

bolt：一个业务处理的代码组件，spout会将数据传送给bolt，各种bolt还可以串联成一个计算链条，java类实现了一个bolt接口

一堆spout+bolt，就会组成一个topology，就是一个拓扑，实时计算作业，spout+bolt，一个拓扑涵盖数据源获取/生产+数据处理的所有的代码逻辑，topology

tuple：就是一条数据，每条数据都会被封装在tuple中，在多个spout和bolt之间传递

stream：就是一个流，务虚的一个概念，抽象的概念，源源不断过来的tuple，就组成了一条数据流














## Storm并行度和流分组




三、Storm的并行度以及流分组

因为我们在这里，是讲给java工程师的storm教程

所以我期望的场景是，你们所在的公司，基本上来说，已经有大数据团队，有人在维护storm集群

我觉得，对于java工程师来说，先不说精通storm

至少说，对storm的核心的基本原理，门儿清，你都很清楚，集群架构、核心概念、并行度和流分组

接下来，掌握最常见的storm开发范式，spout消费kafka，后面跟一堆bolt，bolt之间设定好流分组的策略

在bolt中填充各种代码逻辑

了解如何将storm拓扑打包后提交到storm集群上去运行

掌握如何能够通过storm ui去查看你的实时计算拓扑的运行现状

你在一个公司里，如果说，需要在你的java系统架构中，用到一些类似storm的大数据技术，如果已经有人维护了storm的集群

那么此时你就可以直接用，直接掌握如何开发和部署即可

但是，当然了，如果说，恰巧没人负责维护storm集群，也没什么大数据的团队，那么你可能需要说再去深入学习一下storm

当然了，如果你的场景不是特别复杂，整个数据量也不是特别大，其实自己主要研究一下，怎么部署storm集群

你自己部署一个storm集群，也ok





worker，executor，task，supervisor，流的




并行度：Worker->Executor->Task，没错，是Task

流分组：Task与Task之间的数据流向关系




![image](http://i2.51cto.com/images/blog/201810/01/d3072bc6e5e37c2439186dca1a271d95.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)





Shuffle Grouping：随机发射，负载均衡
Fields Grouping：根据某一个，或者某些个，fields，进行分组，那一个或者多个fields如果值完全相同的话，那么这些tuple，就会发送给下游bolt的其中固定的一个task

你发射的每条数据是一个tuple，每个tuple中有多个field作为字段

比如tuple，3个字段，name，age，salary
    
    {"name": "tom", "age": 25, "salary": 10000} -> tuple -> 3个field，name，age，salary
    
    All Grouping
    Global Grouping
    None Grouping
    Direct Grouping
    Local or Shuffle Grouping


## Storm：WordCount程序



storm核心的基本原理，都了解了

写一下代码，去体验一下，storm的程序是怎么开发的，通过了解了代码之后，再回头，你去看一下之前讲解的一些基本原理，就很清楚了

大数据，入门程序，wordcount，单词计数

你可以认为，storm源源不断的接收到一些句子，然后你需要实时的统计出句子中每个单词的出现次数

（1）搭建工程环境

    <?xml version="1.0" encoding="UTF-8"?>
    <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    
    <modelVersion>4.0.0</modelVersion>
    <artifactId>storm-wordcount</artifactId>
    <packaging>jar</packaging>
    
    <name>storm-wordcount</name>
    
    <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>
    
    <dependencies>
    <dependency>
      <groupId>org.apache.storm</groupId>
      <artifactId>storm-core</artifactId>
      <version>1.1.0</version>
    </dependency>
    <dependency>
      <groupId>commons-collections</groupId>
      <artifactId>commons-collections</artifactId>
      <version>3.2.1</version>
    </dependency>
    <dependency>
      <groupId>com.google.guava</groupId>
      <artifactId>guava</artifactId>
    </dependency>
    </dependencies>
    
    <build>
    <sourceDirectory>src/main/java</sourceDirectory>
    <testSourceDirectory>test/main/java</testSourceDirectory>
    
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
                <filters>
                    <filter>
                        <artifact>*:*</artifact>
                        <excludes>
                            <exclude>META-INF/*.SF</exclude>
                            <exclude>META-INF/*.sf</exclude>
                            <exclude>META-INF/*.DSA</exclude>
                            <exclude>META-INF/*.dsa</exclude>
                            <exclude>META-INF/*.RSA</exclude>
                            <exclude>META-INF/*.rsa</exclude>
                            <exclude>META-INF/*.EC</exclude>
                            <exclude>META-INF/*.ec</exclude>
                            <exclude>META-INF/MSFTSIG.SF</exclude>
                            <exclude>META-INF/MSFTSIG.RSA</exclude>
                        </excludes>
                    </filter>
                </filters>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer" />
                            <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    
      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>exec-maven-plugin</artifactId>
        <version>1.2.1</version>
        <executions>
          <execution>
            <goals>
              <goal>exec</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <executable>java</executable>
          <includeProjectDependencies>true</includeProjectDependencies>
          <includePluginDependencies>false</includePluginDependencies>
          <classpathScope>compile</classpathScope>
          <mainClass></mainClass>
        </configuration>
      </plugin>
    </plugins>
    </build>
    </project>

（2）编写代码

    public class WordCountTopology {
    
    public static class RandomSentenceSpout extends BaseRichSpout {
    
      SpoutOutputCollector _collector;
      Random _rand;
    
    
      @Override
      public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {
        _collector = collector;
        _rand = new Random();
      }
    
      @Override
      public void nextTuple() {
        Utils.sleep(100);
        String[] sentences = new String[]{sentence("the cow jumped over the moon"), sentence("an apple a day keeps the doctor away"),
                sentence("four score and seven years ago"), sentence("snow white and the seven dwarfs"), sentence("i am at two with nature")};
        final String sentence = sentences[_rand.nextInt(sentences.length)];
    
        _collector.emit(new Values(sentence));
      }
    
      protected String sentence(String input) {
        return input;
      }
    
      @Override
      public void ack(Object id) {
      }
    
      @Override
      public void fail(Object id) {
      }
    
      @Override
      public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
      }
    
    }
    
    public static class SplitSentence implements IRichBolt {
    
    public SplitSentence() {
      
    }
    
    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
      declarer.declare(new Fields("word"));
    }
    
    @Override
    public Map<String, Object> getComponentConfiguration() {
      return null;
    }
    }
    
    public static class WordCount extends BaseBasicBolt {
    Map<String, Integer> counts = new HashMap<String, Integer>();
    
    @Override
    public void execute(Tuple tuple, BasicOutputCollector collector) {
      String word = tuple.getString(0);
      Integer count = counts.get(word);
      if (count == null)
        count = 0;
      count++;
      counts.put(word, count);
      collector.emit(new Values(word, count));
    }
    
    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
      declarer.declare(new Fields("word", "count"));
    }
    }
    
    public static void main(String[] args) throws Exception {
    TopologyBuilder builder = new TopologyBuilder();
    
    builder.setSpout("spout", new RandomSentenceSpout(), 5);
    
    builder.setBolt("split", new SplitSentence(), 8).shuffleGrouping("spout");
    builder.setBolt("count", new WordCount(), 12).fieldsGrouping("split", new Fields("word"));
    
    Config conf = new Config();
    conf.setDebug(true);
    
    if (args != null && args.length > 0) {
      conf.setNumWorkers(3);
      StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
    }
    else {
      conf.setMaxTaskParallelism(3);
    
      LocalCluster cluster = new LocalCluster();
      cluster.submitTopology("word-count", conf, builder.createTopology());
    
      Thread.sleep(10000);
    
      cluster.shutdown();
    }
    }
    }

（4）测试代码




## Storm纯手工集群部署



storm wordcount程序

蕴含了很多的知识点
    
    （1）Spout
    （2）Bolt
    （3）OutputCollector，Declarer
    （4）Topology
    （5）设置worker，executor，task，流分组

storm的核心基本原理，基本的开发，学会了

storm集群部署，怎么将storm的拓扑扔到storm集群上去跑

六、部署一个storm集群

（1）安装Java 7和Pythong 2.6.6

（2）下载storm安装包，解压缩，重命名，配置环境变量

（3）修改storm配置文件
    
    mkdir /var/storm
    
    conf/storm.yaml
    
    storm.zookeeper.servers:
      - "111.222.333.444"
      - "555.666.777.888"
    
    storm.local.dir: "/mnt/storm"
    
    nimbus.seeds: ["111.222.333.44"]

slots.ports，指定每个机器上可以启动多少个worker，一个端口号代表一个worker

    supervisor.slots.ports:
        - 6700
        - 6701
        - 6702
        - 6703

(4)启动storm集群和ui界面

    一个节点，storm nimbus >/dev/null 2>&1 &
    三个节点，storm supervisor >/dev/null 2>&1 &
    一个节点，storm ui >/dev/null 2>&1 &

（5）访问一下ui界面，8080端口


## Storm基于集群运行计算拓扑



七、提交作业到storm集群来运行

将eclipse中的工程，进行打包

（1）提交作业到storm集群

    storm jar path/to/allmycode.jar org.me.MyTopology arg1 arg2 arg3

（2）在storm ui上观察storm作业的运行

（3）kill掉某个storm作业

    storm kill topology-name

## 缓存冷启动问题：新系统上线、redis彻底崩溃导致数据无法恢复



缓存冷启动的问题

![image](http://i2.51cto.com/images/blog/201810/01/25cae357922afaadb35d3cc729ba5c54.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



新系统第一次上线，此时在缓存里可能是没有数据的

系统在线上稳定运行着，但是突然间重要的redis缓存全盘崩溃了，而且不幸的是，数据全都无法找回来

系统第一次上线启动，系统在redis故障的情况下重新启动，在高并发的场景下


## 缓存预热解决方案：基于storm实时热点统计的分布式并行缓存预热



0、缓存预热

缓存冷启动，redis启动后，一点数据都没有，直接就对外提供服务了，mysql就裸奔

（1）提前给redis中灌入部分数据，再提供服务

（2）肯定不可能将所有数据都写入redis，因为数据量太大了，第一耗费的时间太长了，第二根本redis容纳不下所有的数据

（3）需要根据当天的具体访问情况，实时统计出访问频率较高的热数据

（4）然后将访问频率较高的热数据写入redis中，肯定是热数据也比较多，我们也得多个服务并行读取数据去写，并行的分布式的缓存预热

（5）然后将灌入了热数据的redis对外提供服务，这样就不至于冷启动，直接让数据库裸奔了

1、nginx+lua将访问流量上报到kafka中

要统计出来当前最新的实时的热数据是哪些，我们就得将商品详情页访问的请求对应的流浪，日志，实时上报到kafka中

2、storm从kafka中消费数据，实时统计出每个商品的访问次数，访问次数基于LRU内存数据结构的存储方案

优先用内存中的一个LRUMap去存放，性能高，而且没有外部依赖


否则的话，依赖redis，我们就是要防止redis挂掉数据丢失的情况，就不合适了; 用mysql，扛不住高并发读写; 用hbase，hadoop生态系统，维护麻烦，太重了

其实我们只要统计出最近一段时间访问最频繁的商品，然后对它们进行访问计数，同时维护出一个前N个访问最多的商品list即可

热数据，最近一段时间，可以拿到最近一段，比如最近1个小时，最近5分钟，1万个商品请求，统计出最近这段时间内每个商品的访问次数，排序，做出一个排名前N的list

计算好每个task大致要存放的商品访问次数的数量，计算出大小

然后构建一个LRUMap，apache commons collections有开源的实现，设定好map的最大大小，就会自动根据LRU算法去剔除多余的数据，保证内存使用限制

即使有部分数据被干掉了，然后下次来重新开始计数，也没关系，因为如果它被LRU算法干掉，那么它就不是热数据，说明最近一段时间都很少访问了

3、每个storm task启动的时候，基于zk分布式锁，将自己的id写入zk同一个节点中

4、每个storm task负责完成自己这里的热数据的统计，每隔一段时间，就遍历一下这个map，然后维护一个前3个商品的list，更新这个list

5、写一个后台线程，每隔一段时间，比如1分钟，都将排名前3的热数据list，同步到zk中去，存储到这个storm task对应的一个znode中去

6、我们需要一个服务，比如说，代码可以跟缓存数据生产服务放一起，但是也可以放单独的服务

服务可能部署了很多个实例

每次服务启动的时候，就会去拿到一个storm task的列表，
然后根据taskid，一个一个的去尝试获取taskid对应的znode的zk分布式锁

如果能获取到分布式锁的话，那么就将那个storm task对应的热数据的list取出来

然后将数据从mysql中查询出来，写入缓存中，进行缓存的预热，多个服务实例，分布式的并行的去做，基于zk分布式锁做了协调了，分布式并行缓存的预热



## 基于nginx+lua完成商品详情页访问流量实时上报kafka的开发



在nginx这一层，接收到访问请求的时候，就把请求的流量上报发送给kafka

这样的话，storm才能去消费kafka中的实时的访问日志，然后去进行缓存热数据的统计

用得技术方案非常简单，从lua脚本直接创建一个kafka producer，发送数据到kafka
    
    wget https://github.com/doujiang24/lua-resty-kafka/archive/master.zip
    
    yum install -y unzip
    
    unzip lua-resty-kafka-master.zip
    
    cp -rf /usr/local/lua-resty-kafka-master/lib/resty /usr/hello/lualib
    
    nginx -s reload
    
    local cjson = require("cjson")  
    local producer = require("resty.kafka.producer")  
    
    local broker_list = {  
        { host = "192.168.31.187", port = 9092 },  
        { host = "192.168.31.19", port = 9092 },  
        { host = "192.168.31.227", port = 9092 }
    }
    
    local log_json = {}  
    log_json["headers"] = ngx.req.get_headers()  
    log_json["uri_args"] = ngx.req.get_uri_args()  
    log_json["body"] = ngx.req.read_body()  
    log_json["http_version"] = ngx.req.http_version()  
    log_json["method"] =ngx.req.get_method() 
    log_json["raw_reader"] = ngx.req.raw_header()  
    log_json["body_data"] = ngx.req.get_body_data()  
    
    local message = cjson.encode(log_json);  
    
    local productId = ngx.req.get_uri_args()["productId"]
    
    local async_producer = producer:new(broker_list, { producer_type = "async" })   
    local ok, err = async_producer:send("access-log", productId, message)  
    
    if not ok then  
        ngx.log(ngx.ERR, "kafka send err:", err)  
        return  
    end

两台机器上都这样做，才能统一上报流量到kafka

    bin/kafka-topics.sh --zookeeper 192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181 --topic access-log --replication-factor 1 --partitions 1 --create
    
    bin/kafka-console-consumer.sh --zookeeper 192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181 --topic access-log --from-beginning

（1）kafka在187上的节点死掉了，可能是虚拟机的问题，杀掉进程，重新启动一下

nohup bin/kafka-server-start.sh config/server.properties &

（2）需要在nginx.conf中，http部分，加入resolver 8.8.8.8;

（3）需要在kafka中加入advertised.host.name = 192.168.31.187，重启三个kafka进程

（4）需要启动eshop-cache缓存服务，因为nginx中的本地缓存可能不在了


## 基于storm+kafka完成商品访问次数实时统计拓扑的开发


maven构建出的一些问题，直接从maven中央仓库可能下载不到jar包，自己去百度一下jar，下载下来

根据错误提示，拷贝到maven本地仓库对应的目录中去，然后手工安装一下

1、kafka consumer spout

单独的线程消费，写入队列

nextTuple，每次都是判断队列有没有数据，有的话再去获取并发射出去，不能阻塞

2、日志解析bolt

3、商品访问次数统计bolt

基于LRUMap完成统计


## 基于storm完成LRUMap中topn热门商品列表的算法讲解与编写

topn：

    （1）直接使用List的Sort方法进行处理。
    （2）使用排序二叉树进行排序，然后取出前N名。
    （3）使用最大堆排序，然后取出前N名。

1、storm task启动的时候，基于分布式锁将自己的taskid累加到一个znode中

2、开启一个单独的后台线程，每隔1分钟算出top3热门商品list

3、每个storm task将自己统计出的热数据list写入自己对应的znode中


## 基于storm+zookeeper完成热门商品列表的分段存储


1、task初始化

2、热门商品list保存





## 基于双重zookeeper分布式锁完成分布式并行缓存预热的代码开发



1、服务启动的时候，进行缓存预热

2、从zk中读取taskid列表

3、依次遍历每个taskid，尝试获取分布式锁，如果获取不到，快速报错，不要等待，因为说明已经有其他服务实例在预热了

4、直接尝试获取下一个taskid的分布式锁

5、即使获取到了分布式锁，也要检查一下这个taskid的预热状态，如果已经被预热过了，就不再预热了

6、执行预热操作，遍历productid列表，查询数据，然后写ehcache和redis

7、预热完成后，设置taskid对应的预热状态



## 将缓存预热解决方案的代码运行后观察效果以及调试和修复所有的bug


缓存预热，我们已经全部搞完了，所以说，接下来呢，storm拓扑，缓存服务，都给跑起来，看看能不能符合我们的期望



## 热点缓存问题：促销抢购时的超级热门商品可能导致系统全盘崩溃的场景



热数据 -> 热数据的统计 -> redis中缓存的预热 -> 避免新系统刚上线，或者是redis崩溃数据丢失后重启，redis中没有数据，redis冷启动 -> 大量流量直接到数据库

redis启动前，必须确保其中是有部分热数据的缓存的

瞬间的缓存热点

热点缓存导致系统崩溃的问题：

![image](http://i2.51cto.com/images/blog/201810/01/7761906a1b3270d074d9550470346916.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


## 基于nginx+lua+storm的热点缓存的流量分发策略自动降级解决方案



热点缓存的解决方案：

![image](http://i2.51cto.com/images/blog/201810/01/f477c98cadd3978350146efb8ee638ea.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)





1、在storm中，实时的计算出瞬间出现的热点

有很多种算法，给大家介绍一种我们的比较简单的算法

某个storm task，上面算出了1万个商品的访问次数，LRUMap

频率高一些，每隔5秒，去遍历一次LRUMap，将其中的访问次数进行排序，统计出往后排的95%的商品访问次数的平均值

    1000
    999
    888
    777
    666
    50
    60
    80
    100
    120

比如说，95%的商品，访问次数的平均值是100

然后，从最前面开始，往后遍历，去找有没有瞬间出现的热点数据

1000，95%的平均值（100）的10倍，这个时候要设定一个阈值，比如说超出95%平均值得n倍，5倍

我们就认为是瞬间出现的热点数据，判断其可能在短时间内继续扩大的访问量，甚至达到平均值几十倍，或者几百倍

当遍历，发现说第一个商品的访问次数，小于平均值的5倍，就安全了，就break掉这个循环

热点数据，热数据，不是一个概念

有100个商品，前10个商品比较热，都访问量在500左右，其他的普通商品，访问量都在200左右，就说前10个商品是热数据

统计出来

预热的时候，将这些热数据放在缓存中去预热就可以了

热点，前面某个商品的访问量，瞬间超出了普通商品的10倍，或者100倍，1000倍，热点

2、storm这里，会直接发送http请求到nginx上，nginx上用lua脚本去处理这个请求

storm会将热点本身对应的productId，发送到流量分发的nginx上面去，放在本地缓存中

storm会将热点对应的完整的缓存数据，发送到所有的应用nginx服务器上去，直接放在本地缓存中

3、流量分发nginx的分发策略降级

流量分发nginx，加一个逻辑，就是每次访问一个商品详情页的时候，如果发现它是个热点，那么立即做流量分发策略的降级

hash策略，同一个productId的访问都同一台应用nginx服务器上

降级成对这个热点商品，流量分发采取随机负载均衡发送到所有的后端应用nginx服务器上去

瞬间将热点缓存数据的访问，从hash分发，全部到一台nginx，变成了，负载均衡发送到多台nginx上去

避免说大量的流量全部集中到一台机器，50万的访问量到一台nginx，5台应用nginx，每台就可以承载10万的访问量

4、storm还需要保存下来上次识别出来的热点list

下次去识别的时候，这次的热点list跟上次的热点list做一下diff，看看可能有的商品已经不是热点了

热点的取消的逻辑，发送http请求到流量分发的nginx上去，取消掉对应的热点数据，从nginx本地缓存中，删除





## 加入热点缓存实时自动识别和感知的代码逻辑，以及向nginx反向推送缓存热点的ID和数据的代码逻辑


    private class HotProductFindThread implements Runnable {
    
       public void run() {
          List<Map.Entry<Long, Long>> productCountList = new ArrayList<Map.Entry<Long, Long>>();
      List<Long> hotProductIdList = new ArrayList<Long>();
      while(true) {
            // 1、将LRUMap中的数据按照访问次数，进行全局的排序
      // 2、计算95%的商品的访问次数的平均值
      // 3、遍历排序后的商品访问次数，从最大的开始
      // 4、如果某个商品比如它的访问量是平均值的10倍，就认为是缓存的热点
      try {
                productCountList.clear();
      hotProductIdList.clear();
      if(productCountMap.size() == 0) {
                   Utils.sleep(100);
     continue;  }
                
                _LOGGER_.info("【HotProductFindThread打印productCountMap的长度】size=" + productCountMap.size());
       // 1、先做全局的排序
       for(Map.Entry<Long, Long> productCountEntry : productCountMap.entrySet()) {
                   if(productCountList.size() == 0) {
                      productCountList.add(productCountEntry);
      } else {
                    // 比较大小，生成最热topn的算法有很多种   boolean bigger = false;
      for(int i = 0; i < productCountList.size(); i++){
                         Map.Entry<Long, Long> topnProductCountEntry = productCountList.get(i);
      if(productCountEntry.getValue() > topnProductCountEntry.getValue()) {
                            int lastIndex = productCountList.size() < productCountMap.size() ? productCountList.size() - 1 : productCountMap.size() - 2;
     for(int j = lastIndex; j >= i; j--) {
                               if(j + 1 == productCountList.size()) {
                                  productCountList.add(null);
      }
                               productCountList.set(j + 1, productCountList.get(j));  
      }
                            productCountList.set(i, productCountEntry);
      bigger = true;
     break;  }
                      }
                      
                      if(!bigger) {
                         if(productCountList.size() < productCountMap.size()) {
                            productCountList.add(productCountEntry);
      }
                      }
                   }
                }
                
                // 2、计算出95%的商品的访问次数的平均值
      int calculateCount = (int)Math.floor(productCountList.size() * 0.95);
       Long totalCount = 0L;
     for(int i = productCountList.size() - 1; i >= productCountList.size() - calculateCount; i--) {
                   totalCount += productCountList.get(i).getValue();
      }
                
                Long avgCount = totalCount / calculateCount;
       // 3、从第一个元素开始遍历，判断是否是平均值得10倍
      for(Map.Entry<Long, Long> productCountEntry : productCountList) {
                   if(productCountEntry.getValue() > 10 \* avgCount) {
                      //缓存热点感知
                      hotProductIdList.add(productCountEntry.getKey());
       // 将缓存热点反向推送到流量分发的nginx中
      String distributeNginxURL = "http://192.168.31.227/hot?productId=" + productCountEntry.getKey();
      HttpClientUtils.sendGetRequest(distributeNginxURL);
       // 将缓存热点，那个商品对应的完整的缓存数据，发送请求到缓存服务去获取，反向推送到所有的后端应用nginx服务器上去
      String cacheServiceURL = "http://192.168.31.179:8080/getProductInfo?productId=" + productCountEntry.getKey();
      String response = HttpClientUtils.sendGetRequest(cacheServiceURL);    String[] appNginxURLs = new String[]{
                            "http://192.168.31.187/hot?productId=" + productCountEntry.getKey() + "&productInfo=" + response,
      "http://192.168.31.19/hot?productId=" + productCountEntry.getKey() + "&productInfo=" + response
      };
      for(String appNginxURL : appNginxURLs) {
                         HttpClientUtils.sendGetRequest(appNginxURL);
      }
                   }
                }
                
                Utils.sleep(5000); 
      } catch (Exception e) {
                e.printStackTrace();
      }
          }
       }
       
    }

## 在流量分发+后端应用双层nginx中加入接收热点缓存数据的接口

流量分发层

编辑引入主nginx.conf的product.conf

    server {    
        listen       80;    
        server_name  _;    
        #添加该路由路径  
        location /hot {    
            default_type 'text/html';    
            content_by_lua_file /usr/example/lua/hot.lua;    
    
        }  
    
    }    

    编辑hot.lua脚本
    
    #获取请求的URI  
    local uri_args = ngx.req.get_uri_args()  
    #获取URI中的参数  
    local product_id = uri_args\["productId"\]  
    #获取Nginx中的缓存  
    local cache_ngx = ngx.shared.my_cache  
    #拼接热点数据id  
    local hot_product_cache_key = "hot_product_"..product_id  
    #将热点数据id放入分发层的缓存中  
    
    cache_ngx:set(hot_product_cache_key, "true", 60 * 60)  
后端应用层

编辑引入主nginx.conf的product.conf

    server {    
        listen       80;    
        server_name  _;    
        #添加该路由路径  
        location /hot {    
            default_type 'text/html';    
            content_by_lua_file /usr/example/lua/hot.lua;    
    
        }  
    
    }  


在后端应用中编辑hot.lua脚本

    local uri_args = ngx.req.get_uri_args()  
    local product_id = uri_args\["productId"\]  
    local product_info = uri_args\["productInfo"\]  
    local product_cache_key = "product_info_"..product_id  
    local cache_ngx = ngx.shared.my_cache  
    #将热点数据和其数据信息，放入应用层的Nginx缓存中，过期时间60秒  
    
    cache_ngx:set(product_cache_key,product_info,60 * 60)



## 在nginx+lua中实现热点缓存自动降级为负载均衡流量分发策略的逻辑




实现热点缓存自动降级为负载均衡流量分发策略

  private class HotProductFindThread implements Runnable {
    
       public void run() {
          List<Map.Entry<Long, Long>> productCountList = new ArrayList<Map.Entry<Long, Long>>();
      List<Long> hotProductIdList = new ArrayList<Long>();
      List<Long> lastTimeHotProductList = new ArrayList<Long>();   while(true) {
             // 1、将LRUMap中的数据按照访问次数，进行全局的排序
      // 2、计算95%的商品的访问次数的平均值
      // 3、遍历排序后的商品访问次数，从最大的开始
      // 4、如果某个商品比如它的访问量是平均值的10倍，就认为是缓存的热点
      try {
                productCountList.clear();
      hotProductIdList.clear();
      if(productCountMap.size() == 0) {
                   Utils.sleep(100);
     continue;  }
                
                _LOGGER_.info("【HotProductFindThread打印productCountMap的长度】size=" + productCountMap.size());
       // 1、先做全局的排序
       for(Map.Entry<Long, Long> productCountEntry : productCountMap.entrySet()) {
                   if(productCountList.size() == 0) {
                      productCountList.add(productCountEntry);
      } else {
                      // 比较大小，生成最热topn的算法有很多种
      // 但是我这里为了简化起见，不想引入过多的数据结构和算法的的东西
      // 很有可能还是会有漏洞，但是我已经反复推演了一下了，而且也画图分析过这个算法的运行流程了
      boolean bigger = false;
      for(int i = 0; i < productCountList.size(); i++){
                         Map.Entry<Long, Long> topnProductCountEntry = productCountList.get(i);
      if(productCountEntry.getValue() > topnProductCountEntry.getValue()) {
                            int lastIndex = productCountList.size() < productCountMap.size() ? productCountList.size() - 1 : productCountMap.size() - 2;
     for(int j = lastIndex; j >= i; j--) {
                               if(j + 1 == productCountList.size()) {
                                  productCountList.add(null);
      }
                               productCountList.set(j + 1, productCountList.get(j));  
      }
                            productCountList.set(i, productCountEntry);
      bigger = true;
     break;  }
                      }
                      
                      if(!bigger) {
                         if(productCountList.size() < productCountMap.size()) {
                            productCountList.add(productCountEntry);
      }
                      }
                   }
                }
                
                // 2、计算出95%的商品的访问次数的平均值
      int calculateCount = (int)Math.floor(productCountList.size() * 0.95);
       Long totalCount = 0L;
     for(int i = productCountList.size() - 1; i >= productCountList.size() - calculateCount; i--) {
                   totalCount += productCountList.get(i).getValue();
      }
                
                Long avgCount = totalCount / calculateCount;
       // 3、从第一个元素开始遍历，判断是否是平均值得10倍
      for(Map.Entry<Long, Long> productCountEntry : productCountList) {
                   if(productCountEntry.getValue() > 10 \* avgCount) {
                      hotProductIdList.add(productCountEntry.getKey());
     if(!lastTimeHotProductList.contains(productCountEntry.getKey())) {
                         // 将缓存热点反向推送到流量分发的nginx中
      String distributeNginxURL = "http://192.168.31.227/hot?productId=" + productCountEntry.getKey();
      HttpClientUtils.sendGetRequest(distributeNginxURL);    // 将缓存热点，那个商品对应的完整的缓存数据，发送请求到缓存服务去获取，反向推送到所有的后端应用nginx服务器上去
      String cacheServiceURL = "http://192.168.31.179:8080/getProductInfo?productId=" + productCountEntry.getKey();
      String response = HttpClientUtils.sendGetRequest(cacheServiceURL);
      URLEncoder.encode(response,HTTP._UTF_8_);
      String[] appNginxURLs = new String[]{
                               "http://192.168.31.187/hot?productId=" + productCountEntry.getKey() + "&productInfo=" + response,
      "http://192.168.31.19/hot?productId=" + productCountEntry.getKey() + "&productInfo=" + response
      };   for (String appNginxURL : appNginxURLs) {
                            HttpClientUtils.sendGetRequest(appNginxURL);
      }
                      }
                   }
                }
               //4 实时感知热点数据的消失
      //如果上次的热点数据列表为空，则直接将这次查询出的热点数据加入上次的热点数据列表
      if(lastTimeHotProductList.isEmpty()){
                   if(!hotProductIdList.isEmpty()){
                      for(Long productId : hotProductIdList){
                         lastTimeHotProductList.add(productId);
      }
                   }
                }else{
                   lastTimeHotProductList.forEach(pid->{
                      if(!hotProductIdList.contains(pid)){
                         //说明上次的热点数据消失了
      //发送一个http请求到流量分发的nginx中，取消热点缓存的标识
      String url = "http://192.168.31.227/cancel_hot?product="+pid;
      }
                   });   if(!hotProductIdList.isEmpty()){
                      //清空上一次的热点数据列表，将这次的数据加入
      lastTimeHotProductList.clear();
     for(Long productId : hotProductIdList){
                         lastTimeHotProductList.add(productId);
      }
                   }else{
                      lastTimeHotProductList.clear();
      }
                }
    
                Utils.sleep(5000); 
      } catch (Exception e) {
                e.printStackTrace();
      }
          }
       }
       
    }
	
    
      

分发层lua脚本修改

    #获取请求参数，比如 productId  
    local uri_args = ngx.req.get_uri_args()  
    local productId = uri_args\["productId"\]  
    local shopId = uri_args\["shopId"\]  
    local host = {"192.168.31.19", "192.168.31.187"}  
    local backend = ""  
    
    local hot_product__key = "hot_product_"..productId
    
    #获取nginx缓存  
    local cache_nginx = ngx.shared.my_cache  
    
    local hot_product_flag = cache_nginx.get:(hot_product__key )
    
    #如果hot_product_flag 有值说明是热点数据  
    if hot_product_flag == "true" then  
        math.randomseed(tostring((os.time)):reverse():sub(1,7))  
        #将分发策略改变成随机分发，负载均衡策略，两台服务器就随机取值1 ~ 2  
        local index = math.random(1 , 2)  
        backend = "http://"..host[hash]  
    
    else  
        #对productId进行hash  
        local hash = ngx.crc32_long(productId)  
    
        hash = (hash % 2) + 1   
        #hash值对应用服务器数量取模，获取到一个应用服务器  
        backend = "http://"..host[hash]  
    end
    
    local method = uri_args["method"]  
    local requestBody = "/"..method.."?productId="..productId.."&shopId="..shopId  
      
    #利用http发送请求到应用层nginx  
    local http = require("resty.http")    
    local httpc = http.new()    
    local resp, err = httpc:request_uri(backend, {    
        method = "GET",    
        path = requestBody  
    })  
      
    if not resp then    
        ngx.say("request error :", err)    
        return    
    
    end
    
    #获取响应后返回  
    ngx.say(resp.body)    
    
    httpc:close() 








## 在storm拓扑中加入热点缓存消失的实时自动识别和感知的代码逻辑


    local host = {"192.168.31.19", "192.168.31.187"}
    
    private class HotProductFindThread implements Runnable {
    
       public void run() {
          List<Map.Entry<Long, Long>> productCountList = new ArrayList<Map.Entry<Long, Long>>();
      List<Long> hotProductIdList = new ArrayList<Long>();
      List<Long> lastTimeHotProductList = new ArrayList<Long>();   while(true) {
             // 1、将LRUMap中的数据按照访问次数，进行全局的排序
      // 2、计算95%的商品的访问次数的平均值
      // 3、遍历排序后的商品访问次数，从最大的开始
      // 4、如果某个商品比如它的访问量是平均值的10倍，就认为是缓存的热点
      try {
                productCountList.clear();
      hotProductIdList.clear();
      if(productCountMap.size() == 0) {
                   Utils.sleep(100);
     continue;  }
                
                _LOGGER_.info("【HotProductFindThread打印productCountMap的长度】size=" + productCountMap.size());
       // 1、先做全局的排序
       for(Map.Entry<Long, Long> productCountEntry : productCountMap.entrySet()) {
                   if(productCountList.size() == 0) {
                      productCountList.add(productCountEntry);
      } else {
                      // 比较大小，生成最热topn的算法有很多种
      // 但是我这里为了简化起见，不想引入过多的数据结构和算法的的东西
      // 很有可能还是会有漏洞，但是我已经反复推演了一下了，而且也画图分析过这个算法的运行流程了
      boolean bigger = false;
      for(int i = 0; i < productCountList.size(); i++){
                         Map.Entry<Long, Long> topnProductCountEntry = productCountList.get(i);
      if(productCountEntry.getValue() > topnProductCountEntry.getValue()) {
                            int lastIndex = productCountList.size() < productCountMap.size() ? productCountList.size() - 1 : productCountMap.size() - 2;
     for(int j = lastIndex; j >= i; j--) {
                               if(j + 1 == productCountList.size()) {
                                  productCountList.add(null);
      }
                               productCountList.set(j + 1, productCountList.get(j));  
      }
                            productCountList.set(i, productCountEntry);
      bigger = true;
     break;  }
                      }
                      
                      if(!bigger) {
                         if(productCountList.size() < productCountMap.size()) {
                            productCountList.add(productCountEntry);
      }
                      }
                   }
                }
                
                // 2、计算出95%的商品的访问次数的平均值
      int calculateCount = (int)Math.floor(productCountList.size() * 0.95);
       Long totalCount = 0L;
     for(int i = productCountList.size() - 1; i >= productCountList.size() - calculateCount; i--) {
                   totalCount += productCountList.get(i).getValue();
      }
                
                Long avgCount = totalCount / calculateCount;
       // 3、从第一个元素开始遍历，判断是否是平均值得10倍
      for(Map.Entry<Long, Long> productCountEntry : productCountList) {
                   if(productCountEntry.getValue() > 10 \* avgCount) {
                      hotProductIdList.add(productCountEntry.getKey());
     if(!lastTimeHotProductList.contains(productCountEntry.getKey())) {
                         // 将缓存热点反向推送到流量分发的nginx中
      String distributeNginxURL = "http://192.168.31.227/hot?productId=" + productCountEntry.getKey();
      HttpClientUtils.sendGetRequest(distributeNginxURL);    // 将缓存热点，那个商品对应的完整的缓存数据，发送请求到缓存服务去获取，反向推送到所有的后端应用nginx服务器上去
      String cacheServiceURL = "http://192.168.31.179:8080/getProductInfo?productId=" + productCountEntry.getKey();
      String response = HttpClientUtils.sendGetRequest(cacheServiceURL);
      URLEncoder.encode(response,HTTP._UTF_8_);
      String[] appNginxURLs = new String[]{
                               "http://192.168.31.187/hot?productId=" + productCountEntry.getKey() + "&productInfo=" + response,
      "http://192.168.31.19/hot?productId=" + productCountEntry.getKey() + "&productInfo=" + response
      };   for (String appNginxURL : appNginxURLs) {
                            HttpClientUtils.sendGetRequest(appNginxURL);
      }
                      }
                   }
                }
                //4 实时感知热点数据的消失
      //如果上次的热点数据列表为空，则直接将这次查询出的热点数据加入上次的热点数据列表
      if(lastTimeHotProductList.isEmpty()){
                   if(!hotProductIdList.isEmpty()){
                      for(Long productId : hotProductIdList){
                         lastTimeHotProductList.add(productId);
      }
                   }
                }else{
                   lastTimeHotProductList.forEach(pid->{
                      if(!hotProductIdList.contains(pid)){
                         //说明上次的热点数据消失了
      //发送一个http请求到流量分发的nginx中，取消热点缓存的标识
      String url = "http://192.168.31.227/cancel_hot?product="+pid;
      }
                   });   if(!hotProductIdList.isEmpty()){
                      //清空上一次的热点数据列表，将这次的数据加入
      lastTimeHotProductList.clear();
     for(Long productId : hotProductIdList){
                         lastTimeHotProductList.add(productId);
      }
                   }else{
                      lastTimeHotProductList.clear();
      }
                }
    
                Utils.sleep(5000); 
      } catch (Exception e) {
                e.printStackTrace();
      }
          }
       }
       
    }
    
    编辑nginx的server配置文件
    
    server {    
        listen       80;    
        server_name  _;    
        #添加该路由路径  
        location /cancel_hot {    
            default_type 'text/html';    
            content_by_lua_file /usr/example/lua/cancel_hot .lua;    
    
        }  
    
    }  
    
    编辑cancel_hot .lua
    
    local uri_args = ngx.req.get_uri_args()  
    local product_id = uri_args["productId"]  
    local product_cache_key = "product_info_"..product_id  
    local cache_ngx = ngx.shared.my_cache  
    #将缓存中热点数据id对应的值置为false，过期时间60秒  
    
    cache_ngx:set(product_cache_key, "false" ,60)
    
    local host = {"192.168.31.19", "192.168.31.187"}
    




## 将热点缓存自动降级解决方案的代码运行后观察效果以及调试和修复bug



1、storm中打印日志

2、重新部署storm拓扑

3、nginx中修改html模板

4、手动构造出一个热点缓存出来，看热点缓存能否进行负载均衡

5、手动让热点缓存消失，看热点缓存能否自动小时，重新进行hash分发

    http://192.168.31.187/hot?productId=15&productInfo
    ={
    "id":15,
    "name":"iphone7手机",
    "price":5599.0,
    "pictureList":"a.jpg,b.jpg",
    "specification":"iphone7的规格",
    "service":"iphone7的售后服务",
    "color":"红色,白色,黑色",
    "size":"5.5",
    "shopId":1,
    "modifiedTime":"2017-01-01 12:01:00"
    }


## hystrix与高可用系统架构：资源隔离+限流+熔断+降级+运维监控


前半部分，专注在高并发这一块，缓存架构，承载高并发，在各种高并发导致的令人崩溃/异常的场景下，运行着

缓存架构，高可用性，在各种系统的各个地方有乱七八糟的异常和故障的情况下，整套缓存系统还能继续健康的run着

HA，HAProxy，主备服务间的切换，这就做到了高可用性，主备实例，多冗余实例，高可用最最基础的东西

什么样的情况下，可能会导致系统的崩溃，以及系统不可用，针对各种各样的一些情况，然后我们用什么技术，去保护整个系统处于高可用的一个情况下

1、hystrix是什么？

netflix（国外最大的类似于，爱奇艺，优酷）视频网站，五六年前，也是，感觉自己的系统，整个网站，经常出故障，可用性不太高

有时候一些vip会员不能支付，有时候看视频就卡顿，看不了视频。。。

影响公司的收入。。。

五六年前，netflix，api team，提升高可用性，开发了一个框架，类似于spring，mybatis，hibernate，等等这种框架

高可用性的框架，hystrix

hystrix，框架，提供了高可用相关的各种各样的功能，然后确保说在hystrix的保护下，整个系统可以长期处于高可用的状态，100%，99.99999%

最理想的状况下，软件的故障，就不应该说导致整个系统的崩溃，服务器硬件的一些故障，服务的冗余

唯一有可能导致系统彻底崩溃，就是类似于之前，支付宝的那个事故，工人施工，挖断了电缆，导致几个机房都停电

不可用，和产生一些故障或者bug的区别

2、高可用系统架构

资源隔离、限流、熔断、降级、运维监控

资源隔离：让你的系统里，某一块东西，在故障的情况下，不会耗尽系统所有的资源，比如线程资源

一个case，有一块东西，是要用多线程做一些事情，小伙伴做项目的时候，没有太留神，资源隔离，那块代码，在遇到一些故障的情况下，每个线程在跑的时候，因为那个bug，直接就死循环了，导致那块东西启动了大量的线程，每个线程都死循环

最终导致我的系统资源耗尽，崩溃，不工作，不可用，废掉了

资源隔离，那一块代码，最多最多就是用掉10个线程，不能再多了，就废掉了，限定好的一些资源

限流：高并发的流量涌入进来，比如说突然间一秒钟100万QPS，废掉了，10万QPS进入系统，其他90万QPS被拒绝了

熔断：系统后端的一些依赖，出了一些故障，比如说mysql挂掉了，每次请求都是报错的，熔断了，后续的请求过来直接不接收了，拒绝访问，10分钟之后再尝试去看看mysql恢复没有

降级：mysql挂了，系统发现了，自动降级，从内存里存的少量数据中，去提取一些数据出来

运维监控：监控+报警+优化，各种异常的情况，有问题就及时报警，优化一些系统的配置和参数，或者代码

3、如何讲解这块内容？

（1）如何将eshop-cache，核心的缓存服务改造成高可用的架构

（2）hystrix中的一部分内容，单拉出来，做成一个免费的小课程，作为福利发放出去

（3）eshop-cache，写代码，eshop-cache-ha，业务场景，跟之前衔接起来，重新去写代码

（4）hystrix做服务高可用这一块的内容，讲解成只有一个业务背景，重新写代码，独立


eshop-cache，在各级缓存数据都失效的情况下，会重新从源系统中调用接口，依赖源系统去查询mysql数据库去重新获取数据

如果你的各种依赖的服务有了故障，那么很可能会导致你的系统不可用

hystrix对系统进行各种高可用性的系统加固，来应对各种不可用的情况


缓存雪崩那一块去讲解，redis肯定挂，mysql有较大概率挂掉，在风雨飘摇中

我之前做的一个项目，我们多个项目都用了公司里公用的缓存的存储，缓存彻底挂了，雪崩了，导致各种业务系统全部崩溃，崩溃了好几个小时

导致公司损失了大量的资金的损失

其中导致公司损失最大的负责人，受到了很大的处分






















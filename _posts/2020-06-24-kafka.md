---
title: Kafka（1）
categories:
- Kafka
tags:
- Kafka
---

## Kafka术语

- Apache Kafka 是消息引擎系统，也是一个分布式流处理平台

- 生产者和消费者统称为客户端（Clients）

- Kafka 的服务器端由被称为 Broker 的服务进程构成，即一个 Kafka 集群由多个 Broker 组成，Broker 负责接收和处理客户端发送过来的请求，以及对消息进行持久化。

  - 虽然多个 Broker 进程能够运行在同一台机器上，但更常见的做法是将不同的 Broker 分散运行在不同的机器上，这样如果集群中某一台机器宕机，即使在它上面运行的所有 Broker 进程都挂掉了，其他机器上的 Broker 也依然能够对外提供服务。这其实就是 Kafka 提供高可用的手段之一。

- 实现高可用的另一个手段就是备份机制（Replication）。

  - Kafka 定义了两类副本：领导者副本（Leader Replica）和追随者副本（Follower Replica）。前者对外提供服务，这里的对外指的是与客户端程序进行交互；而后者只是被动地追随领导者副本而已，不能与外界进行交互。
  - 副本的工作机制也很简单：生产者总是向领导者副本写消息；而消费者总是从领导者副本读消息。

- Kafka 的三层消息架构

  - 第一层是主题层，每个主题可以配置 M 个分区，而每个分区又可以配置 N 个副本。
  - 第二层是分区层，每个分区的 N 个副本中只能有一个充当领导者角色，对外提供服务；其他 N-1 个副本是追随者副本，只是提供数据冗余之用
  - 第三层是消息层，分区中包含若干条消息，每条消息的位移从 0 开始，依次递增。
  - 最后，客户端程序只能与分区的领导者副本进行交互。

- Kafka Broker 是如何持久化数据的。

  - 总的来说，Kafka 使用消息日志（Log）来保存数据，一个日志就是磁盘上一个只能追加写（Append-only）消息的物理文件。因为只能追加写入，故避免了缓慢的随机 I/O 操作，改为性能较好的顺序I/O 写操作，这也是实现 Kafka 高吞吐量特性的一个重要手段。
  - 日志段（Log Segment）机制。在 Kafka 底层，一个日志又近一步细分成多个日志段，消息被追加写到当前最新的日志段中

- 为什么kafka不支持主从分离

  - 对于那种读操作很多而写操作相对不频繁的负载类型而言，采用读写分离是非常不错的方案——我们可以添加很多follower横向扩展，提升读操作性能。反观Kafka，它的主要场景还是在消息引擎而不是以数据存储的方式对外提供读服务，通常涉及频繁地生产消息和消费消息，这不属于典型的读多写少场景，因此读写分离方案在这个场景下并不太适合。

  - Kafka中的领导者副本一般均匀分布在不同的broker中，已经起到了负载的作用。即：同一个topic的已经通过分区的形式负载到不同的broker上了，读写的时候针对的领导者副本

    





##  Kafka线上集群部署方案

- Kafka 客户端底层使用了 Java的 selector，selector 在 Linux 上的实现机制是 epoll。将 Kafka 部署在 Linux 上是有优势的，因为能够获得更高效的I/O 性能。
- 在 Linux 部署 Kafka 能够享受到零拷贝技术所带来的快速数据传输特性
- 假设你所在公司有个业务每天需要向Kafka 集群发送 1 亿条消息，每条消息保存两份以防止数据丢失，另外消息默认保存两周时间。现在假设消息的平均大小是 1KB，那么你能说出你的 Kafka 集群需要为这个业务预留多少磁盘空间吗？
  - 我们来计算一下：每天 1 亿条 1KB 大小的消息，保存两份且留存两周的时间，那么总的空间大小就等于 1 亿 * 1KB * 2 / 1000 / 1000 = 200GB。一般情况下 Kafka 集群除了消息数据还有其他类型的数据，比如索引数据等，故我们再为这些数据预留出 10% 的磁盘空间，因此总的存储容量就是 220GB。既然要保存两周，那么整体容量即为 220GB * 14，大约 3TB 左右。Kafka 支持数据的压缩，假设压缩比是 0.75，那么最后你需要规划的存储空间就是 0.75 * 3 = 2.25TB。
- 真正要规划的是所需的 Kafka 服务器的数量。
  - 假设你公司的机房环境是千兆网络，即 1Gbps，现在你有个业务，其业务目标或 SLA 是在 1 小时内处理 1TB 的业务数据。那么问题来了，你到底需要多少台 Kafka 服务器来完成这个业务呢？
    - 通常情况下你只能假设 Kafka 会用到 70% 的带宽资源，因为总要为其他应用或进程留一些资源
    - 这只是它能使用的最大带宽资源，你不能让 Kafka 服务器常规性使用这么多资源，故通常要再额外预留出 2/3 的资源，即单台服务器使用带宽 700Mb / 3 ≈ 240Mbps。
    - 有了 240Mbps，我们就可以计算 1 小时内处理 1TB 数据所需的服务器数量了。根据这个目标，我们每秒需要处理 2336Mb 的数据，除以 240，约等于 10 台服务器。如果消息还需要额外复制两份，那么总的服务器台数还要乘以 3，即 30 台。







## 集群参数配置

- 针对存储信息的重要参数
  - log.dirs：这是非常重要的参数，指定了 Broker 需要使用的若干个文件目录路径。要知道这个参数是没有默认值的
  - 如果有条件的话你最好保证这些目录挂载到不同的物理磁盘上。
    - 提升读写性能
    - 能够实现故障转移：即 Failover。

-  ZooKeeper 相关的设置

  - 分布式协调框架，负责协调管理并保存 Kafka 集群的所有元数据信息

  - Kafka 与 ZooKeeper 相关的最重要的参数当属zookeeper.connect

  - > 如果你有两套 Kafka 集群，假设分别叫它们 kafka1 和kafka2，那么两套集群的zookeeper.connect参数可以这样指定：zk1:2181,zk2:2181,zk3:2181/kafka1和zk1:2181,zk2:2181,zk3:2181/kafka2。

-  Broker 连接相关
  - 客户端程序或其他Broker 如何与该 Broker 进行通信的设置
  - 监听器的概念，从构成上来说，它是若干个逗号分隔的三元组，每个三元组的格式为<协议名称，主机名，端口号>
  - advertised.listeners
    - 你的Kafka Broker机器上配置了双网卡，一块网卡用于内网访问（即我们常说的内网IP）；另一个块用于外网访问。那么你可以配置listeners为内网IP，advertised.listeners为外网IP。

- Topic 管理
  - auto.create.topics.enable参数我建议最好设置成false，即不允许自动创建 Topic。在我们的线上环境里面有很多名字稀奇古怪的 Topic，我想大概都是因为该参数被设置成了 true 的缘故。
  - unclean.leader.election.enable：是否允许定期进行 Leader 选举。
    - 这个参数在最新版的 Kafka 中默认就是 false
  - auto.leader.rebalance.enable
    - true 表示允许 Kafka 定期地对一些 Topic 分区进行Leader 重选举，当然这个重选举不是无脑进行的，它要满足一定的条件才会发生。严格来说它与上一个参数中Leader 选举的最大不同在于，它不是选 Leader，而是换Leader
    - 这种换 Leader本质上没有任何性能收益，因此我建议你在生产环境中把这个参数设置成 false。
- 数据留存
  - log.retention.{hour|minutes|ms}：这是个“三
    兄弟”，都是控制一条消息数据被保存多长时间。从优先级上来说 ms 设置最高、minutes 次之、hour 最低。
  - log.retention.bytes：这是指定 Broker 为消息保存的总磁盘容量大小。
  - max.message.bytes：控制 Broker 能够接收的最大消息大小。
    - 默认的 1000012 太少了，还不到 1MB。实际场景中突破 1MB 的消息都是屡见不鲜的，因此在线上环境中设置一个比较大的值还是比较保险的做法。毕竟它只是一个标尺而已，仅仅衡量 Broker 能够处理的最大消息大小，即使设置大一点也不会耗费什么磁盘空间的。
- Topic 级别参数
  - Topic 级别参数会覆盖全局Broker 参数的值，而每个 Topic 都能设置自己的参数值，这就是所谓的 Topic 级别参数。
  - 保存消息
    - retention.ms：规定了该 Topic 消息被保存的时长。
    - retention.bytes：规定了要为该 Topic 预留多大的磁盘空间。

- 将你的 JVM 堆大小设置成 6GB 吧，这是目前业界比较公认的一个合理值。
- 操作系统参数
  - 文件描述符限制
    - 通常情况下将它设置成一个超大的值是合理的做法，比如ulimit -n 1000000
  - 文件系统类型
    - 生产环境最好还是使用 XFS。
  - Swappiness
    - 网上很多文章都提到设置其为 0，将
      swap 完全禁掉以防止 Kafka 进程使用 swap 空间。我个人反倒觉得还是不要设置成 0 比较好，我们可以设置成一个较小的值。为什么呢？因为一旦设置成 0，当物理内存耗尽时，操作系统会触发 OOM killer 这个组件，它会随机挑选一个进程然后 kill 掉，即根本不给用户任何的预警。
    - 但如果设置成一个比较小的值，当开始使用 swap 空间时，你至少能够观测到 Broker 性能开始出现急剧下降，从而给你进一步调优和诊断问题的时间。基于这个考虑，我个人建议将swappniess 配置成一个接近 0 但不为 0 的值，比如 1。
  - 提交时间
    - 向 Kafka 发送数据并不是真要等数据被写入磁盘才会认为成功，而是只要数据被写入到操作系统的页缓存（Page Cache）上就可以了，随后操作系统根据 LRU 算法会定期将页缓存上的“脏”数据落盘到物理磁盘上。这个定期就是由提交时间来确定的，默认是 5 秒。







## 生产者消息分区机制原理

- 其实分区的作用就是提供负载均衡的能力，或者说对数据进行分区的主要原因，就是为了实现系统的高伸缩性（Scalability）。
  - 不同的分区能够被放置到不同节点的机器上，而数据的读写操作也都是针对分区这个粒度而进行的
- 所谓分区策略是决定生产者将消息发送到哪个分区的算法。
- kafka同一个topic是无法保证数据的顺序性的，但是同一个partition中的数据是有顺序的





## 生产者压缩算法

- Kafka 的消息层次都分为两层：消息集合（message set）以及消息（message）。一个消息集合中包含若干条日志项（record item），而日志项才是真正封装消息的地方。
  - Kafka 通常不会直接操作具体的一条条消息，它总是在消息集合这个层面上进行写入操作。
  - V2 版本的做法是对整个消息集合进行压缩。
- 在 Kafka 中，压缩可能发生在两个地方：生产者端和 Broker 端。
  - 生产者程序中配置 compression.type 参数即表示启用指定类型的压缩算法
  - 其实大部分情况下 Broker 从 Producer 端接收到消息后仅仅是原封不动地保存而不会对其进行任何修改，但这里的“大部分情况”也是要满足一定条件的。有两种例外情况就可能让Broker 重新压缩消息。
    - Broker 端指定了和 Producer 端不同的压缩算法
    - Broker 端发生了消息格式转换
      - 所谓的消息格式转换主要是为了兼容老版本的消费者程序。
      - 一般情况下这种消息格式转换对性能是有很大影响的，除了这里的压缩之外，它还让 Kafka 丧失了引以为豪的 Zero Copy 特性。
  - 每个压缩过的消息集合在 Broker 端写入时都要发生解压缩操作，目的就是为了对消息执行各种验证。这和前面提到消息格式转换时发生的解压缩是不同的场景。
- 从 2.1.0 开始，Kafka 正式支持 Zstandard 算法（简写为 zstd）。它是 Facebook 开源的一个压缩算法，能够提供超高的压缩比（compression ratio）。
  - 这年头，带宽可是比CPU 和内存还要珍贵的稀缺资源，毕竟万兆网络还不是普通公司的标配，因此千兆网络中Kafka 集群带宽资源耗尽这件事情就特别容易出现。如果你的客户端机器 CPU 资源有很多富余，我强烈建议你开启 zstd 压缩，这样能极大地节省网络资源消耗。







## 无消息丢失配置

- 一句话概括，Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证。
  - Kafka 不丢消息是有前提条件的。假如你的消息保存在 N 个 Kafka Broker 上，那么这个前提条件就是这 N个 Broker 中至少有 1 个存活。只要这个条件成立，Kafka 就能保证你的这条消息永远不会丢失。
- “消息丢失”案例
  - 案例 1：生产者程序丢失数据
    - 目前 Kafka Producer 是异步发送消息的，也就是说如果你调用的是 producer.send(msg)这个 API，那么它通常会立即返回，但此时你不能认为消息发送已成功完成。
    - Producer 永远要使用带有回调通知的发送 API，不要使用producer.send(msg)，而要使用 producer.send(msg, callback)。不要小瞧这里的callback（回调），它能准确地告诉你消息是否真的提交成功了。
  - 案例 2：消费者程序丢失数据
    - Consumer 端丢失数据主要体现在 Consumer 端要消费的消息不见了。
    - 要对抗这种消息丢失，办法很简单：维持先消费消息（阅读），再更新位移（书签）的顺序即可
    - 当然，这种处理方式可能带来的问题是消息的重复处理，类似于同一页书被读了很多遍，但这不属于消息丢失的情形。

- 最佳实践

  - 不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。记住，一定要使用带有回调通知的 send 方法。
  - 设置 acks = all。acks 是 Producer 的一个参数，代表了你对“已提交”消息的定义。如果设置成 all，则表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。这是最高等级的“已提交”定义。
  - 设置 retries 为一个较大的值。这里的 retries 同样是 Producer 的参数，对应前面提到的 Producer 自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了retries > 0 的 Producer 能够自动重试消息发送，避免消息丢失。
  - 设置 unclean.leader.election.enable = false。这是 Broker 端的参数，它控制的是哪些 Broker 有资格竞选分区的 Leader。如果一个 Broker 落后原先的 Leader 太多，那么它一旦成为新的 Leader，必然会造成消息的丢失。故一般都要将该参数设置成 false，即不允许这种情况的发生。
  - 设置 replication.factor >= 3。这也是 Broker 端的参数。其实这里想表述的是，最好将消息多保存几份，毕竟目前防止消息丢失的主要机制就是冗余
  - 设置 min.insync.replicas > 1。这依然是Broker 端参数，控制的是消息至少要被写入到多少个副本才算是“已提交”。设置成大于 1 可以提升消息持久性。在实际环境中千万不要使用默认值 1。
    - 推荐设置成 replication.factor = min.insync.replicas +1。
  - 确保消息消费完成再提交。Consumer 端有个参数 enable.auto.commit，最好把它设置成 false，并采用手动提交位移的方式。就像前面说的，这对于单 Consumer 多线程处理的场景而言是至关重要的

  




















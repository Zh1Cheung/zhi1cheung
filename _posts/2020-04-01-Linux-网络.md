---
title: Linux-网络
categories:
- Linux
tags:
- Linux

---







## **怎么评估系统的网络性能**

- **性能指标回顾**

  - 首先，**带宽**，表示链路的最大传输速率，单位是 b/s（比特 / 秒）。在你为服务器选购网卡 时，带宽就是最核心的参考指标。常用的带宽有 1000M、10G、40G、100G 等。 
  - 第二，**吞吐量**，表示没有丢包时的最大数据传输速率，单位通常为 b/s （比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽的限制，吞吐量 / 带宽也就是该网络链路的使用率。 
  - 第三，**延时**，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。这个指标 在不同场景中可能会有不同的含义。它可以表示建立连接需要的时间（比如 TCP 握手延时），或者一个数据包往返所需时间（比如 RTT）。 
  - 最后，**PPS**，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速 率。PPS 通常用来评估网络的转发能力，而基于 Linux 服务器的转发，很容易受到网络包 大小的影响（交换机通常不会受到太大影响，即交换机可以线性转发）。 

- **网络基准测试**

  - 基于 HTTP 或者 HTTPS 的 Web 应用程序，显然属于应用层，需要我们测试 HTTP/HTTPS 的性能；
  - 而对大多数游戏服务器来说，为了支持更大的同时在线人数，通常会基于 TCP 或 UDP ，与客户端进行交互，这时就需要我们测试 TCP/UDP 的性能

- **各协议层的性能测试**

  - 转发性能

    - hping3 作为一个 SYN 攻击的工具来使用

    - Linux 内核自带的高性能网络测试工具 pktgen。 pktgen 支持丰富的自定义选项，方便你根据实际需要构造所需网络包，从而更准确地测试 出目标服务器的性能。

    - 不过，在 Linux 系统中，你并不能直接找到 pktgen 命令。因为 pktgen 作为一个内核线 程来运行，需要你加载 pktgen 内核模块后，再通过 /proc 文件系统来交互。

    - ```shell
      1 $ modprobe pktgen
      2 $ ps -ef | grep pktgen | grep -v grep
      
      5 $ ls /proc/net/pktgen/
      
      
      ```

    - pktgen 在每个 CPU 上启动一个内核线程，并可以通过 /proc/net/pktgen 下面的同名文 件，跟这些线程交互；而 pgctrl 则主要用来控制这次测试的开启和停止。

  - $ cat /proc/net/pktgen/eth0

    - 你可以看到，测试报告主要分为三个部分： 
      - 第一部分的 Params 是测试选项； 
      - 第二部分的 Current 是测试进度，其中， packts so far（pkts-sofar）表示已经发送了 100 万个包，也就表明测试已完成。 
      - 第三部分的 Result 是测试结果，包含测试所用时间、网络包数量和分片、PPS、吞吐量 以及错误数。
      - PPS 为 12 万
        - 作为对比，你可以计算一下千兆交换机的 PPS。交换机可以达到线速（满负载时，无差错 转发），它的 PPS 就是 1000Mbit 除以以太网帧的大小，即 1000Mbps/((64+20)*8bit) = 1.5 Mpps（其中，20B 为以太网帧前导和帧间距的大小）。 你看，即使是千兆交换机的 PPS，也可以达到 150 万 PPS，比我们测试得到的 12 万大多 了。

  - TCP/UDP 性能

    - 相应的测试工具，比如 iperf 或者 netperf

      - 特别是现在的云计算时代，在你刚拿到一批虚拟机时，首先要做的，应该就是用 iperf ，测 试一下网络性能是否符合预期。 iperf 和 netperf 都是最常用的网络性能测试工具，测试 TCP 和 UDP 的吞吐量。它们都以 客户端和服务器通信的方式，测试一段时间内的平均吞吐量。

    - ```shell
      1 # -s 表示启动服务端，-i 表示汇报间隔，-p 表示监听端口
      2 $ iperf3 -s -i 1 -p 10000
      ```

    - ```shell
      # 接着，在另一台机器上运行 iperf 客户端，运行测试
      1 # -c 表示启动客户端，192.168.0.30 为目标服务器的 IP
      2 # -b 表示目标带宽 (单位是 bits/s)
      3 # -t 表示测试时间
      4 # -P 表示并发数，-p 表示目标服务器监听端口
      5 $ iperf3 -c 192.168.0.30 -b 1G -t 15 -P 2 -p 10000
      ```

    - 稍等一会儿（15 秒）测试结束后，回到目标服务器，查看 iperf 的报告

      - 最后的 SUM 行就是测试的汇总结果，包括测试时间、数据传输量以及带宽等。按照发送和 接收，这一部分又分为了 sender 和 receiver 两行。 从测试结果你可以看到，这台机器 TCP 接收的带宽（吞吐量）为 860 Mb/s， 跟目标的 1Gb/s 相比，还是有些差距的。 

  - HTTP 性能

    - 从传输层再往上，到了应用层。有的应用程序，会直接基于 TCP 或 UDP 构建服务。当 然，也有大量的应用，基于应用层的协议来构建服务，HTTP 就是最常用的一个应用层协 议。比如，常用的 Apache、Nginx 等各种 Web 服务，都是基于 HTTP。

    - 要测试 HTTP 的性能，也有大量的工具可以使用，比如 ab、webbench 等，都是常用的 HTTP 压力测试工具。其中，ab 是 Apache 自带的 HTTP 压测工具，主要测试 HTTP 服务 的每秒请求数、请求延迟、吞吐量以及请求延迟的分布情况等。

      - ```shell
        1 # -c 表示并发请求数为 1000，-n 表示总的请求数为 10000
        2 $ ab -c 1000 -n 10000 http://192.168.0.30/
        ```

      - 可以看到，ab 的测试结果分为三个部分，分别是请求汇总、连接时间汇总还有请求延迟汇 总。以上面的结果为例，我们具体来看。

        - **Requests per second** 为 1074； 

          每个请求的延迟（Time per request）分为两行，第一行的 927 ms 表示平均延迟，包 括了线程运行的调度时间和网络请求响应时间，而下一行的 0.927ms ，则表示实际请求 的响应时间； 

        - **Transfer rate 表示吞吐量（BPS）**为 890 KB/s。 

          连接时间汇总部分，则是分别展示了建立连接、请求、等待以及汇总等的各类时间，包括最 小、最大、平均以及中值处理时间。 

        - 最后的**请求延迟汇总部分**，则给出了不同时间段内处理请求的百分比，比如， 90% 的请 求，都可以在 274ms 内完成。

  - 应用负载性能

    - 当你用 iperf 或者 ab 等测试工具，得到 TCP、HTTP 等的性能数据后，这些数据是否就能 表示应用程序的实际性能呢？我想，你的答案应该是否定的。

      - 比如，你的应用程序基于 HTTP 协议，为最终用户提供一个 Web 服务。这时，使用 ab 工 具，可以得到某个页面的访问性能，但这个结果跟用户的实际请求，很可能不一致。因为用户请求往往会附带着各种各种的负载（payload），而这些负载会影响 Web 应用程序内部 的处理逻辑，从而影响最终性能。

    - 幸运的是，我们还可以用 wrk、TCPCopy、Jmeter 或 者 LoadRunner 等实现这个目标。

      - wrk 的命令行参数比较简单。比如，我们可以用 wrk ，来重新测一下前面已经启动的 Nginx 的性能。

        - ```shell
          1 # -c 表示并发连接数 1000，-t 表示线程数为 2
          2 $ wrk -c 1000 -t 2 http://192.168.0.30/
          ```

        - 你可以看到，每秒请求 数为 9641，吞吐量为 7.82MB，平均延迟为 65ms，比前面 ab 的测试结果要好很多。

        



## 网络性能优化的几个思路

- **确定优化目标**

  - 实际上，虽然网络性能优化的整体目标，是降低网络延迟（如 RTT）和提高吞吐量（如 BPS 和 PPS），但具体到不同应用中，每个指标的优化标准可能会不同，优先级顺序也大 相径庭。 
    - NAT 网关来说，由于其直接影响整个数据中心的网络出入性能，所以 NAT 网关通常需要达到或接近线性转发，也就是说， PPS 是最主要的性能目标。 
    - 再如，对于数据库、缓存等系统，快速完成网络收发，即低延迟，是主要的性能目标。
    - 而对于我们经常访问的 Web 服务来说，则需要同时兼顾

- Linux 网络协议栈

  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105407329-759102076.png)
  - **首先是网络接口层和网络层**，它们主要负责网络包的封装、寻址、路由，以及发送和接收。 每秒可处理的网络包数 PPS，就是它们最重要的性能指标（特别是在小包的情况下）。你可以用内核自带的发包工具 pktgen ，来测试 PPS 的性能。
  - **再向上到传输层的 TCP 和 UDP**，它们主要负责网络传输。对它们而言，吞吐量（BPS）、 连接数以及延迟，就是最重要的性能指标。你可以用 iperf 或 netperf ，来测试传输层的性 能。 
    - 不过要注意，网络包的大小，会直接影响这些指标的值。所以，通常，你需要测试一系列不 同大小网络包的性能。
  - 最后，再往上到了应用层，最需要关注的是吞吐量（BPS）、每秒请求数以及延迟等指标。 你可以用 wrk、ab 等工具，来测试应用程序的性能

- 网络性能工具

  - 网络性能指标的工具
    - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105438150-732021018.png)
    - 性能工具
      - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105457114-2056652101.png)

- 应用程序

  - 从网络 I/O 的角度来说，主要有下面两种优化思路

    - 从网络 I/O 的角度来说，主要有下面两种优化思路。 第一种是最常用的 I/O 多路复用技术 epoll，主要用来取代 select 和 poll。这其实是解决 C10K 问题的关键，也是目前很多网络应用默认使用的机制。
    - 第二种是使用异步 I/O（Asynchronous I/O，AIO）。AIO 允许应用程序同时发起很多 I/O 操作，而不用等待这些操作完成。等到 I/O 完成后，系统会用事件通知的方式，告诉应 用程序结果。不过，AIO 的使用比较复杂，你需要小心处理很多边缘情况

  - 而从进程的工作模型来说，也有两种不同的模型用来优化

    - 第一种，主进程 + 多个 worker 子进程。其中，主进程负责管理网络连接，而子进程负责 实际的业务处理。这也是最常用的一种模型。 

      第二种，监听到相同端口的多进程模型。在这种模型下，所有进程都会监听相同接口，并且 开启 SO_REUSEPORT 选项，由内核负责，把请求负载均衡到这些监听进程中去。 

  - 应用层的网络协议优化

    - 使用长连接取代短连接，可以显著降低 TCP 建立连接的成本。在每秒请求次数较多时， 这样做的效果非常明显。 
    - 使用内存等方式，来缓存不常变化的数据，可以降低网络 I/O 次数，同时加快应用程序 的响应速度。 
    - 使用 Protocol Buffer 等序列化的方式，压缩网络 I/O 的数据量，可以提高应用程序的吞 吐。 
    - 使用 DNS 缓存、预取、HTTPDNS 等方式，减少 DNS 解析的延迟，也可以提升网络 I/O 的整体速度

- 套接字

  - 套接字可以屏蔽掉 Linux 内核中不同协议的差异，为应用程序提供统一的访问接口。每个 套接字，都有一个读写缓冲区。 

    - 读缓冲区，缓存了远端发过来的数据。如果读缓冲区已满，就不能再接收新的数据。 
    - 写缓冲区，缓存了要发出去的数据。如果写缓冲区已满，应用程序的写操作就会被阻塞。

  - > 增大每个套接字的缓冲区大小 net.core.optmem_max； 
    >
    > 增大套接字接收缓冲区大小 net.core.rmem_max 和发送缓冲区大小 
    >
    > net.core.wmem_max； 
    >
    > 增大 TCP 接收缓冲区大小 net.ipv4.tcp_rmem 和发送缓冲区大小 
    >
    > net.ipv4.tcp_wmem。
    >
    > 
    >
    > 发送缓冲区大小，理想数值是吞吐量 * 延迟，这样才可以达到最大网络利用 率

- 传输层

  - 第一类，在请求数比较大的场景下，你可能会看到大量处于 TIME_WAIT 状态的连接，它 们会占用大量内存和端口资源。这时，我们可以优化与 TIME_WAIT 状态相关的内核选 项，比如采取下面几种措施。
    - 增大处于 TIME_WAIT 状态的连接数量 net.ipv4.tcp_max_tw_buckets ，并增大连接跟踪表的大小 net.netfilter.nf_conntrack_max。 
    - 减小 net.ipv4.tcp_fin_timeout 和 net.netfilter.nf_conntrack_tcp_timeout_time_wait ，让系统尽快释放它们所占用的资源。 
    - 开启端口复用 net.ipv4.tcp_tw_reuse。这样，被 TIME_WAIT 状态占用的端口，还能用 到新建的连接中。 
    - 增大本地端口的范围 net.ipv4.ip_local_port_range 。这样就可以支持更多连接，提高整 体的并发能力。 
    - 增加最大文件描述符的数量。你可以使用 fs.nr_open 和 fs.file-max ，分别增大进程和 
    - 系统的最大文件描述符数；或在应用程序的 systemd 配置文件中，配置 LimitNOFILE ，设置应用程序的最大文件描述符数。
  - 第二类，为了缓解 SYN FLOOD 等，利用 TCP 协议特点进行攻击而引发的性能问题，你可 以考虑优化与 SYN 状态相关的内核选项，比如采取下面几种措施。 
    - 增大 TCP 半连接的最大数量 net.ipv4.tcp_max_syn_backlog ，或者开启 TCP SYN Cookies net.ipv4.tcp_syncookies ，来绕开半连接数量限制的问题（注意，这两个选项 不可同时使用）。 
    - 减少 SYN_RECV 状态的连接重传 SYN+ACK 包的次数 net.ipv4.tcp_synack_retries。 
  - 第三类，在长连接的场景中，通常使用 Keepalive 来检测 TCP 连接的状态，以便对端连接 断开后，可以自动回收。但是，系统默认的 Keepalive 探测间隔和重试次数，一般都无法 满足应用程序的性能要求。所以，这时候你需要优化与 Keepalive 相关的内核选项，比 如
    - 缩短最后一次数据包到 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_time；缩短发送 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_intvl； 
    - 减少 Keepalive 探测失败后，一直到通知应用程序前的重试次数net.ipv4.tcp_keepalive_probes。 
  - UDP 的优化
    - UDP 提供了面向数据报的网络协议，它不需要网络连接，也不提供可靠性保障。所以， UDP 优化，相对于 TCP 来说，要简单得多。这里我也总结了常见的几种优化方案。 
      - 跟上篇套接字部分提到的一样，增大套接字缓冲区大小以及 UDP 缓冲区范围； 
      - 跟前面 TCP 部分提到的一样，增大本地端口号的范围； 
      - 根据 MTU 大小，调整 UDP 数据包的大小，减少或者避免分片的发生。

- 网络层

  - 网络层，负责网络包的封装、寻址和路由，包括 IP、ICMP 等常见协议。在网络层，最主 要的优化，其实就是对路由、 IP 分片以及 ICMP 等进行调优。 
  - 第一种，从路由和转发的角度出发，你可以调整下面的内核选项。 
    - 在需要转发的服务器中，比如用作 NAT 网关的服务器或者使用 Docker 容器时，开启 IP 转发，即设置 net.ipv4.ip_forward = 1。 
    - 调整数据包的生存周期 TTL，比如设置 net.ipv4.ip_default_ttl = 64。注意，增大该值会 降低系统性能。 
    - 开启数据包的反向地址校验，比如设置 net.ipv4.conf.eth0.rp_filter = 1。这样可以防止 IP 欺骗，并减少伪造 IP 带来的 DDoS 问题。
  - 第二种，从分片的角度出发，最主要的是调整 MTU（Maximum Transmission Unit）的 大小。 
    - 通常，MTU 的大小应该根据以太网的标准来设置。以太网标准规定，一个网络帧最大为 1518B，那么去掉以太网头部的 18B 后，剩余的 1500 就是以太网 MTU 的大小。在使用 VXLAN、GRE 等叠加网络技术时，要注意，网络叠加会使原来的网络包变大，导致 MTU 也需要调整。 
    - 比如，就以 VXLAN 为例，它在原来报文的基础上，增加了 14B 的以太网头部、 8B 的 VXLAN 头部、8B 的 UDP 头部以及 20B 的 IP 头部。换句话说，每个包比原来增大了 50B。 
    - 所以，我们就需要把交换机、路由器等的 MTU，增大到 1550， 或者把 VXLAN 封包前 （比如虚拟化环境中的虚拟网卡）的 MTU 减小为 1450。
  - 第三种，从 ICMP 的角度出发，为了避免 ICMP 主机探测、ICMP Flood 等各种网络问 题，你可以通过内核选项，来限制 ICMP 的行为。 
    - 比如，你可以禁止 ICMP 协议，即设置 net.ipv4.icmp_echo_ignore_all = 1。这样，外 部主机就无法通过 ICMP 来探测主机。 
    - 或者，你还可以禁止广播 ICMP，即设置 net.ipv4.icmp_echo_ignore_broadcasts = 1。



## 服务吞吐量下降很厉害，怎么分析

- 测试一下，案例中 Nginx 的吞吐量。

  - ```shell
    1 # 默认测试时间为 10s，请求超时 2s
    2 $ wrk --latency -c 1000 http://192.168.0.30
    
     1910 requests in 10.10s, 573.56KB read
     Non-2xx or 3xx responses: 1910
    ```

  - 从 wrk 的结果中，你可以看到吞吐量（也就是每秒请求数）只有 189，并且所有 1910 个 请求收到的都是异常响应（非 2xx 或 3xx）。这些数据显然表明，吞吐量太低了，并且请 求处理都失败了。这是怎么回事呢？

  - 根据 wrk 输出的统计结果，我们可以看到，总共传输的数据量只有 573 KB，那就肯定不会 是带宽受限导致的。所以，我们应该从请求数的角度来分析。 分析请求数，特别是 HTTP 的请求数，有什么好思路吗？当然就要**从 TCP 连接数入手**。

- 连接数优化

  - 要查看 TCP 连接数的汇总情况，首选工具自然是 ss 命令。为了观察 wrk 测试时发生的问 题，我们在终端二中再次启动 wrk，并且把总的测试时间延长到 30 分钟

    - ```shell
      # 测试时间 30 分钟
      $ wrk --latency -c 1000 -d 1800 http://192.168.0.30
      
      #观察 TCP 连接数
      $ ss -s
      ```

    - 从这里看出，wrk 并发 1000 请求时，建立连接数只有 5，而 closed 和 timewait 状态的 连接则有 1100 多 。其实从这儿你就可以发现两个问题

      - 一个是建立连接数太少了； 
      - 另一个是 timewait 状态连接太多了

    - 分析问题，自然要先从相对简单的下手。我们先来看第二个关于 timewait 的问题。在之前 的 NAT 案例中，我已经提到过，内核中的连接跟踪模块，有可能会导致 timewait 问题。 我们今天的案例还是基于 Docker 运行，而 Docker 使用的 iptables ，就会使用**连接跟踪 模块**来管理 NAT。那么，怎么确认是不是连接跟踪导致的问题呢？

      - 其实，最简单的方法，就是通过 dmesg 查看系统日志，如果有连接跟踪出了问题，应该会 看到 nf_conntrack 相关的日志

        - $ dmesg | tail

        - 从日志中，你可以看到 nf_conntrack: table full, dropping packet 的错误日志。这说明， 正是连接跟踪导致的问题。 

        - 这种情况下，我们应该想起前面学过的两个内核选项——连接跟踪数的最大限制 nf_conntrack_max ，以及当前的连接跟踪数 nf_conntrack_count。

        - 这次的输出中，你可以看到最大的连接跟踪限制只有 200，并且全部被占用了。200 的限 制显然太小，不过相应的优化也很简单，调大就可以了。

        - ```shell
          # 我们执行下面的命令，将 nf_conntrack_max 增大：
          # 将连接跟踪限制增大到 1048576
           $ sysctl -w net.netfilter.nf_conntrack_max=1048576
          
          ```

      - 从 wrk 的输出中，你可以看到，连接跟踪的优化效果非常好，吞吐量已经从刚才的 189 增 大到了 5382。看起来性能提升了将近 30 倍

        - 别急，我们再来看看 wrk 汇报的其他数据。果然，10s 内的总请求数虽然增大到了 5 万， 但是有 4 万多响应异常，说白了，真正成功的只有 8000 多个（54221-45577=8644）

- 工作进程优化

  - 由于这些响应并非 Socket error，说明 Nginx 已经收到了请求，只不过，响应的状态码并 不是我们期望的 2xx （表示成功）或 3xx（表示重定向）。所以，这种情况下，搞清楚 Nginx 真正的响应就很重要了。 

    - ```shell
      $ docker logs nginx --tail 3
      
      
      ```

    - 从 Nginx 的日志中，我们可以看到，响应状态码为 499。499 并非标准的 HTTP 状态码，而是由 Nginx 扩展而来，表示服务器端还没来得及响应 时，客户端就已经关闭连接了。换句话说，问题在于服务器端处理太慢，客户端因为超时 （wrk 超时时间为 2s），主动断开了连接。

    - 既然问题出在了服务器端处理慢，而案例本身是 Nginx+PHP 的应用，那是不是可以猜 测，是因为 PHP 处理过慢呢

    - ```shell
      # 查询 PHP 容器日志
      $ docker logs phpfpm --tail 5
      
      
      ```

    - 从这个日志中，我们可以看到两条警告信息，server reached max_children setting (5)， 并建议增大 max_children。

      - 一般来说，每个 php-fpm 子进程可能会占用 20 MB 左右的内存。所以，你可以根据内存 和 CPU 个数，估算一个合理的值。这儿我把它设置成了 20，并将优化后的配置重新打包 成了 Docker 镜像。

      - ```shell
        # 停止旧的容器
        $ docker rm -f nginx phpfpm
        # 使用新镜像启动 Nginx 和 PHP
        $ docker run --name nginx --network host --privileged -itd feisky/nginx-tp:1
        $ docker run --name phpfpm --network host --privileged -itd feisky/php-fpm-tp:1
        
        
        ```

    - 从 wrk 的输出中，可以看到，虽然吞吐量只有 4683，比刚才的 5382 少了一些；但是测试 期间成功的请求数却多了不少，从原来的 8000，增长到了 15000（47210- 31692=15518）。 

- 套接字优化

  - 观察有没有发生套接字的丢包现象

    - ```shell
      1 # 只关注套接字统计
      2 $ netstat -s | grep socket
      
      
      ```

    - 根据两次统计结果中 socket overflowed 和 sockets dropped 的变化，你可以看到，有大 量的套接字丢包，并且丢包都是套接字队列溢出导致的。所以，接下来，我们应该分析连接 队列的大小是不是有异常。

  - 查看套接字的队列大小$ ss -ltnp

    - 这次可以看到，Nginx 和 php-fpm 的监听队列 （Send-Q）只有 10，而 nginx 的当前监 听队列长度 （Recv-Q）已经达到了最大值，php-fpm 也已经接近了最大值。很明显，套 接字监听队列的长度太小了，需要增大

  - 关于套接字监听队列长度的设置，既可以在应用程序中，通过套接字接口调整，也支持通过 内核选项来配置

    - ```shell
      1 # 查询 nginx 监听队列长度配置
      2 $ docker exec nginx cat /etc/nginx/nginx.conf | grep backlog
      3 listen 80 backlog=10;
      45 # 查询 php-fpm 监听队列长度
      6 $ docker exec phpfpm cat /opt/bitnami/php/etc/php-fpm.d/www.conf | grep backlog
      7 ; Set listen(2) backlog.
      8 ;listen.backlog = 511
      9
      10 # somaxconn 是系统级套接字监听队列上限
      11 $ sysctl net.core.somaxconn
      12 net.core.somaxconn = 10
      
      
      ```

    - 从输出中可以看到，Nginx 和 somaxconn 的配置都是 10，而 php-fpm 的配置也只有 511，显然都太小了。那么，优化方法就是增大这三个配置，比如，可以把 Nginx 和 phpfpm 的队列长度增大到 8192，而把 somaxconn 增大到 65536

    - 现在的吞吐量已经增大到了 6185，并且在测试的时候，如果你在终端一中重新执行 netstat -s | grep socket，还会发现，现在已经没有套接字丢包问题了

  - 不过，这次 Nginx 的响应，再一次全部失败了，都是 Non-2xx or 3xx

    - 你可以看到，Nginx 报出了无法连接 fastcgi 的错误，错误消息是 Connect 时， Cannot assign requested address。这个错误消息对应的错误代码为 EADDRNOTAVAIL，表示 IP 地址或者端口号不可用。 
    - 在这里，显然只能是端口号的问题

- 端口号优化

  - 我们执行下面的命令，就可以查询系统配置的临时端口号范围

    - ```shell
      1 $ sysctl net.ipv4.ip_local_port_range
      2 net.ipv4.ip_local_port_range=20000 20050
      
      ```

  - 你可以看到，临时端口的范围只有 50 个，显然太小了 。优化方法很容易想到，增大这个 范围就可以了。比如，你可以执行下面的命令，把端口号范围扩展为 “10000 65535”：

    - ```shell
      1 $ sysctl -w net.ipv4.ip_local_port_range="10000 65535"
      2 net.ipv4.ip_local_port_range = 10000 65535
      
      ```

  - 这次，异常的响应少多了 ，不过，吞吐量也下降到了 3208。并且，这次还出现了很多 Socket read errors。显然，还得进一步优化。

- 火焰图

  - 前面我们已经优化了很多配置。这些配置在优化网络的同时，却也会带来其他资源使用的上 升

  - 执行 top ，观察 CPU 和内存的使用

  - 从 top 的结果中可以看到，可用内存还是很充足的，但系统 CPU 使用率（sy）比较高，两 个 CPU 的系统 CPU 使用率都接近 50%，且空闲 CPU 使用率只有 2%。再看进程部分， CPU 主要被两个 Nginx 进程和两个 docker 相关的进程占用，使用率都是 30% 左右。

    - CPU 使用率上升了，该怎么进行分析呢？我想，你已经还记得我们多次用到的 perf，再配 合前两节讲过的火焰图，很容易就能找到系统中的热点函数。 

    - ```shell
      1 # 执行 perf 记录事件
      2 $ perf record -g
      34 # 切换到 FlameGraph 安装路径执行下面的命令生成火焰图
      
      ```

  - 根据我们讲过的火焰图原理，这个图应该从下往上、沿着调用栈中最宽的函数，来分析执行 次数最多的函数。 

  - 如果有大量连接 占用着端口，那么检查端口号可用的函数，不就会消耗更多的 CPU 吗

    - $ ss -s

    - 这回可以看到，有大量连接（这儿是 32768）处于 timewait 状态，而 timewait 状态的连 接，本身会继续占用端口号。如果这些端口号可以重用，那么自然就可以缩短 __init_check_established 的过程。而 Linux 内核中，恰好有一个 tcp_tw_reuse 选项，用 来控制端口号的重用。

      ```shell
      # 我们在终端一中，运行下面的命令，查询它的配置：
      
      1 $ sysctl net.ipv4.tcp_tw_reuse
      2 net.ipv4.tcp_tw_reuse = 0
      # 你可以看到，tcp_tw_reuse 是 0，也就是禁止状态。其实看到这里，我们就能理解，为什么临时端口号的分配会是系统运行的热点了。当然，优化方法也很容易，把它设置成 1 就可以开启了。
      
      
      ```

      




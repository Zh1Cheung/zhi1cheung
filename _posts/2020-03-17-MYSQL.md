---
title: MYSQL（1）
categories:
- MYSQL
tags:
- MYSQL
---




## 基础架构

- MySQL 可以分为 Server 层和存储引擎层两部分
  - Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能
  - 存储引擎层负责数据的存储和提取，最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。







## 日志系统

- 与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log（重做日志）和 binlog（归档日志）。
- redo log
  - WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘
  - 当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面
  - 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。
- binlog
  - Bin log 用于记录了完整的逻辑记录，所有的逻辑记录在 bin log 里都能找到，所以在备份恢复时，是以 bin log 为基础，通过其记录的完整逻辑操作，备份出一个和原库完整的数据。
  - 这两种日志有以下三点不同
    - redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
    - redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
    - redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。
- 将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是"两阶段提交"。
  - 由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redolog 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。
  - update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？
  -  先写 redo log 后写 binlog
    - redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来
    - 这时候 binlog 里面就没有记录这个语句，之后备份日志的时候，存起来的 binlog 里面就没有这条语句，恢复临时库的话，临时库就会少了这一次更新，恢复出来的这一行字段的值就是 0
  - 先写 binlog 后写 redo log
    - 由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0
    - 但是 binlog 里面已经记录了“把c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。
- 双1
  - redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。
  - sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。







## 事务隔离

- 在 MySQL中，事务支持是在引擎层实现的。

- SQL 标准的事务隔离级别

  - > 读未提交：别人改数据的事务尚未提交，我在我的事务中也能读到。
    > 读已提交：别人改数据的事务已经提交，我在我的事务中才能读到。
    > 可重复读：别人改数据的事务已经提交，我在我的事务中也不去读。

  - > 读未提交”：直接返回记录上的最新值
    > “读提交”：在每个 SQL 语句开始执行的时候创建
    > “可重复读”：在事务启动时创建的，整个事务存在期间都用这个视图

- 事务隔离的实现

  - 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值
  - 同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）
  - 建议你尽量不要使用长事务
    - 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。
    - 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库

- 事务的启动方式

  - 在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。
  -  set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接

- 如何避免长事务对业务的影响？

  - 确认是否使用了 set autocommit=0。MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认
  - 确认是否有不必要的只读事务
  - 通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。
  - 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；
  - 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。





## 索引

- 索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。

  - > alter table T engine=InnoDB。

- 覆盖索引

  - 如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引
  - 需要注意的是，在引擎内部使用覆盖索引在索引 k 上其实读了三个记录，R3~R5，对于 MySQL 的 Server 层来说，它就是找引擎拿到了两条记录，因此 MySQL 认为扫描行数是 2。

- 最左前缀原则

- InnoDB会把主键字段放到索引定义字段后面，当然同时也会去重。所以，当主键是(a,b)的时候，定义为c的索引，实际上是（c,a,b);定义为(c,a)的索引，实际上是(c,a,b)你看着加是相同的





## 全局锁和表锁

- 全局锁的典型使用场景是，做全库逻辑备份。

  - 官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。

- 表级锁

  - 业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。

  - MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。

    - MDL 不需要显式使用，在访问一个表的时候会被自动加上。
      - MDL作用是防止DDL和DML并发的冲突
      - 当对一个表做增删改查操作的时候，加 MDL读锁；当要对表做结构变更操作的时候，加 MDL 写锁。
      - 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。读写锁之间、写锁之间是互斥的

  - online ddl

    - Online DDL的过程是这样的：1. 拿MDL写锁2. 降级成MDL读锁3. 真正做DDL4. 升级成MDL写锁5. 释放MDL锁

      





## 行锁

- MySQL 的行锁是在引擎层由各个引擎自己实现的。

  - InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。
  - 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

- 死锁和死锁检测

  - 当出现死锁以后，有两种策略：

    - 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout 来设置。

    - 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。

    - 正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且

      innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。

  - 怎么解决由这种热点行更新导致的性能问题呢？问题的症结在于，死锁检测要耗费大量的 CPU 资源。
    
    - 你可以考虑通过将一行改成逻辑上的多行来减少锁冲突









## 事务到底是隔离的还是不隔离的

- “快照”在 MVCC 里是怎么工作的？

  - 每个事务有一个唯一的事务 ID，叫作 transaction id，按申请顺序严格递增的。
  - 每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID
    - 也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 rowtrx_id。
    - 假设同一行数据的 4 个版本，当前最新版本是 V4，V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。

-  InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前启动了但还没提交的所有事务 ID

  - 数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。

  - 已提交事务、未提交事务、未开始事务

  - 数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。

  - > 版本未提交，不可见；
    >
    > 版本已提交，但是是在视图创建后提交的，不可见；
    >
    > 版本已提交，而且是在视图创建前提交的，可见。

- 一条规则：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）

  - 其实，除了 update 语句外，select 语句如果加锁（lock in share mode 或 for update），也是当前读。

- 事务的可重复读的能力是怎么实现的

  - 可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。
  - 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
  - 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。





## 普通索引和唯一索引

- 查询过程

  - InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。

- 更新过程

  - 如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 changebuffer 中
    - 在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作
    - 虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。
  - 将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。
  - 将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。changebuffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。

- change buffer

  - 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。
  - change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。
  - 对于写多读少的业务来说，change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。
  - 如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。

- change buffer 和 redo log

  - > mysql> insert into t(id,k) values(id1,k1),(id2,k2);
    >
    > 
    >
    > 查找到位置后，k1 所在的数据页在内存 (InnoDBbuffer pool) 中，k2 所在的数据页不在内存中
    >
    > 
    >
    > Page 1 在内存中，直接更新内存；
    > Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息中
    > 将上述两个动作记入 redo log 

  - 那在这之后的读请求，要怎么处理呢？

  - >  读 Page 1 的时候，直接从内存返回,虽然磁盘上还是之前的数据，但是这里直接从内存返回结果
    >  要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果
    > 可以看到，直到需要读 Page 2 的时候，这个数据页才会被读入内存。

  - redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。

- change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢

  - change buffer有一部分在内存，有一部分在ibdata做merge操作，会把change buffer里相应的数据持久化到ibdata
    - merge 的执行流程
      - 从磁盘读入数据页到内存（老版本的数据页）
      - 从 change buffer 里找出这个数据页的 change buffer 记录，依次应用，得到新版数据页；
      - 写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。
      - 这时候，数据页和内存中 change buffer 对应的磁盘位置都还没有修改，属于脏页，之后各自刷回自己的物理数据，就是另外一个过程了。
  - 虽然是只更新内存，但是在事务提交的时候，我们把 change buffer 的操作也记录到 redo log 里了，所以崩溃恢复的时候，change buffer 也能找回来。
  - change buffer中分两部分，一部分是本次写入未写完的，一部分是已经写入完成的。
    - 针对未写完的，此部分操作，还未写入redo log，因此事务还未提交，所以没影响。
    - 针对已经写完成的，可以通过redo log来进行恢复




















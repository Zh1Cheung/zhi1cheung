---
title: elasticsearch高级篇(六)
categories:
- ELK
tags:
- elasticsearch


---



### 小型流量分析系统：logstash部署以及上手使用



第一个项目案例，简单介绍一下，es在国内主要用来做什么呢

其中一个es比较重要的一个应用的领域，就是elk做一些数据分析，比如说将一些网站的用户访问的日志，通过logstash，从apache或者nginx服务器上获取到

然后将那些日志文件通过logstash导入到es中

然后通过kibana对es中的数据，进行分析，生成一些报表

小型流量分析系统

elk课程，所以我这里就不重复讲解elk了，直接默认大家对elk都有一定的认识了，如果需要学习elk，可以去龙果看另外一个讲师的课程

如果我这里重复讲解的话，那么没意思，而且对其他课程并不是太好

logstash，简单来说，就是用来从各种各样的数据源，采集数据，mysql，apache日志，mq，将数据进行一些简单的处理，然后将数据写入到其他的一些地方，比如es，mq

1、下载和解压缩logstash

2、logstash pipeline

两个组成部分：input和output，也可以包含一个可选的filter

input plugin负责从数据源中获取数据

filter plugin负责对数据进行定制化的处理和修改

output plugin负责将数据写入目的地中

3、运行最基础的logstash pipeline

bin/logstash -e 'input { stdin { } } output { stdout {} }'

-e：直接在命令行对logstash进行配置，从命令行接受输入，将输出写入命令行

输入：hello world，可以看到输出，logstash会补充timestamp和ip地址

用ctrl-d可以结束这个piepline



### 小型流量分析系统：整体流程讲解



小型的流量分析系统，怎么来做，就讲解其中的一个做法和场景

1、一般来说，可以通过hive对昨日的流量日志数据，进行离线批处理，先预先按照维度将一些指标预先聚合出来，将结果写入mysql

做一点特殊的说明，因为这是一个单课，没法深入在hadoop，hive这块展开，所以这块是不讲解的，大家有兴趣的可以自己去看看一些资料

默认有一些预先处理好的数据已经存在于mysql中了

2、我们手动准备一些样例数据，然后写入mysql中，装一个mysql，模拟成是hive导入mysql的一份数据

3、通过logstash，将mysql中的数据导入es中

4、通过kibana+各种es聚合语法，生成各种各样的报表出来



### 小型流量分析系统：安装mysql以及手动导入样例数据


10 * 60 * 1000

用最简单的方式装一个mysql数据库，然后后面的话，就有数据库可以用来开发了

    wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm
    rpm -ivh mysql-community-release-el7-5.noarch.rpm
    yum install -y mysql-community-server
    
    service mysqld restart
    
    mysql -u root 
    
    set password for 'root'@'localhost' =password('password');

datekey cookie section userid province city pv is_return_visit is_bounce_visit visit_time visit_page_cnt

    日期    cookie 版块    用户id 省份     城市 pv 是否老用户回访  是否跳出        访问时间   访问页面数量

    create table user_access_log_aggr (
      datekey varchar(255),
      cookie varchar(255),
      section varchar(255),
      userid int,
      province varchar(255),
      city varchar(255),
      pv int,
      is_return_visit int,
      is_bounce_visit int,
      visit_time int,
      visit_page_cnt int
    )
    
    insert into user_access_log_aggr values('20171001', 'dasjfkaksdfj33', 'game', 1, 'beijing', 'beijing', 10, 0, 1, 600000, 3);
    insert into user_access_log_aggr values('20171001', 'dasjadfssdfj33', 'game', 2, 'jiangsu', 'nanjing', 5, 0, 0, 700000, 5);
    insert into user_access_log_aggr values('20171001', 'dasjffffksfj33', 'sport', 1, 'beijing', 'beijing', 8, 1, 0, 800000, 6);
    insert into user_access_log_aggr values('20171001', 'dasjdddksdfj33', 'sport', 2, 'jiangsu', 'nanjing', 20, 0, 1, 900000, 7);
    insert into user_access_log_aggr values('20171001', 'dasjeeeksdfj33', 'sport', 3, 'jiangsu', 'nanjing', 30, 1, 0, 600000, 10);
    insert into user_access_log_aggr values('20171001', 'dasrrrrksdfj33', 'news', 3, 'jiangsu', 'nanjing', 40, 0, 0, 600000, 12);
    insert into user_access_log_aggr values('20171001', 'dasjtttttdfj33', 'news', 4, 'shenzhen', 'shenzhen', 50, 0, 1, 500000, 4);
    insert into user_access_log_aggr values('20171001', 'dasjfkakkkfj33', 'game', 4, 'shenzhen', 'shenzhen', 20, 1, 0, 400000, 3);
    insert into user_access_log_aggr values('20171001', 'dasjyyyysdfj33', 'sport', 5, 'guangdong', 'guangzhou', 10, 0, 0, 300000, 1);
    insert into user_access_log_aggr values('20171001', 'dasjqqqksdfj33', 'news', 5, 'guangdong', 'guangzhou', 9, 0, 1, 200000, 2);



### 小型流量分析系统：使用logstash将mysql数据导入elasticsearch



1、安装es

    adduser elasticsearch
    passwd elasticsearch
    chown -R elasticsearch /usr/local/elasticsearch

2、安装logstash-input-jdbc插件
    
    yum install -y gem
    
    gem sources --add https://ruby.taobao.org/ --remove https://rubygems.org/
    gem sources -l
    
    gem install bundler
    bundle config mirror.https://rubygems.org https://ruby.taobao.org

在logstash目录下，vi Gemfile，修改 source 的值 为： "https://ruby.taobao.org"

在bin目录下，./logstash-plugin install logstash-input-jdbc



    wget https://github.com/logstash-plugins/logstash-input-jdbc/archive/v4.2.4.zip
    unzip master.zip
    
    cd logstash-input-jdbc-master
    
    vi Gemfile，修改 source 的值 为： "https://ruby.taobao.org"
    
    vi logstash-input-jdbc.gemspec

找到 

        s.files = `git ls-files`.split($\)
改为：

        s.files = [".gitignore", "CHANGELOG.md", "Gemfile", "LICENSE", "NOTICE.TXT", "README.md", "Rakefile", "lib/logstash/inputs/jdbc.rb", "lib/logstash/plugin_mixins/jdbc.rb", "logstash-input-jdbc.gemspec", "spec/inputs/jdbc_spec.rb"]
    	
    gem build logstash-input-jdbc.gemspec
    
    mv logstash-input-jdbc-4.2.4.gem /usr/local/logstash/
    
    plugin install logstash-input-jdbc-4.2.4.gem


3、启动logstash

在logstash目录中创建一份配置pipeline配置文件，user-access-log-pipeline.conf

    input {
      jdbc {
        jdbc_driver_library => "/usr/local/mysql-connector-java-5.1.36-bin.jar"
        jdbc_driver_class => "com.mysql.jdbc.Driver"
        jdbc_connection_string => "jdbc:mysql://localhost:3306/website"
        jdbc_user => "root"
    	jdbc_password => "root"
        schedule => "* * * * *"
        statement => "SELECT * from user_access_log_aggr"
      }
    }
    
    output {
    	elasticsearch {
            hosts => [ "localhost:9200" ]
        }
    }

运行下面的命令检查配置文件语法是否正确：
    
    bin/logstash -f user-access-log-pipeline.conf --config.test_and_exit

启动logstash：bin/logstash -f user-access-log-pipeline.conf --config.reload.automatic

--config.reload.automatic，会自动重新加载配置文件的内容



### 小型流量分析系统：安装和部署kibana



1、下载kibana

从kibana官网下载，并且解压缩

./bin/kibana，即可运行kibana，用ctrl+c可以终止kibana进程

2、配置kibana

在$KIBANA_HOME/config/kibana.yml文件中可以对kibana进行配置

    server.port
    server.host
    elasticsearch.url

3、访问kibana web管理工作台

默认通过5601端口进行访问，通过浏览器访问即可

4、设置index pattern

我们需要为kibana配置一个index pattern来匹配es中的索引名称，默认是logstash-*，匹配logstash写入es中的数据

同时还要配置一个time-field name，那个field是timestamp类型的，这是给kibana用来按照时间进行过滤的，kibana会自动加载出来给我们选择


### 小型流量分析系统：基于kibana制作网站流量分析报表（一）


对指定的版块进行查询，然后统计出如下指标的汇总

    pv: 所有人的pv相加
    uv: 对userid进行去重
    return_visit_uv: 回访uv
    total_visit_time: 总访问时长
    bounce_visit_uv: 跳出次数
    
    curl -XGET 'http://localhost:9200/logstash-2017.10.14/logs/_search?q=section:news&pretty' -d '
    {
        "size": 0,
        "aggs": {
          "pv": {"sum": {"field": "pv"}},
          "uv": {"cardinality": {"field": "userid", "precision_threshold": 40000}},
          "total_visit_time": {"sum": {"field": "visit_time"}},
          "return_visit_uv": {
            "filter": {"term": {"is_return_visit": 1}},
            "aggs": {
              "total_return_visit_uv": {"cardinality": {"field": "userid", "precision_threshold": 40000}}
            }
          },
          "bounce_visit_uv": {
            "filter": {"term": {"is_bounce_visit": 1}},
            "aggs": {
              "total_bounce_visit_uv": {"cardinality": {"field": "userid", "precision_threshold": 40000}}
            }
          }
        }
    }'




### 小型流量分析系统：基于kibana制作网站流量分析报表（二）


对指定的版块进行查询，然后统计出如下指标的汇总

    pv: 所有人的pv相加
    uv: 对userid4进行去重
    total_visit_time: 总访问时长
    return_visit_uv: 回访uv
    bounce_visit_uv: 跳出次数
    
    curl -XGET 'http://localhost:9200/logstash-2017.10.14/logs/_search?q=section:news&pretty' -d '
    {
        "size": 0,
        "aggs": {
          "pv": {"sum": {"field": "pv"}},
          "uv": {"cardinality": {"field": "userid", "precision_threshold": 40000}},
          "total_visit_time": {"sum": {"field": "visit_time"}},
          "return_visit_uv": {
            "filter": {"term": {"is_return_visit": 1}},
            "aggs": {
              "total_return_visit_uv": {"cardinality": {"field": "userid", "precision_threshold": 40000}}
            }
          },
          "bounce_visit_uv": {
            "filter": {"term": {"is_bounce_visit": 1}},
            "aggs": {
              "total_bounce_visit_uv": {"cardinality": {"field": "userid", "precision_threshold": 40000}}
            }
          }
        }
    }'
    






---
title: Linux-IO
categories:
- Linux
tags:
- Linux
---



## Linux 文件系统是怎么工作的

- 概述
  - 磁盘为系统提供了最基本的持久化存储
  - 文件系统则在磁盘的基础上，提供了一个用来管理文件的树状结构
- 索引节点和目录项
  - 为了方便管理，Linux 文件系统为每个文件都分配两个数据结构，索引节点（index node）和目录项（directory entry）。它们主要用来记录文件的元信息和目录结构。
  - 换句话说，索引节点是每个文件的唯一标志，而目录项维护的正是文件系统的树状结构。
    - 索引节点，简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以记住，索引节点同样占用磁盘空间。
    - 目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存。
  - 实际上，磁盘读写的最小单位是扇区，然而扇区只有 512B 大小，如果每次都读写这么小的单位，效率一定很低。所以，文件系统又把连续的扇区组成了逻辑块，然后每次都以逻辑块为最小单元，来管理数据。常见的逻辑块大小为 4KB，也就是由连续的 8 个扇区组成。

- 磁盘在执行文件系统格式化时，会被分成三个存储区域，超级块、索引节点区和数据块区

  - 超级块，存储整个文件系统的状态。
  - 索引节点区，用来存储索引节点。
  - 数据块区，则用来存储文件数据。

- 虚拟文件系统

  - 为了支持各种不同的文件系统，Linux 内核在用户进程和文件系统的中间，又引入了一个抽象层，也就是虚拟文件系统 VFS（Virtual File System）。
    - 用户进程和内核中的其他子系统，只需要跟 VFS 提供的统一接口进行交互就可以了，而不需要再关心底层各种文件系统的实现细节。
  - 这些文件系统，要先挂载到 VFS 目录树中的某个子目录（称为挂载点），然后才能访问其中的文件。
    - 基于磁盘的文件系统为例，在安装系统时，要先挂载一个根目录（/），在根目录下再把其他文件系统（比如其他的磁盘分区、/proc 文件系统、/sys 文件系统、NFS 等）挂载进来。

- 文件系统 I/O

  - 文件读写方式的各种差异，导致 I/O 的分类多种多样。
    - 缓冲 I/O，是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件。
    - 非缓冲 I/O，是指直接通过系统调用来访问文件，不再经过标准库缓存。
    - 无论缓冲 I/O 还是非缓冲 I/O，它们最终还是要经过系统调用来访问文件。我们知道，系统调用后，还会通过页缓存，来减少磁盘的 I/O 操作。
  - 第一种，根据是否利用标准库缓存，可以把文件 I/O 分为缓冲 I/O 与非缓冲 I/O。
  - 第二，根据是否利用操作系统的页缓存，可以把文件 I/O 分为直接 I/O 与非直接 I/O。
    - 直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件。
    - 非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘
  - 第三，根据应用程序是否阻塞自身运行，可以把文件 I/O 分为阻塞 I/O 和非阻塞 I/O：
  - 第四，根据是否等待响应结果，可以把文件 I/O 分为同步和异步 I/O

- 你也应该可以理解，“Linux 一切皆文件”的深刻含义。无论是普通文件和块设备、还是网络套接字和管道等，它们都通过统一的 VFS 接口来访问。

- 性能观测

  - > 对文件系统来说，最常见的一个问题就是空间不足。用 df 命令，就能查看文件系统的磁盘空间使用情况
    > 	 $ df  -h /dev/sda1

  - 不过有时候，明明你碰到了空间不足的问题，可是用 df 查看磁盘空间后，却发现剩余空间还有很多。这是怎么回事呢？

    - 除了文件数据，索引节点也占用磁盘空间。你可以给 df 命令加上 -i 参数，查看索引节点的使用情况
    - 当你发现索引节点空间不足，但磁盘空间充足时，很可能就是过多小文件导致的。
    - 所以，一般来说，删除这些小文件，或者把它们移动到索引节点充足的其他磁盘中，就可以解决这个问题







## 磁盘I/O

- 通用块层

  - 为了减小不同块设备的差异带来的影响，Linux 通过一个统一的通用块层，来管理各种不同的块设备。
  - 通用块层，其实是处在文件系统和磁盘驱动中间的一个块设备抽象层。它主要有两个功能
    - 向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序
    - 对 I/O 请求排序的过程，也就是我们熟悉的 I/O 调度

- I/O 栈

  - 我们可以把 Linux 存储系统的 I/O 栈，由上到下分为三个层次，分别是文件系统层、通用块层和设备层

    - 文件系统层，包括虚拟文件系统和其他各种文件系统的具体实现。它为上层的应用程序，提供标准的文件访问接口；对下会通过通用块层，来存储和管理磁盘数据。

    - 通用块层，包括块设备 I/O 队列和 I/O 调度器。它会对文件系统的 I/O 请求进行排队，再通

      过重新排序和请求合并，然后才要发送给下一级的设备层。

    - 设备层，包括存储设备和相应的驱动程序，负责最终物理设备的 I/O 操作。

  - 存储系统的 I/O ，通常是整个系统中最慢的一环。所以， Linux 通过多种缓存机制来优化I/O 效率

    - 比方说，为了优化文件访问的性能，会使用页缓存、索引节点缓存、目录项缓存等多种缓存机制，以减少对下层块设备的直接调用。
    - 同样，为了优化块设备的访问效率，会使用缓冲区，来缓存块设备的数据

- 磁盘性能指标

  - 必须要提到五个常见指标，也就是我们经常用到的，使用率、饱和度、IOPS、吞吐量以及响应时间等。这五个指标，是衡量磁盘性能的基本指标。

    - 使用率，是指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈

      - 这里要注意的是，使用率只考虑有没有 I/O，而不考虑 I/O 的大小。换句话说，当使用率是100% 的时候，磁盘依然有可能接受新的 I/O 请求。

    - 饱和度，是指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性

      能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。

    - IOPS（Input/Output Per Second），是指每秒的 I/O 请求数。

    - 吞吐量，是指每秒的 I/O 请求大小。

    - 响应时间，是指 I/O 请求从发出到收到响应的间隔时间

- 磁盘 I/O 观测

  - iostat 是最常用的磁盘 I/O 性能观测工具，它提供了每个磁盘的使用率、IOPS、吞吐量等各种常见的性能指标，当然，这些指标实际上来自 /proc/diskstats。

  - > 1 # -d -x 表示显示所有磁盘 I/O 的指标
    > 2 $ iostat -d -x 1
    >
    > 
    >
    > %util ，就是我们前面提到的磁盘 I/O 使用率；
    > r/s+ w/s ，就是 IOPS；
    > rkB/s+wkB/s ，就是吞吐量；
    > r_await+w_await ，就是响应时间。

- 进程 I/O 观测

  - 要观察进程的 I/O 情况，你还可以使用 pidstat 和 iotop 这两个工具。

  - > $ pidstat -d 1

  -  iotop。它是一个类似于 top 的工具，你可以按照 I/O大小对进程排序，然后找到 I/O 较大的那些进程。





##  I/O瓶颈

- 我们可以先用 top ，来观察 CPU 和内存的使用情况；然后再用 iostat ，来观察磁盘的 I/O 情况。

- 观察 top 的输出，CPU0 的使用率非常高，它的系统 CPU 使用率（sys%）为6%，而 iowait 超过了 90%。这说明 CPU0 上，可能正在运行 I/O 密集型的进程

- 看内存的使用情况，总内存 8G，剩余内存只有 730 MB，而 Buffer/Cache 占用内存高达 6GB 之多，这说明内存主要被缓存占用。

- 到这一步，你基本可以判断出，CPU 使用率中的 iowait 是一个潜在瓶颈，而内存部分的缓存占比较大，那磁盘 I/O 又是怎么样的情况呢？

- 使用 pidstat 加上 -d 参数，就可以显示每个进程的 I/O 情况

-  lsof

  - 它专门用来查看进程打开文件列表，不过，这里的“文件”不只有普通文件，还包括了目录、块设备、动态库、网络套接字等

  - -p 参数需要指定进程号

    - > -t 表示显示线程，-a 表示显示命令行参数
      >
      > $ pstree -t -a -p [pid]
      >
      > 找到了原因，lsof 的问题就容易解决了。把线程号换成进程号，继续执行 lsof 命令

  - > FD 表示文件描述符号，TYPE 表示文件类型，NAME 表示文件路径
    >
    > python 18940 root 3w REG 8,1 117944320 303 /tmp/logtest.txt
    >
    > 再看最后一行，这说明，这个进程打开了文件 /tmp/logtest.txt，并且它的文件描述符是 3号，而 3 后面的 w ，表示以写的方式打开







## 磁盘I/O延迟很高

- 随便执行一个命令，比如执行 df 命令，查看一下文件系统的使用情况。奇怪的是，这么简单的命令，居然也要等好久才有输出

  - 写文件是由子线程执行的，所以直接strace跟踪进程没有看到write系统调用，可以通过pstree查看进程的线程信息，再用strace跟踪。或者，通过strace -fp pid 跟踪所有线程。
  - 从 strace 中，你可以看到大量的 stat 系统调用，并且大都为 python 的文件，但是，请注意，这里并没有任何 write 系统调用。
  - 我们只好综合 strace、pidstat 和 iostat 这三个结果来分析了。很明显，你应该发现了这里的矛盾：iostat 已经证明磁盘 I/O 有性能瓶颈，而 pidstat 也证明了，这个瓶颈是由12280 号进程导致的，但 strace 跟踪这个进程，却没有找到任何 write 系统调用
  - 文件写，明明应该有相应的 write 系统调用，但用现有工具却找不到痕迹，这时就该想想换工具的问题了。怎样才能知道哪里在写文件呢？

-  filetop

  - 它是 bcc 软件包的一部分，主要跟踪内核中文件的读写情况，并输出线程 ID（TID）、读写大小、读写类型以及文件名称。

  - > 1 # 切换到工具目录
    > 2 $ cd /usr/share/bcc/tools 
    > 34 # -C 选项表示输出新内容时不清空屏幕
    > 5 $ ./filetop -C
    >
    > 
    >
    > 多观察一会儿，你就会发现，每隔一段时间，线程号为 514 的 python 应用就会先写入大量的 txt 文件，再大量地读。

- opensnoop 

  - 它同属于 bcc 软件包，可以动态跟踪内核中的open 系统调用。这样，我们就可以找出这些 txt 文件的路径。
  - 这次，通过 opensnoop 的输出，你可以看到，这些 txt 路径位于 /tmp 目录下。你还能看到，它打开的文件数量，按照数字编号，从 0.txt 依次增大到 999.txt，这可远多于前面用filetop 看到的数量
  - 综合 filetop 和 opensnoop ，我们就可以进一步分析了。我们可以大胆猜测，案例应用在写入 1000 个 txt 文件后，又把这些内容读到内存中进行处理。
  - 结合前面的所有分析，我们基本可以判断，案例应用会动态生成一批文件，用来临时存储数据，用完就会删除它们。但不幸的是，正是这些文件读写，引发了 I/O 的性能瓶颈，导致整个处理过程非常慢

  





## SQL慢查询

- top、iostat、pidstat、strace
- lsof
  - 从输出中可以看到， mysqld 进程确实打开了大量文件，而根据文件描述符（FD）的编号，我们知道，描述符为 38 的是一个路径为/var/lib/mysql/test/products.MYD 的文件。这里注意， 38 后面的 u 表示， mysqld 以读写的方式访问文件。
    - MYD 文件，是 MyISAM 引擎用来存储表数据的文件；
    - 文件名就是数据表的名字；
    - 而这个文件的父目录，也就是数据库的名字。
    - 换句话说，这个文件告诉我们，mysqld 在读取数据库 test 中的 products 表。
- 既然已经找出了数据库和表，接下来要做的，就是弄清楚数据库中正在执行什么样的 SQL了







## Redis响应严重延迟

- top、iostat、pidstat、strace
- lsof
  - 结合磁盘写的现象，我们知道，只有 7 号普通文件才会产生磁盘写，而它操作的文件路径是 /data/appendonly.aof，相应的系统调用包括 write 和 fdatasync
  - 这对应着正是 Redis 持久化配置中的 appendonly 和 appendfsync 选项
- 查询 appendonly 和 appendfsync 的配置
  - 从这个结果你可以发现，appendfsync 配置的是 always，而 appendonly 配置的是yes。
  - appendfsync 配置的是 always，意味着每次写数据时，都会调用一次 fsync，从而造成比较大的磁盘 I/O 压力。
-  iowait不代表磁盘I/O存在瓶颈，只是代表CPU上I/O操作的时间占用的百分比。假如这时候没有其他进程在运行，那么很小的I/O就会导致iowait升高
- 进程iowait高，磁盘iowait不高，说明是单个进程使用了一些blocking的磁盘打开方式，比如每次都fsync







## 如何迅速分析出系统I/O的瓶颈

- 性能指标
  - 文件系统 I/O 性能指标
    - 首先，最容易想到的是存储空间的使用情况，包括容量、使用量以及剩余空间等
      - 不过要注意，这些只是文件系统向外展示的空间使用，而非在磁盘空间的真实用量，因为文件系统的元数据也会占用磁盘空间。
      - 而且，如果你配置了 RAID，从文件系统看到的使用量跟实际磁盘的占用空间，也会因为RAID 级别的不同而不一样。比方说，配置RAID10 后，你从文件系统最多也只能看到所有磁盘容量的一半。
      - 除了数据本身的存储空间，还有一个容易忽略的是索引节点的使用情况，它也包括容量、使用量以及剩余量等三个指标。如果文件系统中存储过多的小文件，就可能碰到索引节点容量已满的问题
    - 其次，你应该想到的是前面多次提到过的缓存使用情况，包括页缓存、目录项缓存、索引节点缓存以及各个具体文件系统（如 ext4、XFS 等）的缓存
    - 除了以上这两点，文件 I/O 也是很重要的性能指标，包括 IOPS（包括 r/s 和 w/s）、响应时间（延迟）以及吞吐量（B/s）等
  - 磁盘 I/O 性能指标
    - 四个核心的磁盘 I/O 指标。
    - 考察这些指标时，一定要注意综合 I/O 的具体场景来分析，比如读写类型（顺序还是随机）、读写比例、读写大小、存储类型（有无RAID 以及 RAID 级别、本地存储还是网络存储）等
    - 缓冲区（Buffer）也是要重点掌握的指标，它经常出现在内存和磁盘问题的分析中
- 
































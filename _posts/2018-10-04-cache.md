---
title: Cache（四）
categories:
- Caching architecture
tags:
- Caching architecture
- Kafka
- Redis
- Zookeeper


---

## redis的LRU缓存清除算法讲解以及相关配置使用

多级缓存架构，缓存数据生产服务，监听各个数据源服务的数据变更的消息，得到消息之后，然后调用接口拉去数据

将拉去到的数据，写入本地ehcache缓存一份，spring boot整合，演示过

数据写入redis分布式缓存中一份，你不断的将数据写入redis，写入redis，然后redis的内存是有限的，每个redis实例最大一般也就是设置给10G

那如果你不断的写入数据，当数据写入的量超过了redis能承受的范围之后，改该怎么玩儿呢？？？

redis是会在数据达到一定程度之后，超过了一个最大的限度之后，就会将数据进行一定的清理，从内存中清理掉一些数据

只有清理掉一些数据之后，才能将新的数据写入内存中

1、LRU算法概述

redis默认情况下就是使用LRU策略的，因为内存是有限的，但是如果你不断地往redis里面写入数据，那肯定是没法存放下所有的数据在内存的

所以redis默认情况下，当内存中写入的数据很满之后，就会使用LRU算法清理掉部分内存中的数据，腾出一些空间来，然后让新的数据写入redis缓存中

LRU：Least Recently Used，最近最少使用算法

将最近一段时间内，最少使用的一些数据，给干掉。比如说有一个key，在最近1个小时内，只被访问了一次; 还有一个key在最近1个小时内，被访问了1万次

这个时候比如你要将部分数据给清理掉，你会选择清理哪些数据啊？肯定是那个在最近小时内被访问了1万次的数据

2、缓存清理设置

redis.conf

maxmemory，设置redis用来存放数据的最大的内存大小，一旦超出这个内存大小之后，就会立即使用LRU算法清理掉部分数据

如果用LRU，那么就是将最近最少使用的数据从缓存中清除出去

对于64 bit的机器，如果maxmemory设置为0，那么就默认不限制内存的使用，直到耗尽机器中所有的内存为止; 但是对于32 bit的机器，有一个隐式的闲置就是3GB

maxmemory-policy，可以设置内存达到最大闲置后，采取什么策略来处理

    （1）noeviction: 如果内存使用达到了maxmemory，client还要继续写入数据，那么就直接报错给客户端
    （2）allkeys-lru: 就是我们常说的LRU算法，移除掉最近最少使用的那些keys对应的数据
    （3）volatile-lru: 也是采取LRU算法，但是仅仅针对那些设置了指定存活时间（TTL）的key才会清理掉
    （4）allkeys-random: 随机选择一些key来删除掉
    （5）volatile-random: 随机选择一些设置了TTL的key来删除掉
    （6）volatile-ttl: 移除掉部分keys，选择那些TTL时间比较短的keys

在redis里面，写入key-value对的时候，是可以设置TTL，存活时间，比如你设置了60s。那么一个key-value对，在60s之后就会自动被删除

redis的使用，各种数据结构，list，set，等等

allkeys-lru

这边拓展一下思路，对技术的研究，一旦将一些技术研究的比较透彻之后，就喜欢横向对比底层的一些原理

storm，科普一下

玩儿大数据的人搞得，领域，实时计算领域，storm

storm有很多的流分组的一些策略，按shuffle分组，global全局分组，direct直接分组，fields按字段值hash后分组

分组策略也很多，但是，真正公司里99%的场景下，使用的也就是shuffle和fields，两种策略

redis，给了这么多种乱七八糟的缓存清理的算法，其实真正常用的可能也就那么一两种，allkeys-lru是最常用的

3、缓存清理的流程

（1）客户端执行数据写入操作

（2）redis server接收到写入操作之后，检查maxmemory的限制，如果超过了限制，那么就根据对应的policy清理掉部分数据

（3）写入操作完成执行

4、redis的LRU近似算法

科普一个相对来说稍微高级一丢丢的知识点

redis采取的是LRU近似算法，也就是对keys进行采样，然后在采样结果中进行数据清理

redis 3.0开始，在LRU近似算法中引入了pool机制，表现可以跟真正的LRU算法相当，但是还是有所差距的，不过这样可以减少内存的消耗

redis LRU算法，是采样之后再做LRU清理的，跟真正的、传统、全量的LRU算法是不太一样的

maxmemory-samples，比如5，可以设置采样的大小，如果设置为10，那么效果会更好，不过也会耗费更多的CPU资源



## zookeeper+kafka集群的安装部署以及如何简单使用的介绍


多级缓存的架构

主要是用来解决什么样的数据的缓存的更新的啊？？？

时效性不高的数据，比如一些商品的基本信息，如果发生了变更，假设在5分钟之后再更新到页面中，供用户观察到，也是ok的

时效性要求不高的数据，那么我们采取的是异步更新缓存的策略

时效性要求很高的数据，库存，采取的是数据库+缓存双写的技术方案，也解决了双写的一致性的问题

缓存数据生产服务，监听一个消息队列，然后数据源服务（商品信息管理服务）发生了数据变更之后，就将数据变更的消息推送到消息队列中

缓存数据生产服务可以去消费到这个数据变更的消息，然后根据消息的指示提取一些参数，然后调用对应的数据源服务的接口，拉去数据，这个时候一般是从mysql库中拉去的

消息队列是什么东西？采取打的就是kafka

我工作的时候，很多项目是跟大数据相关的，当然也有很多是纯java系统的架构，最近用kafka用得比较多

kafka比较简单易用，讲课来说，很方便



，kafka和activemq肯定有区别，但是说，在有些场景下，其实可能没有那么大的区分度，kafka和activemq其实是一样的

生产者+消费者的场景，kafka+activemq都ok

涉及的这种架构，对时效性要求高和时效性要求低的数据，分别采取什么技术方案？数据库+缓存双写一致性？异步+多级缓存架构？大缓存的维度化拆分？

你要关注的，是一些架构上的东西和思想，而不是具体的什么mq的使用

activemq的课程，书籍，资料

kafka集群，zookeeper集群，先搭建zookeeper集群，再搭建kafka集群

kafka另外一个原因：kafka，本来就要搭建zookeeper，zookeeper这个东西，后面我们还要用呢，缓存的分布式并发更新的问题，分布式锁解决

zookeeper + kafka的集群，都是三节点

java高级工程师的思想，在干活儿，在思考，jvm，宏观的思考，通盘去考虑整个架构，还有未来的技术规划，业务的发展方向，架构的演进方向和路线

把课程里讲解的各种技术方案组合成、修改成你需要的适合你的业务的缓存架构

1、zookeeper集群搭建

将课程提供的zookeeper-3.4.5.tar.gz使用WinSCP拷贝到/usr/local目录下。
对zookeeper-3.4.5.tar.gz进行解压缩：tar -zxvf zookeeper-3.4.5.tar.gz。
对zookeeper目录进行重命名：mv zookeeper-3.4.5 zk

配置zookeeper相关的环境变量

    vi ~/.bashrc
    export ZOOKEEPER_HOME=/usr/local/zk
    export PATH=$ZOOKEEPER_HOME/bin
    source ~/.bashrc
    
    cd zk/conf
    cp zoo_sample.cfg zoo.cfg
    
    vi zoo.cfg
    修改：dataDir=/usr/local/zk/data
    新增：
    server.0=eshop-cache01:2888:3888	
    server.1=eshop-cache02:2888:3888
    server.2=eshop-cache03:2888:3888
    
    cd zk
    mkdir data
    cd data
    
    vi myid
    0

在另外两个节点上按照上述步骤配置ZooKeeper，使用scp将zk和.bashrc拷贝到eshop-cache02和eshop-cache03上即可。唯一的区别是标识号分别设置为1和2。

分别在三台机器上执行：zkServer.sh start。
检查ZooKeeper状态：zkServer.sh status，应该是一个leader，两个follower
jps：检查三个节点是否都有QuromPeerMain进程

2、kafka集群搭建

scala，我就不想多说了，就是一门编程语言，现在比较火，很多比如大数据领域里面的spark（计算引擎）就是用scala编写的

将课程提供的scala-2.11.4.tgz使用WinSCP拷贝到/usr/local目录下。
对scala-2.11.4.tgz进行解压缩：tar -zxvf scala-2.11.4.tgz。
对scala目录进行重命名：mv scala-2.11.4 scala

配置scala相关的环境变量

    vi ~/.bashrc
    export SCALA_HOME=/usr/local/scala
    export PATH=$SCALA_HOME/bin
    source ~/.bashrc

查看scala是否安装成功：scala -version

按照上述步骤在其他机器上都安装好scala。使用scp将scala和.bashrc拷贝到另外两台机器上即可。

将课程提供的kafka_2.9.2-0.8.1.tgz使用WinSCP拷贝到/usr/local目录下。
对kafka_2.9.2-0.8.1.tgz进行解压缩：tar -zxvf kafka_2.9.2-0.8.1.tgz。
对kafka目录进行改名：mv kafka_2.9.2-0.8.1 kafka

配置kafka

    vi /usr/local/kafka/config/server.properties
    broker.id：依次增长的整数，0、1、2，集群中Broker的唯一id
    zookeeper.connect=192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181

安装slf4j
将课程提供的slf4j-1.7.6.zip上传到/usr/local目录下
unzip slf4j-1.7.6.zip
把slf4j中的slf4j-nop-1.7.6.jar复制到kafka的libs目录下面

解决kafka Unrecognized VM option 'UseCompressedOops'问题
    
    vi /usr/local/kafka/bin/kafka-run-class.sh 
    
    if [ -z "$KAFKA_JVM_PERFORMANCE_OPTS" ]; then
      KAFKA_JVM_PERFORMANCE_OPTS="-server  -XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true"
    fi

去掉-XX:+UseCompressedOops即可

按照上述步骤在另外两台机器分别安装kafka。用scp把kafka拷贝到其他机器即可。
唯一区别的，就是server.properties中的broker.id，要设置为1和2

在三台机器上的kafka目录下，分别执行以下命令：nohup bin/kafka-server-start.sh config/server.properties &

使用jps检查启动是否成功

使用基本命令检查kafka是否搭建成功

    bin/kafka-topics.sh --zookeeper 192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181 --topic test --replication-factor 1 --partitions 1 --create
    
    bin/kafka-console-producer.sh --broker-list 192.168.31.181:9092,192.168.31.19:9092,192.168.31.227:9092 --topic test
    
    bin/kafka-console-consumer.sh --zookeeper 192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181 --topic test --from-beginning


## 基于kafka+ehcache+redis完成缓存数据生产服务的开发与测试



1、将kafka整合到spring boot中

    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka_2.9.2</artifactId>
        <version>0.8.1</version>
    </dependency>
    
    @Bean
    public ServletListenerRegistrationBean servletListenerRegistrationBean() {
    	ServletListenerRegistrationBean servletListenerRegistrationBean = 
    			new ServletListenerRegistrationBean();
    	servletListenerRegistrationBean.setListener(new InitListener());  
    	return servletListenerRegistrationBean;
    }
    
    public class KafkaConcusmer implements Runnable {
    
        private final ConsumerConnector consumer;
        private final String topic;
     	
        public ConsumerGroupExample(String topic) {
            consumer = Consumer.createJavaConsumerConnector(createConsumerConfig());
            this.topic = topic;
        }
    
        private static ConsumerConfig createConsumerConfig(String a_zookeeper, String a_groupId) {
            Properties props = new Properties();
            props.put("zookeeper.connect", "192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181");
            props.put("group.id", "eshop-cache-group");
            props.put("zookeeper.session.timeout.ms", "400");
            props.put("zookeeper.sync.time.ms", "200");
            props.put("auto.commit.interval.ms", "1000");
            return new ConsumerConfig(props);
        }
     	
        public void run() {
            Map<String, Integer> topicCountMap = new HashMap<String, Integer>();
            topicCountMap.put(topic, 1);
            Map<String, List<KafkaStream<byte[], byte[]>>> consumerMap = consumer.createMessageStreams(topicCountMap);
            List<KafkaStream<byte[], byte[]>> streams = consumerMap.get(topic);
    
            for (final KafkaStream stream : streams) {
                new Thread(new KafkaMessageProcessor(stream)).start();
            }
        }
    
    }
    
    public class KafkaMessageProcessor implements Runnable {
    
        private KafkaStream kafkaStream;
     
        public ConsumerTest(KafkaStream kafkaStream) {
        	this.kafkaStream = kafkaStream;
        }
     
        public void run() {
            ConsumerIterator<byte[], byte[]> it = kafkaStream.iterator();
            while (it.hasNext()) {
            	String message = new String(it.next().message());
            }
        }
    
    }
    
    ServletContext sc = sce.getServletContext();
    ApplicationContext context = WebApplicationContextUtils.getWebApplicationContext(sc);

2、编写业务逻辑

（1）两种服务会发送来数据变更消息：商品信息服务，商品店铺信息服务，每个消息都包含服务名以及商品id

（2）接收到消息之后，根据商品id到对应的服务拉取数据，这一步，我们采取简化的模拟方式，就是在代码里面写死，会获取到什么数据，不去实际再写其他的服务去调用了

（3）商品信息：id，名称，价格，图片列表，商品规格，售后信息，颜色，尺寸

（4）商品店铺信息：其他维度，用这个维度模拟出来缓存数据维度化拆分，id，店铺名称，店铺等级，店铺好评率

（5）分别拉取到了数据之后，将数据组织成json串，然后分别存储到ehcache中，和redis缓存中

3、测试业务逻辑

（1）创建一个kafka topic

（2）在命令行启动一个kafka producer

（3）启动系统，消费者开始监听kafka topic

C:\Windows\System32\drivers\etc\hosts

（4）在producer中，分别发送两条消息，一个是商品信息服务的消息，一个是商品店铺信息服务的消息

（5）能否接收到两条消息，并模拟拉取到两条数据，同时将数据写入ehcache中，并写入redis缓存中

（6）ehcache通过打印日志方式来观察，redis通过手工连接上去来查询



## 基于“分发层+应用层”双层nginx架构提升缓存命中率方案分析



1、缓存命中率低


![image](http://i2.51cto.com/images/blog/201810/01/56e481ba398a8edb11a2b6e06c2bf6ae.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


缓存数据生产服务那一层已经搞定了，相当于三层缓存架构中的本地堆缓存+redis分布式缓存都搞定了

就要来做三级缓存中的nginx那一层的缓存了

如果一般来说，你默认会部署多个nginx，在里面都会放一些缓存，就默认情况下，此时缓存命中率是比较低的

2、如何提升缓存命中率

分发层+应用层，双层nginx


![image](http://i2.51cto.com/images/blog/201810/01/e2354a04b8f8cb3372c9801f1fae69c1.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

分发层nginx，负责流量分发的逻辑和策略，这个里面它可以根据你自己定义的一些规则，比如根据productId去进行hash，然后对后端的nginx数量取模

将某一个商品的访问的请求，就固定路由到一个nginx后端服务器上去，保证说只会从redis中获取一次缓存数据，后面全都是走nginx本地缓存了

后端的nginx服务器，就称之为应用服务器; 最前端的nginx服务器，被称之为分发服务器

看似很简单，其实很有用，在实际的生产环境中，可以大幅度提升你的nginx本地缓存这一层的命中率，大幅度减少redis后端的压力，提升性能



## 基于OpenResty部署应用层nginx以及nginx+lua开发hello world



我们这里玩儿nginx，全都会在nginx里去写lua脚本，因为我们需要自定义一些特殊的业务逻辑

比如说，流量分发，自己用lua去写分发的逻辑，在分发层nginx里去写的

再比如说，要用lua去写多级缓存架构存取的控制逻辑，在应用层nginx里去写的

后面还要做热点数据的自动降级机制，也是用lua脚本去写降级机制的，在分发层nginx里去写的

因为我们要用nginx+lua去开发，所以会选择用最流行的开源方案，就是用OpenResty

nginx+lua打包在一起，而且提供了包括redis客户端，mysql客户端，http客户端在内的大量的组件

我们这一讲是去部署应用层nginx，会采用OpenResty的方式去部署nginx，而且会带着大家写一个nginx+lua开发的一个hello world

1、部署第一个nginx，作为应用层nginx（192.168.31.187那个机器上）

（1）部署openresty

    mkdir -p /usr/servers  
    cd /usr/servers/
    
    yum install -y readline-devel pcre-devel openssl-devel gcc
    
    wget http://openresty.org/download/ngx_openresty-1.7.7.2.tar.gz  
    tar -xzvf ngx_openresty-1.7.7.2.tar.gz  
    cd /usr/servers/ngx_openresty-1.7.7.2/
    
    cd bundle/LuaJIT-2.1-20150120/  
    make clean && make && make install  
    ln -sf luajit-2.1.0-alpha /usr/local/bin/luajit
    
    cd bundle  
    wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gz  
    tar -xvf 2.3.tar.gz  
    
    cd bundle  
    wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz  
    tar -xvf v0.3.0.tar.gz  
    
    cd /usr/servers/ngx_openresty-1.7.7.2  
    ./configure --prefix=/usr/servers --with-http_realip_module  --with-pcre  --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2  
    make && make install 
    
    cd /usr/servers/  
    ll
    
    /usr/servers/luajit
    /usr/servers/lualib
    /usr/servers/nginx
    /usr/servers/nginx/sbin/nginx -V 
    
    启动nginx: /usr/servers/nginx/sbin/nginx

（2）nginx+lua开发的hello world
    
    vi /usr/servers/nginx/conf/nginx.conf

在http部分添加：

    lua_package_path "/usr/servers/lualib/?.lua;;";  
    lua_package_cpath "/usr/servers/lualib/?.so;;";  
    
    /usr/servers/nginx/conf下，创建一个lua.conf
    
    server {  
        listen       80;  
        server_name  _;  
    }  

在nginx.conf的http部分添加：

    include lua.conf;

验证配置是否正确：

    /usr/servers/nginx/sbin/nginx -t
    
    在lua.conf的server部分添加：
    
    location /lua {  
        default_type 'text/html';  
        content_by_lua 'ngx.say("hello world")';  
    } 
    
    /usr/servers/nginx/sbin/nginx -t  

重新nginx加载配置

    /usr/servers/nginx/sbin/nginx -s reload  
    
    访问http: http://192.168.31.187/lua
    
    vi /usr/servers/nginx/conf/lua/test.lua
    
    ngx.say("hello world"); 

修改lua.conf

    location /lua {  
        default_type 'text/html';  
        content_by_lua_file conf/lua/test.lua; 
    }

查看异常日志

    tail -f /usr/servers/nginx/logs/error.log

（3）工程化的nginx+lua项目结构

项目工程结构
    
    hello
        hello.conf     
        lua              
          hello.lua
        lualib            
          *.lua
          *.so

放在/usr/hello目录下

    /usr/servers/nginx/conf/nginx.conf
    
    worker_processes  2;  
    
    error_log  logs/error.log;  
    
    events {  
        worker_connections  1024;  
    }  
    
    http {  
        include       mime.types;  
        default_type  text/html;  
      
        lua_package_path "/usr/hello/lualib/?.lua;;";  
        lua_package_cpath "/usr/hello/lualib/?.so;;"; 
        include /usr/hello/hello.conf;  
    }  
    
    /usr/hello/hello.conf
    
    server {  
        listen       80;  
        server_name  _;  
      
        location /lua {  
            default_type 'text/html';  
            lua_code_cache off;  
            content_by_lua_file /usr/example/lua/test.lua;  
        }  
    }  

2、如法炮制，在另外一个机器上，也用OpenResty部署一个nginx





topic，主题，基于dubbo复杂的分布式系统的通用架构，分布式系统，dubbo rpc的调用，服务的开发; zookeeper注册中心; redis分布式集群; mysql读写分离; tomcat集群; hudson持续集成

它告诉你的是一个通用的分布式系统的架构




## 部署分发层nginx以及基于lua完成基于商品id的定向流量分发策略




大家可以自己按照上一讲讲解的内容，基于OpenResty在另外两台机器上都部署一下nginx+lua的开发环境

我已经在eshop-cache02和eshop-cache03上都部署好了

我这边的话呢，是打算用eshop-cache01和eshop-cache02作为应用层nginx服务器，用eshop-cache03作为分发层nginx

在eshop-cache03，也就是分发层nginx中，编写lua脚本，完成基于商品id的流量分发策略

当然了，我们这里主要会简化策略，简化业务逻辑，实际上在你的公司中，你可以随意根据自己的业务逻辑和场景，去制定自己的流量分发策略

    1、获取请求参数，比如productId
    2、对productId进行hash
    3、hash值对应用服务器数量取模，获取到一个应用服务器
    4、利用http发送请求到应用层nginx
    5、获取响应后返回

这个就是基于商品id的定向流量分发的策略，lua脚本来编写和实现

我们作为一个流量分发的nginx，会发送http请求到后端的应用nginx上面去，所以要先引入lua http lib包

    cd /usr/hello/lualib/resty/  
    wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http_headers.lua  
    wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http.lua 

代码：

    local uri_args = ngx.req.get_uri_args()
    local productId = uri_args["productId"]
    
    local host = {"192.168.31.19", "192.168.31.187"}
    local hash = ngx.crc32_long(productId)
    hash = (hash % 2) + 1  
    backend = "http://"..host[hash]
    
    local method = uri_args["method"]
    local requestBody = "/"..method.."?productId="..productId
    
    local http = require("resty.http")  
    local httpc = http.new()  
    
    local resp, err = httpc:request_uri(backend, {  
        method = "GET",  
        path = requestBody
    })
    
    if not resp then  
        ngx.say("request error :", err)  
        return  
    end
    
    ngx.say(resp.body)  
      
    httpc:close() 
    
    /usr/servers/nginx/sbin/nginx -s reload

基于商品id的定向流量分发策略的lua脚本就开发完了，而且也测试过了

我们就可以看到，如果你请求的是固定的某一个商品，那么就一定会将流量打到固定的一个应用nginx上面去


## 基于nginx+lua+java完成多级缓存架构的核心业务逻辑



分发层nginx，lua应用，会将商品id，商品店铺id，都转发到后端的应用nginx

    /usr/servers/nginx/sbin/nginx -s reload

1、应用nginx的lua脚本接收到请求

2、获取请求参数中的商品id，以及商品店铺id

3、根据商品id和商品店铺id，在nginx本地缓存中尝试获取数据

4、如果在nginx本地缓存中没有获取到数据，那么就到redis分布式缓存中获取数据，如果获取到了数据，还要设置到nginx本地缓存中

但是这里有个问题，建议不要用nginx+lua直接去获取redis数据

因为openresty没有太好的redis cluster的支持包，所以建议是发送http请求到缓存数据生产服务，由该服务提供一个http接口

缓存数生产服务可以基于redis cluster api从redis中直接获取数据，并返回给nginx

    cd /usr/hello/lualib/resty/  
    wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http_headers.lua  
    wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http.lua 

5、如果缓存数据生产服务没有在redis分布式缓存中没有获取到数据，那么就在自己本地ehcache中获取数据，返回数据给nginx，也要设置到nginx本地缓存中

6、如果ehcache本地缓存都没有数据，那么就需要去原始的服务中拉去数据，该服务会从mysql中查询，拉去到数据之后，返回给nginx，并重新设置到ehcache和redis中

这里先不考虑，后面要专门讲解一套分布式缓存重建并发冲突的问题和解决方案

7、nginx最终利用获取到的数据，动态渲染网页模板

    cd /usr/hello/lualib/resty/
    wget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template.lua
    mkdir /usr/hello/lualib/resty/html
    cd /usr/hello/lualib/resty/html
    wget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template/html.lua
    
    在hello.conf的server中配置模板位置
    
    set $template_location "/templates";  
    set $template_root "/usr/hello/templates";
    
    mkdir /usr/hello/templates
    
    vi product.html
    
    product id: {* productId *}<br/>
    product name: {* productName *}<br/>
    product picture list: {* productPictureList *}<br/>
    product specification: {* productSpecification *}<br/>
    product service: {* productService *}<br/>
    product color: {* productColor *}<br/>
    product size: {* productSize *}<br/>
    shop id: {* shopId *}<br/>
    shop name: {* shopName *}<br/>
    shop level: {* shopLevel *}<br/>
    shop good cooment rate: {* shopGoodCommentRate *}<br/>

8、将渲染后的网页模板作为http响应，返回给分发层nginx

hello.conf中：

    lua_shared_dict my_cache 128m;

lua脚本中：

    local uri_args = ngx.req.get_uri_args()
    local productId = uri_args["productId"]
    local shopId = uri_args["shopId"]
    
    local cache_ngx = ngx.shared.my_cache
    
    local productCacheKey = "product_info_"..productId
    local shopCacheKey = "shop_info_"..shopId
    
    local productCache = cache_ngx:get(productCacheKey)
    local shopCache = cache_ngx:get(shopCacheKey)
    
    if productCache == "" or productCache == nil then
    	local http = require("resty.http")
    	local httpc = http.new()
    
    	local resp, err = httpc:request_uri("http://192.168.31.179:8080",{
      		method = "GET",
      		path = "/getProductInfo?productId="..productId
    	})
    
    	productCache = resp.body
    	cache_ngx:set(productCacheKey, productCache, 10 * 60)
    end
    
    if shopCache == "" or shopCache == nil then
    	local http = require("resty.http")
    	local httpc = http.new()
    
    	local resp, err = httpc:request_uri("http://192.168.31.179:8080",{
      		method = "GET",
      		path = "/getShopInfo?shopId="..shopId
    	})
    
    	shopCache = resp.body
    	cache_ngx:set(shopCacheKey, shopCache, 10 * 60)
    end
    
    local cjson = require("cjson")
    local productCacheJSON = cjson.decode(productCache)
    local shopCacheJSON = cjson.decode(shopCache)
    
    local context = {
    	productId = productCacheJSON.id,
    	productName = productCacheJSON.name,
    	productPrice = productCacheJSON.price,
    	productPictureList = productCacheJSON.pictureList,
    	productSpecification = productCacheJSON.specification,
    	productService = productCacheJSON.service,
    	productColor = productCacheJSON.color,
    	productSize = productCacheJSON.size,
    	shopId = shopCacheJSON.id,
    	shopName = shopCacheJSON.name,
    	shopLevel = shopCacheJSON.level,
    	shopGoodCommentRate = shopCacheJSON.goodCommentRate
    }
    
    local template = require("resty.template")
    template.render("product.html", context)
    


    /usr/servers/nginx/sbin/nginx -s reload
    
    <html>
            <head>
                    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
                    <title>商品详情页</title>
            </head>
    <body>
    商品id: {* productId *}<br/>
    商品名称: {* productName *}<br/>
    商品图片列表: {* productPictureList *}<br/>
    商品规格: {* productSpecification *}<br/>
    商品售后服务: {* productService *}<br/>
    商品颜色: {* productColor *}<br/>
    商品大小: {* productSize *}<br/>
    店铺id: {* shopId *}<br/>
    店铺名称: {* shopName *}<br/>
    店铺等级: {* shopLevel *}<br/>
    店铺好评率: {* shopGoodCommentRate *}<br/>
    </body>
    </html>

第一次访问的时候，其实在nginx本地缓存中是取不到的，所以会发送http请求到后端的缓存服务里去获取，会从redis中获取

拿到数据以后，会放到nginx本地缓存里面去，过期时间是10分钟

然后将所有数据渲染到模板中，返回模板

以后再来访问的时候，就会直接从nginx本地缓存区获取数据了

缓存数据生产 -> 有数据变更 -> 主动更新两级缓存（ehcache+redis）-> 缓存维度化拆分

分发层nginx + 应用层nginx -> 自定义流量分发策略提高缓存命中率

nginx shared dict缓存 -> 缓存服务 -> redis -> ehcache -> 渲染html模板 -> 返回页面

还差最后一个很关键的要点，就是如果你的数据在nginx -> redis -> ehcache三级缓存都不在了，可能就是被LRU清理掉了

这个时候缓存服务会重新拉去数据，去更新到ehcache和redis中



## 分布式缓存重建并发冲突问题以及zookeeper分布式锁解决方案



整个三级缓存的架构已经走通了

我们还遇到一个问题，就是说，如果缓存服务在本地的ehcache中都读取不到数据，那就恩坑爹了

这个时候就意味着，需要重新到源头的服务中去拉去数据，拉取到数据之后，赶紧先给nginx的请求返回，同时将数据写入ehcache和redis中

分布式重建缓存的并发冲突问题

![image](http://i2.51cto.com/images/blog/201810/01/d914a2e11c754f0c4a9fb25fac303efe.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


重建缓存：比如我们这里，数据在所有的缓存中都不存在了（LRU算法弄掉了），就需要重新查询数据写入缓存，重建缓存

分布式的重建缓存，在不同的机器上，不同的服务实例中，去做上面的事情，就会出现多个机器分布式重建去读取相同的数据，然后写入缓存中



缓存更新和缓存重建在不同机器上的并发冲突问题：

![image](http://i2.51cto.com/images/blog/201810/01/d2a850dccc11abe471b4afe5b72156e3.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



分布式重建缓存的并发冲突问题。。。。。。

1、流量均匀分布到所有缓存服务实例上

应用层nginx，是将请求流量均匀地打到各个缓存服务实例中的，可能咱们的eshop-cache那个服务，可能会部署多实例在不同的机器上

2、应用层nginx的hash，固定商品id，走固定的缓存服务实例

分发层的nginx的lua脚本，是怎么写的，怎么玩儿的，搞一堆应用层nginx的地址列表，对每个商品id做一个hash，然后对应用nginx数量取模

将每个商品的请求固定分发到同一个应用层nginx上面去

在应用层nginx里，发现自己本地lua shared dict缓存中没有数据的时候，就采取一样的方式，对product id取模，然后将请求固定分发到同一个缓存服务实例中去

这样的话，就不会出现说多个缓存服务实例分布式的去更新那个缓存了


3、源信息服务发送的变更消息，需要按照商品id去分区，固定的商品变更走固定的kafka分区，也就是固定的一个缓存服务实例获取到

缓存服务，是监听kafka topic的，一个缓存服务实例，作为一个kafka consumer，就消费topic中的一个partition

所以你有多个缓存服务实例的话，每个缓存服务实例就消费一个kafka partition

所以这里，一般来说，你的源头信息服务，在发送消息到kafka topic的时候，都需要按照product id去分区

也就时说，同一个product id变更的消息一定是到同一个kafka partition中去的，也就是说同一个product id的变更消息，一定是同一个缓存服务实例消费到的

我们也不去做了，其实很简单，kafka producer api，里面send message的时候，多加一个参数就可以了，product id传递进去，就可以了

4、问题是，自己写的简易的hash分发，与kafka的分区，可能并不一致！！！

我们自己写的简易的hash分发策略，是按照crc32去取hash值，然后再取模的

关键你又不知道你的kafka producer的hash策略是什么，很可能说跟我们的策略是不一样的

拿就可能导致说，数据变更的消息所到的缓存服务实例，跟我们的应用层nginx分发到的那个缓存服务实例也许就不在一台机器上了

这样的话，在高并发，极端的情况下，可能就会出现冲突

5、分布式的缓存重建并发冲突问题发生了。。。

6、基于zookeeper分布式锁的解决方案

分布式锁，如果你有多个机器在访问同一个共享资源，那么这个时候，如果你需要加个锁，让多个分布式的机器在访问共享资源的时候串行起来

那么这个时候，那个锁，多个不同机器上的服务共享的锁，就是分布式锁

分布式锁当然有很多种不同的实现方案，redis分布式锁，zookeeper分布式锁

zk，做分布式协调这一块，还是很流行的，大数据应用里面，hadoop，storm，都是基于zk去做分布式协调

zk分布式锁的解决并发冲突的方案

（1）变更缓存重建以及空缓存请求重建，更新redis之前，都需要先获取对应商品id的分布式锁

（2）拿到分布式锁之后，需要根据时间版本去比较一下，如果自己的版本新于redis中的版本，那么就更新，否则就不更新

（3）如果拿不到分布式锁，那么就等待，不断轮询等待，直到自己获取到分布式的锁


![image](http://i2.51cto.com/images/blog/201810/01/aef242df4fa3020e66f26927b5d8a7c1.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



## 缓存数据生产服务中的zk分布式锁解决方案的代码实现


zk分布式锁的代码封装

zookeeper java client api去封装连接zk，以及获取分布式锁，还有释放分布式锁的代码

先简单介绍一下zk分布式锁的原理

我们通过去创建zk的一个临时node，来模拟给摸一个商品id加锁

zk会给你保证说，只会创建一个临时node，其他请求过来如果再要创建临时node，就会报错，NodeExistsException

那么所以说，我们的所谓上锁，其实就是去创建某个product id对应的一个临时node

如果临时node创建成功了，那么说明我们成功加锁了，此时就可以去执行对redis立面数据的操作

如果临时node创建失败了，说明有人已经在拿到锁了，在操作reids中的数据，那么就不断的等待，直到自己可以获取到锁为止

基于zk client api，去封装上面的这个代码逻辑

释放一个分布式锁，去删除掉那个临时node就可以了，就代表释放了一个锁，那么此时其他的机器就可以成功创建临时node，获取到锁

即使是用zk去实现一个分布式锁，也有很多种做法，有复杂的，也有简单的

应该说这种分布式锁的做法，是非常简单的一种，但是很实用，大部分情况下，用这种简单的分布式锁都能搞定

    <dependency>
        <groupId>org.apache.zookeeper</groupId>
        <artifactId>zookeeper</artifactId>
        <version>3.4.5</version>
    </dependency>
    
    /**
     * ZooKeeperSession
     * @author Administrator
     *
     */
    public class ZooKeeperSession {
    	
    	private static CountDownLatch connectedSemaphore = new CountDownLatch(1);
    	
    	private ZooKeeper zookeeper;
    
    	public ZooKeeperSession() {
    		// 去连接zookeeper server，创建会话的时候，是异步去进行的
    		// 所以要给一个监听器，说告诉我们什么时候才是真正完成了跟zk server的连接
    		try {
    			this.zookeeper = new ZooKeeper(
    					"192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181", 
    					50000, 
    					new ZooKeeperWatcher());
    			// 给一个状态CONNECTING，连接中
    			System.out.println(zookeeper.getState());
    			
    			try {
    				// CountDownLatch
    				// java多线程并发同步的一个工具类
    				// 会传递进去一些数字，比如说1,2 ，3 都可以
    				// 然后await()，如果数字不是0，那么久卡住，等待
    				
    				// 其他的线程可以调用coutnDown()，减1
    				// 如果数字减到0，那么之前所有在await的线程，都会逃出阻塞的状态
    				// 继续向下运行
    				
    				connectedSemaphore.await();
    			} catch(InterruptedException e) {
    				e.printStackTrace();
    			}
    
    			System.out.println("ZooKeeper session established......");
    		} catch (Exception e) {
    			e.printStackTrace();
    		}
    	}
    	
    	/**
    	 * 获取分布式锁
    	 * @param productId
    	 */
    	public void acquireDistributedLock(Long productId) {
    		String path = "/product-lock-" + productId;
    	
    		try {
    			zookeeper.create(path, "".getBytes(), 
    					Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);
    			System.out.println("success to acquire lock for product[id=" + productId + "]");  
    		} catch (Exception e) {
    			// 如果那个商品对应的锁的node，已经存在了，就是已经被别人加锁了，那么就这里就会报错
    			// NodeExistsException
    			int count = 0;
    			while(true) {
    				try {
    					Thread.sleep(20); 
    					zookeeper.create(path, "".getBytes(), 
    							Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);
    				} catch (Exception e2) {
    					e2.printStackTrace();
    					count++;
    					continue;
    				}
    				System.out.println("success to acquire lock for product[id=" + productId + "] after " + count + " times try......");
    				break;
    			}
    		}
    	}
    	
    	/**
    	 * 释放掉一个分布式锁
    	 * @param productId
    	 */
    	public void releaseDistributedLock(Long productId) {
    		String path = "/product-lock-" + productId;
    		try {
    			zookeeper.delete(path, -1); 
    		} catch (Exception e) {
    			e.printStackTrace();
    		}
    	}
    	
    	/**
    	 * 建立zk session的watcher
    	 * @author Administrator
    	 *
    	 */
    	private class ZooKeeperWatcher implements Watcher {
    
    		public void process(WatchedEvent event) {
    			System.out.println("Receive watched event: " + event.getState());
    			if(KeeperState.SyncConnected == event.getState()) {
    				connectedSemaphore.countDown();
    			} 
    		}
    		
    	}
    	
    	/**
    	 * 封装单例的静态内部类
    	 * @author Administrator
    	 *
    	 */
    	private static class Singleton {
    		
    		private static ZooKeeperSession instance;
    		
    		static {
    			instance = new ZooKeeperSession();
    		}
    		
    		public static ZooKeeperSession getInstance() {
    			return instance;
    		}
    		
    	}
    	
    	/**
    	 * 获取单例
    	 * @return
    	 */
    	public static ZooKeeperSession getInstance() {
    		return Singleton.getInstance();
    	}
    	
    	/**
    	 * 初始化单例的便捷方法
    	 */
    	public static void init() {
    		getInstance();
    	}
    	
    }
    




业务代码

1、主动更新

监听kafka消息队列，获取到一个商品变更的消息之后，去哪个源服务中调用接口拉取数据，更新到ehcache和redis中

先获取分布式锁，然后才能更新redis，同时更新时要比较时间版本

2、被动重建

直接读取源头数据，直接返回给nginx，同时推送一条消息到一个队列，后台线程异步消费

后台现成负责先获取分布式锁，然后才能更新redis，同时要比较时间版本




























---
title: elasticsearch(六)
categories:
- ELK
tags:
- elasticsearch


---


---
title: elasticsearch(五)
categories:
- ELK
tags:
- elasticsearch


---

### 定制化自己的dynamic mapping策略


1、定制dynamic策略

true：遇到陌生字段，就进行dynamic mapping
false：遇到陌生字段，就忽略
strict：遇到陌生字段，就报错

    PUT /my_index
    {
      "mappings": {
        "my_type": {
          "dynamic": "strict",
          "properties": {
            "title": {
              "type": "text"
            },
            "address": {
              "type": "object",
              "dynamic": "true"
            }
          }
        }
      }
    }
    
    PUT /my_index/my_type/1
    {
      "title": "my article",
      "content": "this is my article",
      "address": {
        "province": "guangdong",
        "city": "guangzhou"
      }
    }
    
    {
      "error": {
        "root_cause": [
          {
            "type": "strict_dynamic_mapping_exception",
            "reason": "mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed"
          }
        ],
        "type": "strict_dynamic_mapping_exception",
        "reason": "mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed"
      },
      "status": 400
    }
    
    PUT /my_index/my_type/1
    {
      "title": "my article",
      "address": {
        "province": "guangdong",
        "city": "guangzhou"
      }
    }
    
    GET /my_index/_mapping/my_type
    
    {
      "my_index": {
        "mappings": {
          "my_type": {
            "dynamic": "strict",
            "properties": {
              "address": {
                "dynamic": "true",
                "properties": {
                  "city": {
                    "type": "text",
                    "fields": {
                      "keyword": {
                        "type": "keyword",
                        "ignore_above": 256
                      }
                    }
                  },
                  "province": {
                    "type": "text",
                    "fields": {
                      "keyword": {
                        "type": "keyword",
                        "ignore_above": 256
                      }
                    }
                  }
                }
              },
              "title": {
                "type": "text"
              }
            }
          }
        }
      }
    }

2、定制dynamic mapping策略

（1）date_detection

默认会按照一定格式识别date，比如yyyy-MM-dd。但是如果某个field先过来一个2017-01-01的值，就会被自动dynamic mapping成date，后面如果再来一个"hello world"之类的值，就会报错。可以手动关闭某个type的date_detection，如果有需要，自己手动指定某个field为date类型。

    PUT /my_index/_mapping/my_type
    {
        "date_detection": false
    }

（2）定制自己的dynamic mapping template（type level）

    PUT /my_index
    {
        "mappings": {
            "my_type": {
                "dynamic_templates": [
                    { "en": {
                          "match":              "*_en", 
                          "match_mapping_type": "string",
                          "mapping": {
                              "type":           "string",
                              "analyzer":       "english"
                          }
                    }}
                ]
    }}}
    
    PUT /my_index/my_type/1
    {
      "title": "this is my first article"
    }
    
    PUT /my_index/my_type/2
    {
      "title_en": "this is my first article"
    }

title没有匹配到任何的dynamic模板，默认就是standard分词器，不会过滤停用词，is会进入倒排索引，用is来搜索是可以搜索到的
title_en匹配到了dynamic模板，就是english分词器，会过滤停用词，is这种停用词就会被过滤掉，用is来搜索就搜索不到了

（3）定制自己的default mapping template（index level）

    PUT /my_index
    {
        "mappings": {
            "_default_": {
                "_all": { "enabled":  false }
            },
            "blog": {
                "_all": { "enabled":  true  }
            }
        }
    }


### 基于scoll+bulk+索引别名实现零停机重建索引



1、重建索引

一个field的设置是不能被修改的，如果要修改一个Field，那么应该重新按照新的mapping，建立一个index，然后将数据批量查询出来，重新用bulk api写入index中

批量查询的时候，建议采用scroll api，并且采用多线程并发的方式来reindex数据，每次scoll就查询指定日期的一段数据，交给一个线程即可

（1）一开始，依靠dynamic mapping，插入数据，但是不小心有些数据是2017-01-01这种日期格式的，所以title这种field被自动映射为了date类型，实际上它应该是string类型的

    PUT /my_index/my_type/3
    {
      "title": "2017-01-03"
    }
    
    {
      "my_index": {
        "mappings": {
          "my_type": {
            "properties": {
              "title": {
                "type": "date"
              }
            }
          }
        }
      }
    }

（2）当后期向索引中加入string类型的title值的时候，就会报错
    
    PUT /my_index/my_type/4
    {
      "title": "my first article"
    }
    
    {
      "error": {
        "root_cause": [
          {
            "type": "mapper_parsing_exception",
            "reason": "failed to parse [title]"
          }
        ],
        "type": "mapper_parsing_exception",
        "reason": "failed to parse [title]",
        "caused_by": {
          "type": "illegal_argument_exception",
          "reason": "Invalid format: \"my first article\""
        }
      },
      "status": 400
    }

（3）如果此时想修改title的类型，是不可能的

    PUT /my_index/_mapping/my_type
    {
      "properties": {
        "title": {
          "type": "text"
        }
      }
    }
    
    {
      "error": {
        "root_cause": [
          {
            "type": "illegal_argument_exception",
            "reason": "mapper [title] of different type, current_type [date], merged_type [text]"
          }
        ],
        "type": "illegal_argument_exception",
        "reason": "mapper [title] of different type, current_type [date], merged_type [text]"
      },
      "status": 400
    }

（4）此时，唯一的办法，就是进行reindex，也就是说，重新建立一个索引，将旧索引的数据查询出来，再导入新索引

（5）如果说旧索引的名字，是old_index，新索引的名字是new_index，终端java应用，已经在使用old_index在操作了，难道还要去停止java应用，修改使用的index为new_index，才重新启动java应用吗？这个过程中，就会导致java应用停机，可用性降低

（6）所以说，给java应用一个别名，这个别名是指向旧索引的，java应用先用着，java应用先用goods_index alias来操作，此时实际指向的是旧的my_index

    PUT /my_index/_alias/goods_index

（7）新建一个index，调整其title的类型为string

    PUT /my_index_new
    {
      "mappings": {
        "my_type": {
          "properties": {
            "title": {
              "type": "text"
            }
          }
        }
      }
    }

（8）使用scroll api将数据批量查询出来

    GET /my_index/_search?scroll=1m
    {
        "query": {
            "match_all": {}
        },
        "sort": ["_doc"],
        "size":  1
    }
    
    {
      "_scroll_id": "DnF1ZXJ5VGhlbkZldGNoBQAAAAAAADpAFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAA6QRY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAAOkIWNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAADpDFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAA6RBY0b25zVFlWWlRqR3ZJajlfc3BXejJ3",
      "took": 1,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 3,
        "max_score": null,
        "hits": [
          {
            "_index": "my_index",
            "_type": "my_type",
            "_id": "2",
            "_score": null,
            "_source": {
              "title": "2017-01-02"
            },
            "sort": [
              0
            ]
          }
        ]
      }
    }

（9）采用bulk api将scoll查出来的一批数据，批量写入新索引

    POST /_bulk
    { "index":  { "_index": "my_index_new", "_type": "my_type", "_id": "2" }}
    { "title":    "2017-01-02" }

（10）反复循环8~9，查询一批又一批的数据出来，采取bulk api将每一批数据批量写入新索引

（11）将goods_index alias切换到my_index_new上去，java应用会直接通过index别名使用新的索引中的数据，java应用程序不需要停机，零提交，高可用

    POST /_aliases
    {
        "actions": [
            { "remove": { "index": "my_index", "alias": "goods_index" }},
            { "add":    { "index": "my_index_new", "alias": "goods_index" }}
        ]
    }

（12）直接通过goods_index别名来查询，是否ok

    GET /goods_index/my_type/_search

2、基于alias对client透明切换index

    PUT /my_index_v1/_alias/my_index    

client对my_index进行操作

reindex操作，完成之后，切换v1到v2

    POST /_aliases
    {
        "actions": [
            { "remove": { "index": "my_index_v1", "alias": "my_index" }},
            { "add":    { "index": "my_index_v2", "alias": "my_index" }}
        ]
    }

### 倒排索引组成结构以及其索引可变原因

倒排索引，是适合用于进行搜索的

倒排索引的结构

（1）包含这个关键词的document list

（2）包含这个关键词的所有document的数量：IDF（inverse document frequency）

（3）这个关键词在每个document中出现的次数：TF（term frequency）

（4）这个关键词在这个document中的次序

（5）每个document的长度：length norm

（6）包含这个关键词的所有document的平均长度

    word		doc1		doc2
    
    dog		*		*
    hello		*
    you				*

倒排索引不可变的好处

（1）不需要锁，提升并发能力，避免锁的问题

（2）数据不变，一直保存在os cache中，只要cache内存足够

（3）filter cache一直驻留在内存，因为数据不变

（4）可以压缩，节省cpu和io开销

倒排索引不可变的坏处：每次都要重新构建整个索引



### document写入原理（buffer，segment，commit）



（1）数据写入buffer

（2）commit point

（3）buffer中的数据写入新的index segment

（4）等待在os cache中的index segment被fsync强制刷到磁盘上

（5）新的index sgement被打开，供search使用

（6）buffer被清空

每次commit point时，会有一个.del文件，标记了哪些segment中的哪些document被标记为deleted了

搜索的时候，会依次查询所有的segment，从旧的到新的，比如被修改过的document，在旧的segment中，会标记为deleted，在新的segment中会有其新的数据

![image](https://img2018.cnblogs.com/blog/1279115/201809/1279115-20180928173806054-7488340.png)


### 优化写入流程实现NRT近实时（filesystem cache，refresh）



现有流程的问题，每次都必须等待fsync将segment刷入磁盘，才能将segment打开供search使用，这样的话，从一个document写入，到它可以被搜索，可能会超过1分钟！！！这就不是近实时的搜索了！！！主要瓶颈在于fsync实际发生磁盘IO写数据进磁盘，是很耗时的。

写入流程别改进如下：

（1）数据写入buffer

（2）每隔一定时间，buffer中的数据被写入segment文件，但是先写入os cache

（3）只要segment写入os cache，那就直接打开供search使用，不立即执行commit

数据写入os cache，并被打开供搜索的过程，叫做refresh，默认是每隔1秒refresh一次。也就是说，每隔一秒就会将buffer中的数据写入一个新的index segment file，先写入os cache中。所以，es是近实时的，数据写入到可以被搜索，默认是1秒。

POST /my_index/_refresh，可以手动refresh，一般不需要手动执行，没必要，让es自己搞就可以了

比如说，我们现在的时效性要求，比较低，只要求一条数据写入es，一分钟以后才让我们搜索到就可以了，那么就可以调整refresh interval
    
    PUT /my_index
    {
      "settings": {
        "refresh_interval": "30s" 
      }
    }

![image](https://img2018.cnblogs.com/blog/1279115/201809/1279115-20180928173825820-1036962115.png)



### 继续优化写入流程实现durability可靠存储（translog，flush）




再次优化的写入流程

（1）数据写入buffer缓冲和translog日志文件

（2）每隔一秒钟，buffer中的数据被写入新的segment file，并进入os cache，此时segment被打开并供search使用

（3）buffer被清空

（4）重复1~3，新的segment不断添加，buffer不断被清空，而translog中的数据不断累加

（5）当translog长度达到一定程度的时候，commit操作发生

      （5-1）buffer中的所有数据写入一个新的segment，并写入os cache，打开供使用
      
      （5-2）buffer被清空
      
      （5-3）一个commit ponit被写入磁盘，标明了所有的index segment
      
      （5-4）filesystem cache中的所有index segment file缓存数据，被fsync强行刷到磁盘上
      
      （5-5）现有的translog被清空，创建一个新的translog
  

基于translog和commit point，如何进行数据恢复

![image](https://img2018.cnblogs.com/blog/1279115/201809/1279115-20180928173913381-94687400.png)

fsync+清空translog，就是flush，默认每隔30分钟flush一次，或者当translog过大的时候，也会flush

    POST /my_index/_flush，一般来说别手动flush，让它自动执行就可以了

translog，每隔5秒被fsync一次到磁盘上。在一次增删改操作之后，当fsync在primary shard和replica shard都成功之后，那次增删改操作才会成功

但是这种在一次增删改时强行fsync translog可能会导致部分操作比较耗时，也可以允许部分数据丢失，设置异步fsync translog
    
    PUT /my_index/_settings
    {
        "index.translog.durability": "async",
        "index.translog.sync_interval": "5s"
    }
    
    
![image](https://img2018.cnblogs.com/blog/1279115/201809/1279115-20180928173915373-1240830958.png)

### 最后优化写入流程实现海量磁盘文件合并（segment merge，optimize）


每秒一个segment file，文件过多，而且每次search都要搜索所有的segment，很耗时

默认会在后台执行segment merge操作，在merge的时候，被标记为deleted的document也会被彻底物理删除

每次merge操作的执行流程

（1）选择一些有相似大小的segment，merge成一个大的segment

（2）将新的segment flush到磁盘上去

（3）写一个新的commit point，包括了新的segment，并且排除旧的那些segment

（4）将新的segment打开供搜索

（5）将旧的segment删除

    POST /my_index/_optimize?max_num_segments=1，尽量不要手动执行，让它自动默认执行就可以了

![image](https://img2018.cnblogs.com/blog/1279115/201809/1279115-20180928174226258-51864649.png)


### JAVA API：基于Java实现员工信息的增删改查





员工信息

姓名
年龄
职位
国家
入职日期
薪水


1、maven依赖

    <dependency>
        <groupId>org.elasticsearch.client</groupId>
        <artifactId>transport</artifactId>
        <version>5.2.2</version>
    </dependency>
    <dependency>
        <groupId>org.apache.logging.log4j</groupId>
        <artifactId>log4j-api</artifactId>
        <version>2.7</version>
    </dependency>
    <dependency>
        <groupId>org.apache.logging.log4j</groupId>
        <artifactId>log4j-core</artifactId>
        <version>2.7</version>
    </dependency>
    
    log4j2.properties
    
    appender.console.type = Console
    appender.console.name = console
    appender.console.layout.type = PatternLayout
    
    rootLogger.level = info
    rootLogger.appenderRef.console.ref = console

2、构建client

    Settings settings = Settings.builder()
            .put("cluster.name", "myClusterName").build();
    TransportClient client = new PreBuiltTransportClient(settings);
    
    TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)
            .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("host1"), 9300))
            .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("host2"), 9300));
    
    client.close();

3、创建document

    IndexResponse response = client.prepareIndex("index", "type", "1")
            .setSource(jsonBuilder()
                        .startObject()
                            .field("user", "kimchy")
                            .field("postDate", new Date())
                            .field("message", "trying out Elasticsearch")
                        .endObject()
                      )
            .get();

4、查询document

    GetResponse response = client.prepareGet("index", "type", "1").get();

5、修改document
    
    client.prepareUpdate("index", "type", "1")
            .setDoc(jsonBuilder()               
                .startObject()
                    .field("gender", "male")
                .endObject())
            .get();

6、删除document
    
    DeleteResponse response = client.prepareDelete("index", "type", "1").get();


### JAVA API：基于Java对员工信息进行复杂的搜索操作



    SearchResponse response = client.prepareSearch("index1", "index2")
            .setTypes("type1", "type2")
            .setQuery(QueryBuilders.termQuery("multi", "test"))                 // Query
            .setPostFilter(QueryBuilders.rangeQuery("age").from(12).to(18))     // Filter
            .setFrom(0).setSize(60)
            .get();

需求：

（1）搜索职位中包含technique的员工

（2）同时要求age在30到40岁之间

（3）分页查询，查找第一页

    GET /company/employee/_search
    {
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "position": "technique"
              }
            }
          ],
          "filter": {
            "range": {
              "age": {
                "gte": 30,
                "lte": 40
              }
            }
          }
        }
      },
      "from": 0,
      "size": 1
    }

告诉大家，为什么刚才一边运行创建document，一边搜索什么都没搜索到？？？？

近实时！！！

默认是1秒以后，写入es的数据，才能被搜索到。很明显刚才，写入数据不到一秒，我门就在搜索。

### JAVA API：基于Java对员工信息进行聚合分析


    SearchResponse sr = node.client().prepareSearch()
        .addAggregation(
            AggregationBuilders.terms("by_country").field("country")
            .subAggregation(AggregationBuilders.dateHistogram("by_year")
                .field("dateOfBirth")
                .dateHistogramInterval(DateHistogramInterval.YEAR)
                .subAggregation(AggregationBuilders.avg("avg_children").field("children"))
            )
        )
        .execute().actionGet();

我们先给个需求：

（1）首先按照country国家来进行分组

（2）然后在每个country分组内，再按照入职年限进行分组

（3）最后计算每个分组内的平均薪资

    PUT /company
    {
      "mappings": {
          "employee": {
            "properties": {
              "age": {
                "type": "long"
              },
              "country": {
                "type": "text",
                "fields": {
                  "keyword": {
                    "type": "keyword",
                    "ignore_above": 256
                  }
                },
                "fielddata": true
              },
              "join_date": {
                "type": "date"
              },
              "name": {
                "type": "text",
                "fields": {
                  "keyword": {
                    "type": "keyword",
                    "ignore_above": 256
                  }
                }
              },
              "position": {
                "type": "text",
                "fields": {
                  "keyword": {
                    "type": "keyword",
                    "ignore_above": 256
                  }
                }
              },
              "salary": {
                "type": "long"
              }
            }
          }
        }
    }
    
    GET /company/employee/_search
    {
      "size": 0,
      "aggs": {
        "group_by_country": {
          "terms": {
            "field": "country"
          },
          "aggs": {
            "group_by_join_date": {
              "date_histogram": {
                "field": "join_date",
                "interval": "year"
              },
              "aggs": {
                "avg_salary": {
                  "avg": {
                    "field": "salary"
                  }
                }
              }
            }
          }
        }
      }
    }
    
    Map<String, Aggregation> aggrMap = searchResponse.getAggregations().asMap();
    		StringTerms groupByCountry = (StringTerms) aggrMap.get("group_by_country");
    		Iterator<Bucket> groupByCountryBucketIterator = groupByCountry.getBuckets().iterator();
    		
    		while(groupByCountryBucketIterator.hasNext()) {
    			Bucket groupByCountryBucket = groupByCountryBucketIterator.next();
    			
    			System.out.println(groupByCountryBucket.getKey() + "\t" + groupByCountryBucket.getDocCount()); 
    			
    			Histogram groupByJoinDate = (Histogram) groupByCountryBucket.getAggregations().asMap().get("group_by_join_date"); 
    			Iterator<org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket> groupByJoinDateBucketIterator = groupByJoinDate.getBuckets().iterator();
    			 
    			while(groupByJoinDateBucketIterator.hasNext()) {
    				org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket groupByJoinDateBucket = groupByJoinDateBucketIterator.next();
    				
    				System.out.println(groupByJoinDateBucket.getKey() + "\t" + groupByJoinDateBucket.getDocCount()); 
    				
    				Avg avgSalary = (Avg) groupByJoinDateBucket.getAggregations().asMap().get("avg_salary");
    				System.out.println(avgSalary.getValue()); 
    			}
    		}
    		
    		client.close();
    	}







---
title: 分布式数据库（1）
categories:
- 分布式
tags:
- 分布式
- 分布式数据库

---





## **什么是分布式数据库**

- **分布式数据库是服务于写多读少、低延时、海量并发 OLTP 场景的，具备海量数据存储能力和高可靠性的关系型数据库**。
- 外部视角
  - 定义1.0 OLTP关系型数据库
    - 写多读少、低延时、高并发
      - 之所以强调写多读少，因为写操作的负载只能是单体数据库的主节点上，是无法 转移的;而读操作，如果对一致性要求不高可以转移到备节点，甚至在某些条件下还能保证一致 性。就是说单体数据库可以通过一主多备解决读负载大的问题，而无需引入分布式数据库
  - 定义 2.0 + 海量并发
    - 传统关系型数据库往往是单机模式，也就是主要负载运行在一台机器上。这样，数据库的 并发处理能力与单机的资源配置是线性相关的，所以并发处理能力的上限也就受限于单机 配置的上限。这种依靠提升单机资源配置来扩展性能的方式，被称为**垂直扩展**(Scale Up)。
    - 分布式数据库就不同了，在维持关系型数据库特性不变的基础上，它可以通过**水平扩展**，也就是增加机器数量的方式，提供远高于单体数据库的并发量。这个并发量几乎不受 单机性能限制，我将这个级别的并发量称为“海量并发”。
  - 定义 3.0 + 高可靠
    - 我猜你会建议用 RAID(独立冗余磁盘阵列)来提高磁盘的可靠性。这确实是一个办法，但 也会带来性能上的损耗和存储空间上的损失。分布式数据库的副本机制可以比 RAID 更好地平衡可靠性、性能和空间利用率三者的关系。**副本机制就是将一份数据同时存储在多个 机器上，形成多个物理副本。**
    - 回到数据库的话题上，可靠性还要更复杂一点，包括两个度量指标，恢复时间目标 (Recovery Time Objective, RTO)和恢复点目标(Recovery Point Objective, RPO)。**RTO 是指故障恢复所花费的时间，可以等同于可靠性;RPO 则是指恢复服务后丢 失数据的数量。**
      - 数据库存储着重要数据，而金融行业的数据库更是关系到客户资产安全，不能容忍任何数 据丢失。所以，数据库高可靠意味着 RPO 等于 0，RTO 小于 5 分钟。
      - 分布式数据 库则是一个很好的备选方案，它**凭借节点之间的互为备份、自动切换的机制**，降低了 x86 服务器的单点故障对系统整体的影响，提供了高可靠性保障。
  - 定义 4.0 + 海量存储
    - 虽然单体数据库依靠外置存储设备可以扩展存储能力，但这种方式本质上不是数据库的能 力。现在，借助分布式的**横向扩展架构**，通过物理机的本地磁盘就可以获得强大的存储能 力，这让海量存储成为分布式数据库的标配。

- 内部构成
  - **客户端组件 + 单体数据库**
    - 通过独立的逻辑层建立数据分片和路由规则，实现单体数据库的初步管理，使应用能够对 接多个单体数据库，实现并发、存储能力的扩展。其作为应用系统的一部分，对业务侵入 较为深。
    - 这种客户端组件的典型产品是 Sharding-JDBC。
  - **代理中间件 + 单体数据库**
    - 以独立中间件的方式，管理数据规则和路由规则，以独立进程存在，与业务应用层和单体 数据库相隔离，减少了对应用的影响。随着代理中间件的发展，还会衍生出部分分布式事 务处理能力。
    - 这种中间件的典型产品是 MyCat。
  - **单元化架构 + 单体数据库**
    - 单元化架构是对业务应用系统的彻底重构，**应用系统被拆分成若干实例，配置独立的单体 数据库，让每个实例管理一定范围的数据。**例如对于银行贷款系统，可以为每个支行搭建 独立的应用实例，管理支行各自的用户，当出现跨支行业务时，由应用层代码通过分布式 事务组件保证事务的 ACID 特性。
  - 看过这三种方案，我相信你能够明白，它们共同的特点是单体数据库仍然能够被应用系统 感知到。相反，分布式数据库则是**将技术细节收敛到产品内部，以一个整体面对业务应用。**



## **强一致性**

- 强一致性意味着什么

  - 有人说，只要使用了 Paxos 或者 Raft算法，就可以实现强一致性;也有人说，根据 CAP 原理只能三选二，分区容忍性和高可用 性又是必不可少的，所以分布式数据库是做不到强一致性的。可是，**这些观点或多或少都 是有问题的**。
  - 对于分布式系统而言，一致性是在探讨当系统内的一份逻辑数据存在多个物理的数据副本 时，对其执行读写操作会产生什么样的结果，这也符合 CAP 理论对一致性的表述。
  - 而在数据库领域，“一致性”与事务密切相关，又进一步细化到 ACID 四个方面。其中，I 所代表的隔离性(Isolation)，是“一致性”的核心内容，研究的就是如何协调事务之间 的冲突。
  - 因此，当我们谈论分布式数据库的一致性时，实质上是在谈论**数据一致性**和**事务一致性**两个方面。

- **数据一致性**

  - 包括分布式数据库在内的分布式存储系统，为了避免设备与网络的不可靠带来的影响，通 常会存储多个数据副本。逻辑上的一份数据同时存储在多个物理副本上，自然带来了数据 一致性问题。
  - 强一致性:MySQL全同步复制
    - 显然，用户获得响应时，主库和备库的数据副本已经达成一致，所以后续的读操作肯定是 没有问题的，但这种模式的副作用非常大，体现在性能差、可用性问题。
    - 集群规模越大，这些问题就越严重，所以全同步复制模式在生产系统中也很少使用。更进 一步说，在工程实践中，实现状态视角的强一致性需要付出的代价太大，尤其是与可用性 有无法回避的冲突，所以很多产品选择了状态视角的弱一致性。
  - 弱一致性:NoSQL 最终一致性
    - NoSQL 产品是应用弱一致性的典型代表，但对弱一致性的接受仍然是有限度的，这就是 BASE 理论中的 E 所代表的最终一致性(Eventually Consistency)，弱于最终一致性的产 品就几乎没有了。
    - 对于最终一致性，你可以这样理解:在主副本执行写操作并反馈成功时，不要求其他副本 与主副本保持一致，但在经过一段时间后这些副本最终会追上主副本的进度，重新达到数 据状态的一致。
    - 你再仔细推敲一下，是不是觉得这个定义还有点含糊?“经过一段时间”到底是多久呢? 几秒还是几分钟?如果是一个不确定的数值，怎么在工程中使用呢?

- **操作视角**

  - **写后读一致性**
    - 它也称为“读写一致 性”，或“读自己写一致性”(Read My Writes Consistency)。你可能觉得最后一个名 字听上去有些奇怪，但它却最准确地描述了这种一致性模型的使用效果。
    - 自己写入成功的任何数据，下一刻一定能读取到，其内容保证与自己最后一次写入完全一 致，这就是“读自己写一致性”名字的由来。
  - **单调读一致性**
    - 关于单调读一致性的定义，常见的解释是这样的:一个用户一旦读到某个值，不会读到比这 个值更旧的值。
    - 实现单调读一致性的方式，可以是将用户与副本建立固定的映射关系，比如使用哈希算法 将用户 ID 映射到固定副本上，这样避免了在多个副本中切换。
  - **前缀一致性**
    - 小明和小红的评论分别写入了节点 N1 和 N2，但是它们与 N3 同步数据时，由于网络传输 的问题，N3 节点接收数据的顺序与数据写入的顺序并不一致，所以小刚是**先看到答案后看 到问题**。
    - 显然，问题与答案之间是有因果关系的，但这种关系在复制的过程中被忽略了，于是出现 了异常。
    - 保持这种因果关系的一致性，被称为**前缀读**或**前缀一致性**(Consistent Prefix)。要实现 这种一致性，可以考虑在原有的评论数据上增加一种显式的因果关系，这样系统可以据此 控制在其他进程的读取顺序。
  - **线性一致性**
    - 在“前缀一致性”的案例中，问题与答案之间存在一种显式声明，但在现实中，多数场景 的因果关系更加复杂，也不可能要求全部做显式声明。
    - 线性一致性(Linearizability)就是建立在事件的先后顺序之上的。在线性一致性下，整个 系统表现得好像只有一个副本，所有操作被记录在一条时间线上，并且被原子化，这样任 意两个事件都可以比较先后顺序。
    - 但是，集群中的各个节点不能做到真正的时钟同步，这样节点有各自的时间线。那么，如 何将操作记录在一条时间线上呢?这就需要一个绝对时间，也就是**全局时钟**。
  - **因果一致性**
    - 既然线性一致性不够完美，那么有没有不依赖绝对时间的方法呢?
    - 因果一致性的基础是**偏序关系**，也就是说，部分事件顺序是可以比较的。至少一个节点内 部的事件是可以排序的，依靠节点的本地时钟就行了;节点间如果发生通讯，则参与通讯 的两个事件也是可以排序的，接收方的事件一定晚于调用方的事件。
    - 借助**逻辑时钟**仍然可以建立全序关系，当然这个全序关系是不够精确的。因为如果两个事 件并不相关，那么逻辑时钟给出的大小关系是没有意义的。
    - 多数观点认为，**因果一致性弱于线性一致性，但在并发性能上具有优势**，也足以处理多数 的异常现象，所以因果一致性也在工业界得到了应用。
    - 因果一致性是靠逻辑时钟确定偏序关系，**不需要应用介入**;而前缀一 致性靠事件之间显式声明的依赖关系，**可以在应用层处理**

- 有些时候 大家又会将 Paxos 称为一致性协议。你觉得这个“一致性协议”和数据一致性又是什么关 系呢?

  - 数据一致性是从数据的用户视角出发对数据属性的描述，而paxos协议是达成共识 的过程的一种实现方式，是从数据的生产者或者维护者角度出发的
    - 从状态视角看，是不是只有全同步这种方式实现了强一致性，即使像paxos、raft这些实现了操作上线性一致性的算法，从状态视角看也不是强一致的。
    - 然而全同步降低了系统的可用性，paxos、raft不保证所有节点状态的一致，而是通过额外 的算法来保证操作视角的一致性，同时提高了系统的可用性。
  - **一致性模型里有两个要点，读写策略和多副本状态**
    - Raft是多数派协议，从写入成功那一刻的数据状态来说，肯定不是一致的。不过，通过操作方面 的封装，约定由主副本对外提供服务，所以不会体现出副本间的差异。一致性模型，除了副本的 状态，还要看读写操作。**最终一致性的定义，其实只是描述了副本的状态而已。**我认为，一致性 模型，主要还是从读写操作的效果来分析，也数据副本的一致性有关但不是强依赖。比如，如果 不使用Raft，用半同步，也可以做到线性一致性。
  - CAP的C也是Consistency，是多副本、单操作的数据一致性;而ACID里的 C是指单副本、多操作的事务一致性。
    - Paxos这类共识算法，可以看作是复制协议的一种，虽然有 时也叫做一致性协议，但这个一致性是指Consensus。**Consensus是实现数据一致性目标下的具 体技术，但并不是唯一的选择。**采用主从复制也可以达到同样效果，比如PGXC风格的分布式数据库就是采用主从复制的方式。
    - CAP中的C就是Consistency，是数据一致性，也是我们所说的操作视角的一致 性，这里包含的多副本和读写策略两层含义。共识算法是复制协议层面的内容，并不一定对操作 做严格定义。比如，就算我们使用Raft算法，但是如果开放了Follower读，也有可能达不到线性 一致性或因果一致性的。事实上，CockroachDB的Follower读就是这样的。

- **事务一致性**

  - 虽然 ACID 名义上并列为事务的四大特性，但它们对于数据库的重要程度并不相同。
    - 第一个是**一致性，它无疑是其中存在感最低的特性，可以看作是对 “事务”整体目标的阐 述。**它并没有提出任何具体的功能需求，所以在数据库中也很难找到针对性的设计。
    - 第二个是持久性，它不仅是对数据库的基本要求。如果你仔细琢磨下持久性的定义，就会 发现**它的核心思想就是要应对系统故障**。怎么理解系统故障呢?我们可以把故障分为两 种。
      - **存储硬件无损、可恢复的故障**。这种情况下，主要依托于预写日志(Write Ahead Log, WAL)保证第一时间存储数据。WAL 采用顺序写入的方式，可以保证数据库的低延时 响应。WAL 是单体数据库的成熟技术，NoSQL 和分布式数据库都借鉴了过去。
      - **存储硬件损坏、不可恢复的故障**。这种情况下，需要用到日志复制技术，将本地日志及 时同步到其他节点。实现方式大体有三种:第一种是单体数据库自带的同步或半同步的 方式，其中半同步方式具有一定的容错能力，实践中被更多采用;第二种是将日志存储 到共享存储系统上，后者会通过冗余存储保证日志的安全性，亚马逊的 Aurora 采用了 这种方式，也被称为 Share Storage;第三种是基于 Paxos/Raft 的共识算法同步日志 数据，在分布式数据库中被广泛使用。无论采用哪种方式，目的都是保证在本地节点之 外，至少有一份完整的日志可用于数据恢复。
    - 第三个是原子性，是数据库区别于其他存储系统的重要标志。在单体数据库时代，原子性 问题已经得到妥善解决，但**随着向分布式架构的转型，在引入不可靠的网络因素后，原子 性又成为一个新的挑战。**
      - **分布式数据库是在分布式架构上实现的关系型数 据库，那么就必须支持事务，首先就要支持原子性。**原子性，在实现机制上较为复杂，目标却很简单，和分成多个级别的隔离性不同，原子性就只有支持和不支持的区别。
    - 最后一个是隔离性，它是事务中最复杂的特性。隔离性分为多个隔离级别，较低的隔离级 别就是在正确性上做妥协，将一些异常现象交给应用系统的开发人员去解决，从而获得更 好的性能。
  - 1995 年，Jim Gray 等人 发表了论文“A Critique of ANSI SQL Isolation Levels”(以下简称 **Critique**)，对于 事务隔离性进行了更加深入的分析
    - **幻读和写倾斜**无疑则是通往最高隔离级别的两座大山
      - Critique 对幻读的描述大致是这样的，事务 T1 使用特定的查询条件获得一个结果集，事 务 T2 插入新的数据，并且这些数据符合 T1 刚刚执行的查询条件。T2 提交成功后，T1 再 次执行同样的查询，此时得到的结果集会增大。这种异常现象就是幻读。
      - 不少人会将幻读与不可重复读混淆，这是因为它们在自然语义上非常接近，都是在一个事 务内用相同的条件查询两次，但两次的结果不一样。差异在于，**对不可重复读来说，第二 次的结果集相对第一次，有些记录被修改(Update)或删除(Delete)了;而幻读是第二 次结果集里出现了第一次结果集没有的记录 (Insert)。一个更加形象的说法，幻读是在第一 次结果集的记录“间隙”中增加了新的记录**。所以，MySQL 将防止出现幻读的锁命名为间 隙锁(Gap Lock)。
    - 写倾斜要稍微复杂一点，我用一个黑白球的例子来说明
      - 首先，箱子里有三个白球和三个黑球，两个事务(T1,T2)并发修改，不知道对方的存在。 T1 要让 6 个球都变成白色;T2 则希望 6 个球都变成黑色。
      - 根据可串行化的定义，“多事务并行执行所得到的结果，与串行执行(一个接一个)完全 相同”。比照两张图，很容易发现事务并行执行没有达到串行的同等效果，所以这是一种 异常现象。也可以说，写倾斜是一种更不易察觉的更新丢失。
  - 隔离性的产品实现
    - 第一个方向是，用真正的串行化实现“可串行化”隔离。我们往往认为多线程并发在性能 上更优，但 **Redis 和 VoltDB** 确实通过串行化执行事务的方式获得了不错的性能。考虑到 VoltDB 作为一款分布式数据库的复杂度，其成功就更为难得了。我想，其中部分原因可能 在于内存的大量使用，加速了数据计算的过程。另外，VoltDB 以存储过程为逻辑载体的方 式，也使得事务有了更多的优化机会。
    - 如果说第一个方向有点剑走偏锋，那第二个方向就是硬桥硬马了。没错，还是在并发技术 上继续做文章。PostgreSQL 在 2008 年提出了 **Serializable Snapshot Isolation (SSI)， 这实际就是可串行化**。而后，兼容 PostgreSQL 生态的 CockroachDB，也同样选择支持 SSI，而且是唯一支持的隔离级别。

- **分布式数据库的强一致性**

  - | 隔离级别         | 线性一致性     | 因果一致性  |
    | ---------------- | -------------- | ----------- |
    | 可串行化（SSL）  | Spanner        | CockroachDB |
    | 快照隔离（SL）   | TiDb           | YugabyteDB  |
    | 可重复度读（RR） | GolenDB        |             |
    | 已提交读（RC）   | OceanBase 2.0+ |             |





## NewSQL和PGXC

- **总的来说，分布式数据库大多可以分为两种架构风格**
  - 一种是 NewSQL，它的代表系统是 Google Spanner;
  - 另一种是从单体数据库中间件基础上演进出来的，被称为 Prxoy 风 格，没有公认的代表系统，便于理解，所以选了一个出现较早的产品来指代这种风 格，这就是 PostgreSQL-XC(下文简称 PGXC)。
- 数据库的基本架构
  - 数据库从逻辑上拆分为 5 个部分，分别是客户端 通讯管理器 (Client Communications Manager)、查询处理器(Relational Query Processor)、事务存储管理器(Transactional Storage Manager)、进程管理器 (Process Manager)和共享组件与工具 (Shared Components and Utilities)，每个部 分下面又可以拆分成一些组件。
  - **客户端通讯管理器。**这是应用开发者能够直观感受到的模块，通常我们使用 JDBC 或者 ODBC 协议访问数据库时，连接的就是这个部分。
  - **进程管理器。**连接建好了，数据库会为客户端分配一个进程，客户端后续发送的所有操 作都会通过对应的进程来执行。
  - **查询处理器。**它包括四个部分，功能上是顺序执行的。首先是**解析器**，它将接收到的 SQL 解析为内部的语法树。然后是**查询重写(Query Rewrite)**，它也被称为逻辑优 化，主要是依据关系代数的等价变换，达到简化和标准化的目的，比如会消除重复条件 或去掉一些无意义谓词 ，还有将视图替换为表等操作。再往后就是**查询算法优化 (Query Optimizer)**，它也被称为物理优化，主要是根据表连接方式、连接顺序和排 序等技术进行优化，我们常说的基于规则优化(RBO)和基于代价优化(CBO)就在这 部分。最后就是**计划执行器(Plan Executor)**，最终执行查询计划，访问存储系统。
  - **事务存储管理器。**它包括四个部分，其中访问方式(Access Methods)是指数据在磁 盘的具体存储形式。锁管理(Lock Manager)是指并发控制。日志管理(Log Manager)是确保数据的持久性。缓存管理(Buffer Manager)则是指 I/O 操作相关 的缓存控制。
  - **共享组件和工具。**在整个过程中还会涉及到的一些辅助操作，当然它们对于数据库的运 行也是非常重要的。例如编目数据管理器(Catalog Manager)会记录数据库的表、字 段、视图等元数据信息，并根据这些信息来操作具体数据内容。**复制机制 (Replication)也很重要，它是实现系统高可靠性的基础**，在单体数据库中，通过主备 节点复制的方式来实现数据的复制。
- **PGXC**:单体数据库的自然演进
  - 单体数据库的功能看似已经很完善了，但在面临高并发场景的时候，还是会碰到写入性能 不足的问题，很难解决。因此，也就有了向分布式数据库演进的动力。要解决**写入性能不足的问题**，大家首先想到的，最简单直接的办法就是**分库分表**。
    - 分库分表方案就是在多个单体数据库之前增加代理节点，**本质上是增加了 SQL 路由功能**。 这样，**代理节点首先解析客户端请求，再根据数据的分布情况，将请求转发到对应的单体 数据库。**
    - **代理节点需要实现三个主要功能，它们分别是客户端接入、简单的查询处理器和进程管理 中的访问控制。**
    - 另外，分库分表方案还有一个重要的功能，那就是**分片信息管理**，分片信息就是数据分布 情况，是区别于编目数据的一种元数据。不过考虑到分片信息也存在多副本的一致性的问 题，大多数情况下它会独立出来
    - 显然，如果把每一次的事务写入都限制在一个单体数据库内，业务场景就会很受局限。因 此，跨库事务成为必不可少的功能，但是单体数据库是不感知这个事情的，所以我们就要**在代理节点增加分布式事务组件**。
    - 同时，简单的分库分表不能满足全局性的查询需求，因为每个数据节点只能看到一部分数 据，有些查询运算是无法处理的，比如排序、多表关联等。所以，**代理节点要增强查询计算能力，支持跨多个单体数据库的查询**。
    - 随着分布式事务和跨节点查询等功能的加入，**代理节点已经不再只是简单的路由功能，更 多时候会被称为协调节点。**
  - **很多分库分表方案会演进到这个阶段，比如 MyCat。这时离分布式数据库还差重要的一 步，就是全局时钟**
    - 加上这最后一块拼图，PGXC 区别于单体数据库的功能也就介绍完整了，它们是分片、分 布式事务、跨节点查询和全局时钟。
  - **协调节点与数据节点，实现了一定程度上的计算与存储分离，这也是所有分布式数据库的 一个架构基调。**但是，因为 PGXC 的数据节点本身就是完整的单体数据库，所以也具备很 强的计算能力。
- **NewSQL**:革命性的新架构
  - NewSQL 也叫原生分布式数据库，我 觉得这个名字能更准确地体现这类架构风格的特点，就是说**它的每个组件在设计之初都是基于分布式架构的，不像 PGXC 那样带有明显的单体架构痕迹。**
    - NewSQL 的基础是 NoSQL，更具体地说，是类似 BigTable 的**分布式键值(K/V)系统**。 分布式键值系统选择做了一个减法，**完全放弃了数据库事务处理能力，然后将重点放在对 存储和写入能力的扩展上**，这个能力扩展的基础就是**分片**。引入分片的另一个好处是，系 统能够以更小的粒度调度数据，实现各节点上的存储平衡和访问负载平衡。
    - **分布式键值系统由于具备这些鲜明的特点，所以在不少细分场景获得了成功(比如电商网 站对于商品信息的存储)，但在面对大量的事务处理场景时就无能为力了(比如支付系 统)。**这种状况直到 **Google Spanner** 横空出世才被改变，因为 Spanner 基于 BigTable 构建了新的事务能力。
  - 除了上述内容，NewSQL 还有两个重要的革新，分别出现在**高可靠机制和存储引擎**的设计上。
    - 高可靠机制的变化在于，放弃了粒度更大的主从复制，转而**以分片为单位采用 Paxos 或 Raft 等共识算法**。这样，NewSQL 就实现了更小粒度的高可靠单元，获得了更高的系统整 体可靠性。存储引擎层面，则是使用 **LSM-Tree 模型**替换 B+ Tree 模型，大幅提升了写入 性能。
    - Spanner 是 NewSQL 的开山鼻祖，这个不用说了;其他知名度比较高的产品有 CockroachDB、TiDB 和 YugabyteDB，这三款数据库都宣称设计灵感来自 Spanner
  - 当然，**NewSQL 的架构设计也不是完美无缺**。比如，**作为一个计算与存储分离得更加彻底 的架构，NewSQL 的计算节点需要借助网络才能与存储节点通讯**，这意味着要花费更大的 代价来传输数据。随着 NewSQL 分布式数据库的应用实践越来越多，很多产品为了获得更 好的计算性能，会尽量将更多计算下压到存储节点执行。这种架构上的修正，似乎也可以 理解为，NewSQL 朝 PGXC 的方向做了一点回拨。



## 全局时钟

- TrueTime 作为全局时钟的一种实现形式，它是 Google通过 GPS 和原子钟两种方式混合提供的授时机制，误差可以控制在 7 毫秒以内。正是在这 7 毫秒内，时光是可能倒流的。

- 线性一致性，它的基础就是全局时钟，还有后面会讲到的多版本并发控制(MVCC)、快照、乐观 协议与悲观协议，都和时间有关。

- **常见授时方案**

  - 三个要素

    - 时间源:单个还是多个
    - 使用的时钟类型:物理时钟还是混合逻辑时钟
    - 授时点:一个还是多个

  - 常见的方案主要只有 4 类

    - NTP(Network Time Protocol)误差大， 也不能保证单调递增，所以就没有单独使用 NTP 的产品

    - |          | 物理时钟            | 物理时钟 | 混合逻辑时钟       | 混合逻辑时钟 |
      | -------- | ------------------- | -------- | ------------------ | ------------ |
      |          | 多时间源            | 单时间源 | 多时间源           | 单时间源     |
      | 单点授时 | N/A                 | N/A      | N/A                | TSO（TiDB）  |
      | 多点授时 | TrueTime（Spanner） | NTP      | HLC（CockroachDB） | STP（巨杉）  |

  - **TrueTime**

    - 采用了多点授时机制，就是说集群内有多个时间服务器都可以提供授时 服务。
    - 例如，A、B 两个进程先后调用 TrueTime 服务，各自拿到一个时间区间，如果在其中随机选择，则可能出现 B 的时间早 于 A 的时间。不只是 TrueTime，**任何物理时钟都会存在时钟偏移甚至回拨。**
    - **单个物理时钟会产生误差，而多点授时又会带来整体性的误差**，那 TrueTime 为什么还要 这么设计呢?
      - 因为**它也有两个显著的优势**:首先是**高可靠高性能**，多时间源和多授时点实现了完全的去 中心化设计，不存在单点;其次是**支持全球化部署**，客户端与时间服务器的距离也是可控 的，不会因为两者通讯延迟过长导致时钟失效。

  - **HLC**

    - CockroachDB 和 YugabyteDB 也是以高性能高可靠和全球化部署为目标，不过 Truetime 是 Google 的独门绝技，它依赖于特定硬件设备的思路，不适用于开源软件。所以，它们 使用了混合逻辑时钟(Hybrid Logical Clock，HLC)
    - 同样是多时间源、多点授时，但时钟采用了物理时钟与逻辑时钟混合的方式。HLC 在实现机制上也是蛮复杂的，而且和 TrueTime 同样有整体性的时间误差。

  - **TSO**

    - 其他的分布式数据库大多选择了单时间源、单点授时的方式，承担这个功能的组件在 NewSQL 风格架构中往往被称为 TSO(Timestamp Oracle)，而**在 PGXC 风格架构中被 称为全局事务管理器**(Golobal Transcation Manager，GTM)。**这就是说一个单点递增的时间戳和全局事务号基本是等效的。**
    - 这种授时机制的最大优点就是实现简便，如果能够 保证时钟单调递增，还可以简化事务冲突时的设计。但缺点也很明显，集群不能大范围部 署，同时性能也有上限。TiDB、OceanBase、GoldenDB 和 TBase 等选择了这个方向。

  - **STP**

    - 最后，还有一些小众的方案，比如巨杉的 STP(SequoiaDB Time Protoco)。它采用了单时 间源、多点授时的方式，优缺点介于 HLC 和 TSO 之间。

- **中心化授时:TSO（TiDB）**

  - **TiDB 的全局时钟是一个数值，它由两部分构成，其中高位是物理时间**，也就是操作系统的 毫秒时间;**低位是逻辑时间**，是一个 18 位的数值。那么从存储空间看，1 毫秒最多可以产 生 262,144 个时间戳(2^18)，这已经是一个很大的数字了，一般来说足够使用了。
  - **单点授时首先要解决的肯定是单点故障问题**。TiDB 中提供授时服务的节点被称为 Placement Driver，简称 PD。多**个 PD 节点构成一个 Raft 组**，这样通过共识算法可以保 证在主节点宕机后马上选出新主，在短时间内恢复授时服务。
  - 如何保证新主产生的时间戳一定大于旧主呢?那就必须将旧主的时间戳存储 起来，存储也必须是高可靠的，所以 **TiDB 使用了 etcd**。但是，每产生一个时间戳都要保 存吗?显然不行，那样时间戳的产生速度直接与磁盘 I/O 能力相关，会存在瓶颈的。
  - TiDB的PD虽然是高可靠的，但工作的只是主节点，所以还是**单点授时**;**多时间源**，是说多个独立提供时间的实例，比如部分原子钟和GPS坏掉了， 其他的原子钟可以照常提供时间不受影响。
  - TiDB 采用**预申请时间窗口**的方式
    - 当前 PD(主节点)的系统时间是 103 毫秒，PD 向 etcd 申请了一个“可分配的时间窗 口”。要知道时间窗口的跨度是可以通过参数指定的，系统的默认配置是 3 毫秒，示例采 用了默认配置，所以这个窗口的起点是 PD 当前时间 103，时间窗口的终点就在 106 毫秒 处。写入 etcd 成功后，PD 将得到一个从 103 到 106 的“可分配时间窗口”，在这个 时间窗口内 PD 可以使用系统的物理时间作为高位，拼接自己在内存中累加的逻辑时间， 对外分配时间戳。
    - 上述设计意味着，**所有 PD 已分配时间戳的高位，也就是物理时间，永远小于 etcd 存储的 最大值。**那么，如果 PD 主节点宕机，新主就可以读取 etcd 中存储的最大值，在此基础上 申请新的“可分配时间窗口”，这样新主分配的时间戳肯定会大于旧主了。
    - 此外，为了降低通讯开销，**每个客户端一次可以申请多个时间戳**，时间戳数量作为参数， 由客户端传给 PD。**但要注意的是，一旦在客户端缓存，多个客户端之间时钟就不再是严格 单调递增的**，这也是追求性能需要付出的代价。

- **分布式授时:HLC(CockroachDB)**

  - 假如我们有 ABCD 四个节点，方框是节点上发生的事件，方框内的**三个数字依次是节点的 本地物理时间(简称本地时间，Pt)、HLC 的高位(简称 L 值)和 HLC 的低位(简称 C 值)。**
  - A 节点的本地时间初始值为 10，其他节点的本地时间初始值都是 0。四个节点的第一个事 件都是在节点刚启动的一刻发生的。首先看 A1，它的 HLC 应该是 (10,0)，其中高位直接 取本地时间，低位从 0 开始。同理，其他事件的 HLC 都是 (0,0)。
  - **事件 D2 发生时，首先取上一个事件 D1 的 L 值和本地时间比较**。
    - L 值等于 0，本地时间已 经递增变为 1，取最大值，那么用本地时间作为 D2 的 L 值。高位变更了，低位要归零， 所以 D2 的 HLC 就是 (1,0)。
  - 如果节点间有调用关系，计时逻辑会更复杂一点。
    - **我们看事件 B2，要先判断 B2 的 L 值， 就有三个备选**:
      - 本节点上前一个事件 B1 的 L 值
      - 当前本地时间
      - 调用事件 A1 的 L 值，A1 的 HLC 是随着函数调用传给 B 节点的
    - 这三个值分别是 0、1 和 10。按照规则取最大值，所以 B2 的 L 值是 10，也就是 A1 的 L 值，而 C 值就在 A1 的 C 值上加 1，最终 B2 的 HLC 就是 (10,1)。
  - B3 事件发生时，发现当前本地时间比 B2 的 L 值还要小，所以沿用了 B2 的 L 值，而 C 值 是在 B2 的 C 值上加一，最终 B3 的 HLC 就是 (10,2)。

  



## 分片机制

- 大规模的业务应用下，单体数据库遇 到的主要问题是什么?首先就是写入性能不足，另外还 有存储方面的限制。而分片就是解决性能和存储这两个问题的关键设计，甚至不仅是分布 式数据库，在所有分布式存储系统中，分片这种设计都是广泛存在的。

  - 分布式数据库的分片与单体数据库的分区非常相似，区别在于:分区虽然可以将数据表按 照策略切分成多个数据文件，但这些文件仍然存储在单节点上;而**分片则可以进一步根据 特定规则将切分好的文件分布到多个节点上，从而实现更强大的存储和计算能力。**

- 分片机制通常有两点值得关注

  - 分片策略

    - 主要有 Hash(哈希)和 Range(范围)两种

  - 分片的调度机制

    - 分为静态与动态两种。静态意味着分片在节点上的分布基本是固定的，即使移动也需要人 工的介入;动态则是指通过调度管理器基于算法在各节点之间自动地移动分片。

    - |       | 静态        | 动态   |
      | ----- | ----------- | ------ |
      | Hash  | PGXC/NewSQL | N/A    |
      | Range | PGXC        | NewSQL |

- **PGXC**

  - **Hash** **分片**
    - Hash 分片，就是按照数据记录中指定关键字的 Hash 值将数据记录映射到不同的分片中。
      - **因为 Hash 计算会过滤掉数据原有的业务特性，所以可以保证数据非常均匀地分布到多个 分片上，这是 Hash 分片最大的优势**，而且它的实现也很简洁。但示例中采用的分片方法 直接用节点数作为模，**如果系统节点数量变动，模也随之改变，数据就要重新 Hash 计 算，从而带来大规模的数据迁移。显然，这种方式对于扩展性是非常不友好的。**
    - 那接下来的问题就是，我们需要找一个方法提升系统的扩展性。你可能猜到了，这就是**一致性 Hash**
      - **要在工业实践中应用一致性 Hash 算法，首先会引入虚拟节点，每个虚拟节点就是一个分 片。**为了便于说明，我们在这个案例中将分片数量设定为 16。但实际上，因为分片数量决 定了集群的最大规模，所以它通常会远大于初始集群节点数。
      - 16 个分片构成了整个 Hash 空间，数据记录的主键和节点都要通过 Hash 函数映射到这个 空间。这个 Hash 空间是一个 Hash 环。**节点和数据都通过 Hash 函数映射到 Hash 环上，数据按照顺时针找到最近的节点。**
    - **Hash 函数的优点是数据可以较为均匀地分配到各节点，并发写入性能更好。**
      - **本质上，Hash 分片是一种静态分片方式，必须在设计之初约定分片的最大规模。同时，因 为 Hash 函数已经过滤掉了业务属性，也很难解决访问业务热点问题**。所谓业务热点，就 是由于局部的业务活跃度较高，形成系统访问上的热点。这种情况普遍存在于各类应用 中，比如电商网站的某个商品卖得比较好，或者外卖网站的某个饭店接单比较多，或者某 个银行网点的客户业务量比较大等等。
  - **Range** **静态分片**
    - **与 Hash 分片不同，Range 分片的特点恰恰是能够加入对于业务的预估**。例如，我们 用“Location”作为关键字进行分片时，不是以统一的行政级别为标准。因为注册地在北 京、上海的用户更多，所以这两个区域可以按照区县设置分片，而海外用户较少，可以按 国家设置为分片。这样，分片间的数据更加平衡。
    - 相对 Hash 分片，Range 分片的适用范围更加广泛。其中一个非常重要的原因是，**Range 分片可以更高效地扫描数据记录，而 Hash 分片由于数据被打散，扫描操作的 I/O 开销更 大。**但是，**PGXC 的 Range 分片受限于单体数据库的实现机制，很难随数据变动和负载变化而调整。**
  - 虽然有些 PGXC 同时支持两种分片方式，但 Hash 分片仍是主流，比如 GoldenDB 默认使 用 Hash 分片，而 TBase 仅支持 Hash 分片。

- **NewSQL**

  - 总体上，NewSQL 也是支持 Hash 和 Range 两种分片方式的。具体就产品来说， CockroachDB 和 YugabyteDB 同时支持两种方式，TiDB 仅支持 Range 分片
  - **Range** **动态分片**
    - **NewSQL 的 Range 分片，多数是用主键作为关键字来分片的**，当然主键可以是系统自动 生成的，也可以是用户指定的。既然提供了用户指定主键的方式，那么理论上可以通过设 定主键的产生规则，控制数据流向哪个分片。**但是，主键必须保证唯一性，甚至是单调递 增的，导致这种控制就会比较复杂，使用成本较高**。所以，我们基本可以认为，分片是一 个系统自动处理的过程，用户是感知不到的。这样做的好处显然是提升了系统的易用性。
    - 我们将 NewSQL 的 Range 分片称为动态分片，主要有两个原因:
      - **分片可以自动完成分裂与合并**
        - 当单个分片的数据量超过设定值时，分片可以一分为二，这样就可以保证每个分片的数据 量较为均衡。多个数据量较少的分片，会在一定的周期内被合并为一个分片。
      - **可以根据访问压力调度分片**
        - 我们看到系统之所以尽量维持分片之间，以及节点间的数据量均衡，存储的原因外，还可 以更大概率地将访问压力分散到各个节点上。但是，有少量的数据可能会成为访问热点， 就是上面提到的**业务热点**，从而打破这种均衡。
        - 这时候，系统会根据负载情况，将分片分别调度到不同的节点，来均衡访问压力。
  - **存储均衡**和**访问压力均衡**，是 NewSQL 分片调度机制普遍具备的两项能力。此外，还有两 项能力在Spanner 论文中被提及，但在其他产品中没有看到工程化实现。
    - **减少分布式事务**
      - 对分布式数据库来说，有一个不争的事实，那就是分布式事务的开销永远不会小于单节点 本地事务的开销。因此，所有分布式数据库都试图通过减少分布式事务来提升性能。
      - Spanner 在 Tablet，也就是 Range 分片，之下增加了目录(Directory)，作为数据调度 的最小单位，它的调度范围是可以跨 Tablet 的。**通过调度 Directory 可以将频繁参与同样 事务的数据，转移到同一个 Tablet 下，从而将分布式事务转换为本地事务。**
    - **缩短服务延时**
      - 对于全球化部署的分布式数据库，数据可能存储在相距很远的多个数据中心，**如果用户需 要访问远端机房的数据，操作延时就比较长，这受制于数据传输速度。而 Spanner 可以将 Directory 调度到靠近用户的数据中心，缩短数据传输时间。**当然，这里的调度对象都是数据的主副本，跨中心的数据副本仍然存在，负责保证系统整体的高可靠性。
      - Directory 虽然带来新的特性，但显然也削弱了分片的原有功能，分片内的记录不再连续， 扫描要付出更大成本。而**减少分布式事务和靠近客户端位置这本身就是不能兼顾的，再加 上存储和访问压力，分片调度机制要在四个目标间进行更复杂的权衡。**

- **分片与高可靠的关系**

  - NewSQL 与 PGXC 的区别在于，**对于 NewSQL 来说，分片是高可靠的最小单元;而对于 PGXC，分片的高可靠要依附于节点的高可靠。**
  - **NewSQL 的实现方式是复制组(Group)。在产品层面，通常由一个主副本和若干个副本 组成，通过 Raft 或 Paxos 等共识算法完成数据同步**，称为 Raft Group 或 Paxos Group，所以我们简称这种方式为 Group。因为不相关的数据记录会被并发操作，所以同 一时刻有多个 Group 在工作。因此，NewSQL 通常支持 Multi Raft Group 或者 Multi Paxos Group。
    - **每个 Group 是独立运行的，只是共享相同的网络和节点资源，所以不同复制组的主副本是 可以分布在不同节点的。**
  - **PGXC 的最小高可靠单元由一个主节点和多个备节点组成**，我们借用 TDSQL 中的术语，将 其称为 Set。一个 PGXC 是由多个 Set 组成。S**et 的主备节点间复制，多数采用半同步复 制，平衡可靠性和性能。这意味着，所有分片的主副本必须运行在 Set 的主节点上。**
  - **从架构设计角度看，Group 比 Set 更具优势**，原因主要有两个方面。首先，Group 的**高可靠单元更小**，出现故障时影响的范围就更小，系统整体的可靠性就更高。其次，**在主机房 范围内，Group 的主副本可以在所有节点上运行，资源可以得到最大化使用，而 Set 模式 下，占大多数的备节点是不提供有效服务的，资源白白浪费掉。**



## 数据复制

- **分片元数据的存储**
  - 我们知道，**在任何一个分布式存储系统中，收到客户端请求后，承担路由功能的节点首先 要访问分片元数据(简称元数据)，确定分片对应的节点，然后才能访问真正的数据。**这 里说的元数据，一般会包括分片的数据范围、数据量、读写流量和分片副本处于哪些物理 节点，以及副本状态等信息。
  - 从存储的角度看，元数据也是数据，但特别之处在于每一个请求都要访问它，所以元数据 的存储很容易成为整个系统的性能瓶颈和高可靠性的短板。如果系统支持动态分片，那么 分片要自动地分拆、合并，还会在节点间来回移动。这样，元数据就处在不断变化中，又 带来了**多副本一致性(Consensus)的问题**。
- **静态分片**
  - 最简单的情况是静态分片。我们可以忽略元数据变动的问题，只要把元数据复制多份放在 对应的工作节点上就可以了，这样同时兼顾了性能和高可靠
  - **但如果要更新分片信息，这种方式显然不适合，因为副本数量过多，数据同步的代价太大了**。所以**对于动态分片，通常是不会在有工作负载的节点上存放元数据的**。
    - 那要怎么设计呢?有一个凭直觉就能想到的答案，那就是**专门给元数据搞一个小规模的集群，用 Paxos 协议复制数据**。这样保证了高可靠，数据同步的成本也比较低。
    - TiDB 大致就是这个思路，但具体的实现方式会更巧妙一些。
- **TiDB:无服务状态**
  - 在 TiDB 架构中，**TiKV** 节点是实际存储分片数据的节点，而元数据则由 Placement Driver节点管理。Placement Driver 这个名称来自 Spanner 中对应节点角色，简称为 **PD**。
  - 在 PD 与 TiKV 的通讯过程中，PD 完全是被动的一方。TiKV 节点定期主动向 PD 报送**心跳**，分片的元数据信息也就随着心跳一起报送，而 PD 会将分片调度指令放在心跳的返回信息中。等到 TiKV 下次报送心跳时，PD 就能了解到调度的执行情况。
  - **由于每次 TiKV 的心跳中包含了全量的分片元数据，PD 甚至可以不落盘任何分片元数据， 完全做成一个无状态服务。**这样的好处是，PD 宕机后选举出的新主根本不用处理与旧主的 状态衔接，在一个心跳周期后就可以工作了。当然，在具体实现上，PD 仍然会做部分信息 的持久化，这可以认为是一种缓存。
  - **虽然无状态服务有很大的优势，但 PD 仍然是一个单点，也就是说这个方案还是一个中心 化的设计思路，可能存在性能方面的问题**。
- **CockroachDB:去中心化**
  - CockroachDB 的解决方案是使用 Gossip 协议
  - 为什么不用 Paxos 协议呢?
    - 这是因为 Paxos 协议本质上是一种广播机制，也就是由一个中心节点向其他节点发送消 息。当节点数量较多时，通讯成本就很高。
    - **CockroachDB 采用了 P2P 架构，每个节点都要保存完整的元数据**，这样节点规模就非常 大，当然也就不适用广播机制。**而 Gossip 协议的原理是谣言传播机制**，每一次谣言都在几 个人的小范围内传播，但最终会成为众人皆知的谣言。**这种方式达成的数据一致性是 “最终一致性”**，即执行数据更新操作后，经过一定的时间，集群内各个节点所存储的数据最 终会达成一致。
  - **CockroachDB 真的是基于“最终一致性”的元数据实现了强一致性的分布式数据库**。
    - 节点 A 接到客户端的 SQL 请求，要查询数据表 T1 的记录，根据主键范围确定记录可能 在分片 R1 上，而本地元数据显示 R1 存储在节点 B 上。
    - 节点 A 向节点 B 发送请求。很不幸，节点 A 的元数据已经过时，R1 已经重新分配到节 点 C。
    - 此时节点 B 会回复给节点 A 一个非常重要的信息，R1 存储在节点 C。
    - 节点 A 得到该信息后，向节点 C 再次发起查询请求，这次运气很好 R1 确实在节点 C。
    - 节点 A 收到节点 C 返回的 R1。
    - 节点 A 向客户端返回 R1 上的记录，同时会更新本地元数据。
    - **可以看到，CockroachDB 在寻址过程中会不断地更新分片元数据，促成各节点元数据达成 一致。**
  - 看完 TiDB 和 CockroachDB 的设计，我们可以做个小结了。**复制协议的选择和数据副本 数量有很大关系**
    - 如果副本少，参与节点少，可以采用广播方式，也就是 Paxos、Raft 等 协议;
    - 如果副本多，节点多，那就更适合采用 Gossip 协议。
- **Raft** **的性能缺陷**
  - 在**复制效率**上 Raft 会 差一些，主要原因就是 Raft 必须“**顺序投票**”，不允许日志中出现空洞。在我看来，顺序 投票确实是影响 Raft 算法复制效率的一个关键因素。
  - 单个事务的运行情况中一个完整的 Raft 日志复制过程
    - Leader 收到客户端的请求。
    - Leader 将请求内容(即 Log Entry)追加(Append)到本地的 Log。
    - Leader 将 Log Entry 发送给其他的 Follower。
    - Leader 等待 Follower 的结果，如果大多数节点提交了这个 Log，那么这个 Log Entry 就是 Committed Entry，Leader 就可以将它应用(Apply)到本地的状态机。
    - Leader 返回客户端提交成功。
    - Leader 继续处理下一次请求。
  - 多事务并行操作时
    - 我们设定这个 Raft 组由 5 个节点组成，T1 到 T5 是先后发生的 5 个事务操作，被发送到 这个 Raft 组。
    - **事务 T1 的操作是将 X 置为 1，5 个节点都 Append 成功，Leader 节点 Apply 到本地状 态机，并返回客户端提交成功。**事务 T2 执行时，虽然有一个 Follower 没有响应，但仍然 得到了大多数节点的成功响应，所以也返回客户端提交成功。
    - **现在，轮到 T3 事务执行，没有得到超过半数的响应，这时 Leader 必须等待一个明确的失 败信号，比如通讯超时，才能结束这次操作。因为有顺序投票的规则，T3 会阻塞后续事务 的进行。**T4 事务被阻塞是合理的，因为它和 T3 操作的是同一个数据项，但是 T5 要操作 的数据项与 T3 无关，也被阻塞，显然这不是最优的并发控制策略。
    - 同样的情况也会发生在 Follower 节点上，**第一个 Follower 节点可能由于网络原因没有收 到 T2 事务的日志，即使它先收到 T3 的日志，也不会执行 Append 操作**，因为这样会使 日志出现空洞。
  - **Raft 的顺序投票是一种设计上的权衡，虽然性能有些影响，但是节点间日志比对会非常简单。在两个节点上，只要找到一条日志是一致的，那么在这条日志之前的所有日志就都是 一致的。**这使得选举出的 Leader 与 Follower 同步数据非常便捷，开放 Follower 读操作 也更加容易。
- **Raft** **的性能优化方法(TiDB*)**
  - 四个优化点
    - **批操作(Batch)。**Leader 缓存多个客户端请求，然后将这一批日志批量发送给 Follower。Batch 的好处是减少的通讯成本。
    - **流水线(Pipeline)。**Leader 本地增加一个变量(称为 NextIndex)，每次发送一个 Batch 后，更新 NextIndex 记录下一个 Batch 的位置，然后不等待 Follower 返回，马 上发送下一个 Batch。如果网络出现问题，Leader 重新调整 NextIndex，再次发送 Batch。当然，**这个优化策略的前提是网络基本稳定**。
    - **并行追加日志(Append Log Parallelly)。**Leader 将 Batch 发送给 Follower 的同 时，并发执行本地的 Append 操作。因为 Append 是磁盘操作，开销相对较大，而标准流程中 Follower 与 Leader 的 Append 是先后执行的，当然耗时更长。改为并行就 可以减少部分开销。当然，这时 Committed Entry 的判断规则也要调整。在并行操作 下，即使 Leader 没有 Append 成功，只要有半数以上的 Follower 节点 Append 成 功，那就依然可以视为一个 Committed Entry，Entry 可以被 Apply。
    - **异步应用日志(Asynchronous Apply)。**Apply 并不是提交成功的必要条件，任何处 于 Committed 状态的 Log Entry 都确保是不会丢失的。Apply 仅仅是为了保证状态能 够在下次被正确地读取到，但多数情况下，提交的数据不会马上就被读取。因此，Apply 是可以转为异步执行的，同时读操作配合改造。
- etcd，它是最早的、生产级的 Raft 协议开源实现，TiDB 和 CockroachDB 都借鉴了它的设计。甚至可以说，它们选择 Raft 就是因为 etcd 提供了可 靠的工程实现。





## 原子性

- 实现事务原子性的两种协议

  - **面向应用层的** **TCC**
  - **数据库领域最常用的** **2PC**

- 2PC的三大问题

  - 相比于 TCC，2PC 的优点是借助了数据库的提交和回滚操作，不侵入业务逻辑。但是，它 也存在一些明显的问题:
    - **同步阻塞**。执行过程中，数据库要锁定对应的数据行。如果其他事务刚好也要操作这些数据行，那它 们就只能等待。其实同步阻塞只是设计方式，真正的问题在于这种设计会导致分布式事务 出现高延迟和性能的显著下降。
    - **单点故障**。事务管理器非常重要，一旦发生故障，数据库会一直阻塞下去。尤其是在第二阶段发生故障的话，所有数据库还都处于锁定事务资源的状态中，从而无法继续完成事务操作。
    - **数据不一致**。在第二阶段，当事务管理器向参与者发送 Commit 请求之后，发生了局部网络异常，导致 只有部分数据库接收到请求，但是其他数据库未接到请求所以无法提交事务，整个系统就 会出现数据不一致性的现象。**比如，小明的余额已经能够扣减，但是小红的余额没有增 加，这样就不符合原子性的要求了。**
  - **事实上，多数分布式数据库都是在 2PC 协议基础上改进，来保证分布式事务的原子性。**

- **NewSQL** **阵营:Percolator**

  - Percolator 模型同时涉及了隔离性和原子性的处理
  - 使用 Percolator 模型的前提是事务的参与者，即数据库，要**支持多版本并发控制 (MVCC)**。
  - Percolator 的流程
    - **第一，准备阶段**
      - 事务管理器向分片发送 Prepare 请求，包含了具体的数据操作要求。
      - **分片接到请求后要做两件事，写日志和添加私有版本。**关于私有版本，你可以简单理解 为，在 lock 字段上写入了标识信息的记录就是私有版本，只有当前事务能够操作，通常其 他事务不能读写这条记录。
      - 准备阶段结束的时候，两个分片都增加了私有版本记录，余额正好是转账顺利执行后的数 字。
      - **主锁的选择是随机的**，参与事务的记录都可能拥有主锁，但一个事务只能有一条记录拥有 主锁，其他参与事务的记录在 lock 字段记录了指针信息“primary@Ming.bal”，指向主 锁记录。
    - **第二，提交阶段**
      - 事务管理器只需要和拥有主锁的分片通讯，发送 Commit 指令，且不用 附带其他信息。
      - 分片 P1 增加了一条新记录时间戳为 8，指向时间戳为 7 的记录，后者在准备阶段写入的主锁也被抹去。这时候 7、8 两条记录不再是私有版本，所有事务都可以看到小明的余额变 为 2,700 元，事务结束。
      - **为什么在提交阶段不用更新小红的记录**
        - **Percolator 最有趣的设计就是这里，因为分片 P2 的最后一条记录，保存了指向主锁的指 针。**其他事务读取到 Hong7 这条记录时，会根据指针去查找 Ming.bal，发现记录已经提交，所以小红的记录虽然是私有版本格式，但仍然可视为已经生效了。
        - 当然，这种通过指针查找的方式，会给读操作增加额外的工作。如果每个事务都照做，性 能损耗就太大了。**所以，还会有其他异步线程来更新小红的余额记录**。
  - 对比 2PC 的问题，来看看 Percolator 模型有哪些改进
    - **数据不一致**
      - 2PC 的一致性问题主要缘自第二阶段，不能确保事务管理器与多个参与者的通讯始终正 常。
      - **但在 Percolator 的第二阶段，事务管理器只需要与一个分片通讯，这个 Commit 操作本身就是原子的**。所以，事务的状态自然也是原子的，一致性问题被完美解决了。
    - **单点故障**
      - Percolator 通过**日志和异步线程**的方式弱化了这个问题。
      - **一是，Percolator 引入的异步线程可以在事务管理器宕机后，回滚各个分片上的事务，提供了善后手段**，不会让分片上被占用的资源无法释放。
      - **二是，事务管理器可以用记录日志的方式使自身无状态化，日志通过共识算法同时保存在 系统的多个节点上。**这样，事务管理器宕机后，可以在其他节点启动新的事务管理器，基 于日志恢复事务操作。
    - **Percolator 模型在分布式数据库的工程实践中被广泛借鉴。比如，分布式数据库 TiDB**，完 全按照该模型实现了事务处理;CockroachDB 也从 Percolator 模型获得灵感，设计了自 己的 2PC 协议。
    - **CockroachDB** 的变化在于没有随机选择主锁，而是引入了一张**全局事务表**，所有分片记录 的指针指向了这个事务表中对应的事务记录。

- **GoldenDB** **的一阶段提交**

  - **GoldenDB 遵循 PGXC 架构，包含了四种角色:协调节点、数据节点、全局事务器和管理节点**，其中协调节点和数据节点均有多个。GoldenDB 的数据节点由 MySQL 担任，后者 是独立的单体数据库。
  - 虽然名字叫“一阶段提交”，但 GoldenDB 的流程依然可以分为两个阶段。
    - **第一阶段，GoldenDB 的协调节点接到事务后，在全局事务管理器(GTM)的全局事务列表中将事务标记成活跃的状态。**
      - 这个标记过程是 GoldenDB 的主要改进点，实质是通过全局事务列表来申请资源，规避可能存在的事务竞争。
      - 这样的好处是避免了与所有参与者的通讯，也减少了很多无效的资源锁定动作。
    - **第二阶段，协调节点把一个全局事务分拆成若干子事务，分配给对应的 MySQL 去执行。** 如果所有操作成功，协调者节点会将全局事务列表中的事务标记为结束，整个事务处理完 成。如果失败，子事务在单机上自动回滚，而后反馈给协调者节点，后者向所有数据节点 下发回滚指令。
  - GoldenDB 的“一阶段提交”，本质上是改变了资源的申请方式，更准确的说法是，**并发控制手段从锁调度变为时间戳排序(Timestamp Ordering)**。**这样，在正常情况下协调节点与数据节点只通讯一次**，降低了网络不确定性的影响，数据库的整体性能有明显提 升。**因为第一阶段不涉及数据节点的操作，也就弱化了数据一致性和单点故障的问题。**

- **事务延迟估算**

  - 整个 2PC 的事务延迟由两个阶段组成，可以用公式表达为:

    - $$
      L_{txn}=L_{prep}+L_{commit}
      $$

    - 其中，*L_prep* 是准备阶段的延迟，*L_commit* 是提交阶段的延迟。

  - 准备阶段，它是事务操作的主体，包含若干读操作和若干写操作。

    - 我们选一种最乐观的情况， CockroachDB。
      - 因为它采用 P2P 架构，每个节点既承担了客户端服务接入的工作，也有 请求处理和数据存储的职能。所以，最理想的情况是，读操作的客户端接入节点，同时是 当前事务所访问数据的 Leader 节点，那么所有读取就都是本地操作。
      - 磁盘操作相对网络延迟来说是极短的，所以我们可以忽略掉读取时间。那么，准备阶段的 延迟主要由写入操作决定
    - 我们都知道，分布式数据库的写入，并不是简单的本地操作，而是使用共识算法同时在多个节点上写入数据。所以，一次写入操作延迟等于一轮共识算法开销

  - 第二阶段，提交阶段。

    - Percolator 模型，它的提交阶段只需要写入一次数据，修改整个事务的状态。**对于 CockroachDB，这个事务标识可以保存在本地。那么提交操作的延迟也是一轮共识算法**

  - 小明给小红转账，金额是 500 元

    - 在这个转账事务中，包含两条写操作 SQL，分别是扣减小明账户余额和增加小红账户余 额，W 等于 2。再加上提交操作，一共有 3 个 *L_c*(用 *L_c* 代表一 轮共识算法的用时)。我们可以看到，这个公式里事务的延 迟是与写操作 SQL 的数量线性相关的，而真实场景中通常都会包含多个写操作，那事务延 迟肯定不能让人满意。

- 优化方法

  - **缓存写提交**
    - 第一个办法是**将所有写操作缓存起来，直到 commit 语句时一起执行**，这种方式称为 Buffering Writes until Commit，我把它翻译为**“缓存写提交”**。而 TiDB 的事务处理中 就采用这种方式。
      - 所有从 Client 端提交的 SQL 首先会缓存在 TiDB 节点，只有当客户端发起 Commit 时， TiDB 节点才会启动两阶段提交，将 SQL 被转换为 TiKV 的操作。这样，显然可以压缩第一阶段的延迟，把多个写操作 SQL 压缩到大约一轮共识算法的时间。
    - 但缓存写提交存在两个明显的缺点。
      - 首先是在客户端发送 Commit 前，SQL 要被缓存起来，**如果某个业务场景同时存在长事务和海量并发的特点，那么这个缓存就可能被撑爆或者成为瓶颈。**
      - 其次是客户端看到的 **SQL 交互过程发生了变化**，在 MySQL 中如果出现事务竞争，判断优 先级的规则是 First Write Win，也就是对同一条记录先执行写操作的事务获胜。**而 TiDB 因为缓存了所有写 SQL，所以就变成了 First Commit Win，也就是先提交的事务获胜**。
  - **管道**
    - 既能缩短延迟，又能保持交互事务
    - 这就是 CockroachDB 采用的方式，称为 Pipeline。**具体过程就是在准备阶段是按照顺序将 SQL 转换为 K/V 操作并执行，但是并不等待返回结果，直接执行下一个 K/V 操作。**
      - 这样，准备阶段的延迟，等于最慢的一个写操作延迟，也就是一轮共识算法的开销
      - 同样的操作，按照 Pipeline 方式，增加小红账户余额时并不等待小明扣减账户的动作结 束，两条 SQL 的执行时间约等于 1 个 *L_c*。加上提交阶段的 1 个 *L_c*，一共是 2 个 *L_c*，并 且延迟也不再随着 SQL 数量增加而延长。
  - **并行提交**
    - 但是，像 CockroachDB、YugabyteDB 这样分布式数据库，它们的目标就是全球化部署，所以还要努力去压缩事务延迟。
      - 可是，还能怎么压缩呢?**准备阶段的操作已经压缩到极限了，commit 这个动作也不能少 呀，那就只有一个办法，让这两个动作并行执行。**
    - 并行执行的过程是这样的。
      - **准备阶段的操作，在 CockroachDB 中被称为意向写。这个并行执行就是在执行意向写的同时，就写入事务标志**，当然这个时候不能确定事务是否提交成功的，所以要引入一个新 的状态“Staging”，表示事务正在进行。那么这个记录事务状态的落盘操作和意向写大致 是同步发生的，所以只有一轮共识算法开销。
        - **Writes 部分是意向写的 Key。这是留给异步进程的线索，通过这些 Key 是否写成功，可以 倒推出事务是否提交成功。**
      - 而**客户端得到所有意向写的成功反馈后，可以直接返回调用方事务提交成功**。**注意!这个 地方就是关键了**，**客户端只在当前进程内判断事务提交成功后，不维护事务状态，而直接返回调用方;事后由异步线程根据事务表中的线索，再次确认事务的状态，并落盘维护状态记录。**这样事务操作中就减少了一轮共识算法开销。
    - 你有没有发现，并行提交的优化思路其实和 Percolator 很相似，那就是**不要纠结于在一次 事务中搞定所有事情，可以只做最少的工作，留下必要的线索，就可以达到极致的速度。** 而后续的异步进程，只要根据线索，完成收尾工作就可以了。
    - 并行提交不是BASE，仍然强一致的。因为，**负责发起Pipeline写入的线程是明确知道 这些写入都成功了**，注意，这个成功是说事务涉及的每个Raft组都写入成功了，那么此时线程可以判定事务已经提交成功了。但是，如果接下来它写盘记录事务的最新状态，就会带来新一轮共识算法开销，而不写盘也不会影响事务状态，所以它直接返回客户端，使得延迟更短。
    - 异步化的只是事务状态的落盘操作，而事务状态的确认仍然是同步完成的。







## 隔离性

- 多版本并发控制(Multi-Version Concurrency Control，MVCC)就是**通过记录数据项历史版本的方式，来提升系统应对多事务访问的并发处理能力**。

  - 事务 T1、T2 先后启动，分别对数据库执行写操作和读操作。
    - 果先执行的是 T1 写事务，除了磁盘写入数据的时间， 由**于要保证数据库的高可靠，至少还有一个备库同步复制主库的变更内容。**这样，阻塞时 间就要再加上一次网络通讯的开销。
    - 如果先执行的是 T2 只读事务，情况也好不到哪去，虽然不用考虑复制问题，但是读操作通 常会涉及更大范围的数据，这样一来加锁的记录会更多，被阻塞的写操作也就更多。而 且，**只读事务往往要执行更加复杂的计算，阻塞的时间也就更长。**
    - 所以说，用锁解决读写冲突问题，带来的事务阻塞开销还是不小的。相比之下，用 MVCC 来解决读写冲突，就不存在阻塞问题，要优雅得多了。
  - **PGXC 的 MVCC 实现方式其实就 是单体数据库的实现方式。**

- 单体数据库的MVCC

  - **MVCC 有三类存储方式，一类是将历史版本直接存在数据表中的，称为 Appane-Only**， 典型代表是 PostgreSQL。另外两类都是在独立的表空间存储历史版本，它们区别在于存 储的方式是全量还是增量。**增量存储就是只存储与版本间变更的部分，这种方式称为 Delta**，典型代表是 MySQL 和 Oracle。**全量 存储则是将每个版本的数据全部存储下来，这种方式称为 Time-Travle**，典型代表是 HANA。

  - 三种存储方式的优缺点

    - | 存储方式 | Appane-Only**                                  | Delta                        | Time-Travle    |
      | -------- | ---------------------------------------------- | ---------------------------- | -------------- |
      | 优点     | 可快速回滚、磁盘操作开销小、不会出现回滚段耗尽 | 当前版本查询快、存储要求低   | 当前版本查询快 |
      | 缺点     | 当前版本查询慢，存储要求高                     | 计算开销大、有回滚段耗尽问题 | 存储要求高     |

- **PGXC** **读写冲突处理**

  - PGXC 在实现 RR 时遇到的两个挑战，也就是实现快照的两个挑战
    - **一是如何保证产生单调递增事务 ID。**每个数据节点自行处理显然不行，这就需要由一个集 中点来统一生成。
    - **二是如何提供全局快照。**每个事务要把自己的状态发送给一个集中点，由它维护一个全局 事务列表，并向所有事务提供快照。
  - **所以，PGXC 风格的分布式数据库都有这样一个集中点，通常称为全局事务管理器 (GTM)。**又因为事务 ID 是单调递增的，用来衡量事务发生的先后顺序，和时间戳作用相近，所以全局事务管理器也被称为“全局时钟”。

- **NewSQL** **读写冲突处理**

  - TiDB
    - TiDB 底层是分布式键值系统，我们假设两个事务操作同一个数据项。其中，事务 T1 执行 写操作，由 Prewrite 和 Commit 两个阶段构成，对应了我们之前介绍的两阶段提交协议 (2PC)。**这里你也可以简单理解为 T1的写操作分成了两个阶段，T2 在这两个阶段之间试图执行读操作，但是 T2 会被阻塞，直 到 T1 完成后，T2 才能继续执行。**
    - **你肯定会非常惊讶，这不是 MVCC 出现前的读写阻塞吗?**
      - **TiDB 为什么没有使用快照读取历史版本呢? TiDB 官方文档并没有说明背后的思路，我猜 问题出在全局事务列表上，**因为 TiDB 根本没有设计全局事务列表。当然这应该不是设计上 的疏忽，我更愿意把它理解为一种权衡，是在读写效率和全局事务列表的维护代价之间的 选择。
    - **事实上，PGXC 中的全局事务管理器就是一个单点，很容易成为性能的瓶颈，而分布式系 统一个普遍的设计思想就是要避免对单点的依赖**。当然，TiDB 的这个设计付出的代价也是 巨大的。虽然，TiDB 在 3.0 版本后增加了悲观锁，设计稍有变化，但大体仍是这样。
  - CockroachDB
    - **依旧是 T1 事务先执行写操作，中途 T2 事务启动，执行读操作，此时 T2 会被优先执行。 待 T2 完成后，T1 事务被重启。**重启的意思是 T1 获得一个新的时间戳(等同于事务 ID) 并重新执行。
    - CockroachDB 没有使用快照，不是因为没有全局事务列表，而是因为**它的隔离级别目标不 是 RR，而是 SSI，也就是可串行化。**
    - **总之，CockroachDB 对于读写 冲突、写写冲突采用了几乎同样的处理方式。**
    - 事实上，被重启的事务 并不一定是执行写操作的事务。CockroachDB 的每个事务都有一个优先级，出现事务冲突 时会比较两个事务的**优先级**，高优先级的事务继续执行，低优先级的事务则被重启。而被 重启事务的优先级也会提升，避免总是在竞争中失败，最终被“饿死”。

- **不确定时间窗口**

  - 共有 7 个数据库事务，T1 到 T7，其中 T6 是读事 务，其他都是写事务。**事务 T2 结束的时间点(记为 T2-C)早于事务 T6 启动的时间点 (记为 T6-S)，这是基于数据记录上的时间戳得出的判断，但实际上这个判断很可能是错 的。**
    - **这是因为时间误差的存在，T2-C 时间点附近会形成一个不确定时间窗 口，也称为置信区间或可信区间。**严格来说，我们只能确定 T2-C 在这个时间窗口内，但 无法更准确地判断具体时间点。同样，T6-S 也只是一个时间窗口。时间误差不能消除，但 可以通过工程方式控制在一定范围内，例如在 Spanner 中这个不确定时间窗口(记为ɛ) 最大不超过 7 毫秒，平均是 4 毫秒。
  - 在这个案例中，当我们还原两个时间窗口后，发现两者存在重叠，所以无法判断 T2-C 与 T6-S 的先后关系，怎么解决呢?
    - 只有避免时间窗口出现重叠。 
    - 答案是等待。“waiting out the uncertainty”，用等待来消除不确定性。
    - **在实践中，我们看到有两种方式可供选择，分别是写等待和读等待。**

- **写等待:**Spanner

  - Spanner 选择了写等待方式，更准确地说是用**提交等待(commit-wait)**来消除不确定性。
  - **Spanner 是直接将时间误差暴露出来的**，所以调用当前时间函数 TT.now() 时，会获得的 是一个区间对象 TTinterval。它的两个边界值 earliest 和 latest 分别代表了最早可能时间 和最晚可能时间，而绝对时间就在这两者之间。另外，Spanner 还提供了 TT.before() 和 TT.after() 作为辅助函数，其中 TT.after() 用于判断当前时间是否晚于指定时间。
  - **理论等待时间**
    - **事务 Ta 在获得“提交时间戳”S 后，再等待ɛ时间后才写盘 并提交事务。真正的提交时间是晚于“提交时间戳”的，中间这段时间就是等待。**这样 Tb 事务启动后，能够得到的最早时间 TT2.earliet 肯定不会早于 S 时刻，所以 Tb 就一定能够 读取到 Ta。这样就符合线性一致性的要求了。
    - 综上，事务获得“提交时间戳”后必须等待ɛ时间才能写入磁盘，即 commit-wait。
  - 有什么不对劲的地方吗
    - 对，就是那个绝对时间 S。都说了半天时间有误差，那又怎么可能拿到一个绝对时间呢? 这不是自相矛盾吗?
    - Spanner 确实拿不到绝对时间
  - **实际等待时间**
    - **Spanner 将含有写操作的事务定义为读写事务。**读写事务的写操作会以两阶段提交 (2PC)的方式执行。
      - 2PC 的第一阶段是预备阶段，每个参与者都会获取一个**“预备时间戳”**，与数据一起写入 日志。第二阶段，协调节点写入日志时需要一个**“提交时间戳”**，而它必须要大于任何参 与者的“预备时间戳”。
      - 所以，协调节点调用 TT.now() 函数后，要取该时间区间的lastest 值(记为 s)，而且 s 必须大于所有参与者的“预备时间戳”，作为“提交时间 戳”。
    - 针对同一个数据项，事务 T8 和 T9 分别对进行写入和读取操作。T8 在绝对时间 100ms 的 时候，调用 TT.now() 函数，得到一个时间区间[99,103]，选择最大值 103 作为提交时间 戳，而后等待 8 毫秒(即 2ɛ)后提交。
      - **这样，无论如何 T9 事务启动时间都晚于 T8 的“提交时间戳”，也就能读取到 T8 的更 新。**
    - **回顾一下这个过程，第一个时间差是 2PC 带来的**，如果换成其他事务模型也许可以避免， **而第二个时间差是真正的 commit-wait，来自时间的不确定性，是不能避免的**。
    - TrueTime 的平均误差是 4 毫秒，commit-wait 需要等待两个周期，那 Spanner 读写事 务的平均延迟必然大于等于 8 毫秒。为啥有人会说 Spanner 的 TPS 是 125 呢?原因就是 这个了。其实，这只是事务操作数据出现重叠时的吞吐量，而无关的读写事务是可以并行 处理的。

- **读等待:**CockroachDB

  - 因为 CockroachDB 采用混合逻辑时钟(HLC)，所以对于没有直接关联的事务，只能用 物理时钟比较先后关系。CockroachDB 各节点的物理时钟使用 NTP 机制同步，误差在几 十至几百毫秒之间
  - **写等待模式下，所有包含写操作的事务都受到影响，要延后提交;而读等待只在特殊条件 下才被触发，影响的范围要小得多。**
  - 事务 T6 启动获得了一个时间戳 T6-S1，此时虽然事务 T2 已经在 T2-C 提交，但 **T2-C 与 T6-S1 的间隔小于集群的时间偏移量，所以无法判断 T6 的提交是否真的早于 T2。**
    - 这时，CockroachDB 的办法是**重启(Restart)读操作的事务，就是让 T6 获得一个更晚 的时间戳 T6-S2，使得 T6-S2 与 T2-C 的间隔大于 offset，那么就能读取 T2 的写入了。**
    - **不过，接下来又出现更复杂的情况， T6-S2 与 T3 的提交时间戳 T3-C 间隔太近，又落入 了 T3 的不确定时间窗口，所以 T6 事务还需要再次重启。**而 T3 之后，T6 还要重启越过 T4 的不确定时间窗口。
    - **最后，当 T6 拿到时间戳 T6-S4 后，终于跳过了所有不确定时间窗口，读等待过程到此结 束**，T6 可以正式开始它的工作了。
  - 在这个过程中，可以看到**读等待的两个特点:一是偶发**，只有当读操作与已提交事务间隔 小于设置的时间误差时才会发生;**二是等待时间的更长**，因为事务在重启后可能落入下一 个不确定时间窗口，所以也许需要经过多次重启。

- **并发控制技术的分类**

  - 要想让系统支持海量并发，很重要的基础就是数据库的并发处理能力，而这里面最重要的就是对写写冲突的并发控制。
  - 在这个技术体系中，虽然有多种不同的划分方式，**但最为大家熟知就是悲观协议和乐观协议两大类。**
  - 落到实现机制上，有一个广泛被提到的定义版本。**乐观协议就是直接提交**，遇 到冲突就回滚;**悲观协议**就是在真正提交事务前，**先尝试对需要修改的资源上锁**，只有在 确保事务一定能够执行成功后，才开始提交。
  - 总之，这个版本的核心就是，悲观协议是使用锁的，而乐观协议是不使用锁的

- **乐观锁:TiDB**

  - TiDB 的乐观锁基本上就是 Percolator 模型，它的运行过程可以分为三个阶段。
    - **选择 Primary Row**
      - 收集所有参与修改的行，从中随机选择一行，作为这个事务的 Primary Row，这一行是拥有锁的，称为 Primary Lock，而且**这个锁会负责标记整个事务的完成状态。**所有其他修改 行也有锁，称为 Secondary Lock，都会保留指向 Primary Row 的指针。
    - **写入阶段**
      - **按照两阶段提交的顺序，执行第一阶段。**每个修改行都会执行上锁并执行“prewrite”， prewrite 就是将数据写入私有版本，其他事务不可见。注意这时候，每个修改行都可能碰 到锁冲突的情况，**如果冲突了，就终止事务，返回给 TiDB，那么整个事务也就终止了。如 果所有修改行都顺利上锁，完成 prewrite，第一阶段结束。**
    - **提交阶段**
      - **这是两阶段提交的第二阶段**，提交 Primary Row，也就是写入新版本的提交记录并清除 Primary Lock，如果顺利完成，那么这个事务整体也就完成了，反之就是失败。**而 Secondary Rows 上的锁，则会交给异步线程根据 Primary Lock 的状态去清理。**

- 你看这个过程中不仅有锁，而且锁的数量还不少。那么，为什么又说它是乐观协议呢?

  - **并发控制的三个阶段**
    - **乐观协议 和悲观协议的操作，都统一成四个阶段，分别是有效性验证(V)、读(R)、计算(C)和写(W)。两者的区别就是这四个阶段的顺序不同**:悲观协议的操作顺序是 VRCW，而 乐观协议的操作顺序则是 RCVW。因为在比较两种协议时，计算(C)这个阶段没有实质 影响，可以忽略掉。那么简化后，悲观协议的顺序是 VRW，而乐观协议的顺序就是 RVW。
  - RVW 的三阶段划分
    - **读阶段(Read Pharse)**，每个事务对数据项的局部拷贝进行更新。
    - **有效性确认阶段(Validation Pharse)**，验证准备提交的事务。
    - **写阶段(Write Pharse)**，将读阶段的更新结果写入到数据库中，接受事务的提交结果。
  - 还有一种关于乐观与悲观的表述，也与三阶段的顺序相呼应。乐观，重在事后检测，在事 务提交时检查是否满足隔离级别，如果满足则提交，否则回滚并自动重新执行。悲观，重在事前预防，在事务执行时检查是否满足隔离级别，如果满足则继续执行，否则等待或回 滚。
  - 我们再回到 TiDB 的乐观锁。**虽然对于每一个修改行来说，TiDB 都做了有效性验证，而且 顺序是 VRW，可以说是悲观的，但这只是局部的有效性验证**;从整体看，**TiDB 没有做全 局有效性验证，不符合 VRW 顺序，所以还是相对乐观的。**
    - 局部性是说，只要局部加锁成功，就开始局部的写入了。TiDB的乐观锁也不是在所有 记录加锁成功以后，才写入新的数据版本。如果是，那还会因为事务冲突多而频繁回滚吗?再想 想:)
    - 怎么加锁才是全局有效性验证?TiDB后来增加的悲观锁就是例子。这个加锁动作是统一的，只要 有一条记录加锁不成功，就不会启动任何写入动作。

  




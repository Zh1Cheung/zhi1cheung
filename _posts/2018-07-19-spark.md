---
title: Spark SQL、Spark Streaming实战开发进阶
categories:
- Spark
- Big Data
tags:
- Spark



---

## Spark SQL实战开发进阶-Hive 0.13安装与测试


Spark SQL的Thrift JDBC/ODBC server是基于Hive 0.13的HiveServer2实现的。这个服务启动之后，最主要的功能就是可以让我们通过

Java JDBC来以编程的方式调用Spark SQL。此外，在启动该服务之后，可以通过Spark或Hive 0.13自带的beeline工具来进行测试。

要启动JDBC/ODBC server，主要执行Spark的sbin目录下的start-thriftserver.sh命令即可

start-thriftserver.sh命令可以接收所有spark-submit命令可以接收的参数，额外增加的一个参数是--hiveconf，可以用于指定一些

Hive的配置属性。可以通过执行./sbin/start-thriftserver.sh --help来查看所有可用参数的列表。默认情况下，启动的服务会在

localhost:10000地址上监听请求。

可以使用两种方式来改变服务监听的地址

第一种：指定环境变量

    export HIVE_SERVER2_THRIFT_PORT=<listening-port>
    export HIVE_SERVER2_THRIFT_BIND_HOST=<listening-host>
    ./sbin/start-thriftserver.sh \
      --master <master-uri> \
      ...
  
第二种：使用命令的参数

    ./sbin/start-thriftserver.sh \
      --hiveconf hive.server2.thrift.port=<listening-port> \
      --hiveconf hive.server2.thrift.bind.host=<listening-host> \
      --master <master-uri>
      ...
      
    hdfs dfs -chmod 777 /tmp/hive-root
    
    ./sbin/start-thriftserver.sh \
    --jars /usr/local/hive/lib/mysql-connector-java-5.1.17.jar
      
这两种方式的区别就在于，第一种是针对整个机器上每次启动服务都生效的; 第二种仅仅针对本次启动生效

接着就可以通过Spark或Hive的beeline工具来测试Thrift JDBC/ODBC server

在Spark的bin目录中，执行beeline命令（当然，我们也可以使用Hive自带的beeline工具）：

    ./bin/beeline
    进入beeline命令行之后，连接到JDBC/ODBC server上去：beeline> !connect jdbc:hive2://localhost:10000

beeline通常会要求你输入一个用户名和密码。在非安全模式下，我们只要输入本机的用户名（比如root），以及一个空的密码即可。

对于安全模式，需要根据beeline的文档来进行认证。

除此之外，大家要注意的是，如果我们想要直接通过JDBC/ODBC服务访问Spark SQL，并直接对Hive执行SQL语句，那么就需要将Hive

的hive-site.xml配置文件放在Spark的conf目录下。

Thrift JDBC/ODBC server也支持通过HTTP传输协议发送thrift RPC消息。使用以下方式的配置可以启动HTTP模式：

命令参数

    ./sbin/start-thriftserver.sh \
      --hive.server2.transport.mode=http \
      --hive.server2.thrift.http.port=10001 \
      --hive.server2.http.endpoint=cliservice \
      --master <master-uri>
      ...
      
    ./sbin/start-thriftserver.sh \
      --jars /usr/local/hive/lib/mysql-connector-java-5.1.17.jar \
      --hiveconf hive.server2.transport.mode=http \
      --hiveconf hive.server2.thrift.http.port=10001 \
      --hiveconf hive.server2.http.endpoint=cliservice 
      
beeline连接服务时指定参数

    beeline> !connect jdbc:hive2://localhost:10001/default?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice

最重要的，当然是通过Java JDBC的方式，来访问Thrift JDBC/ODBC server，调用Spark SQL，并直接查询Hive中的数据


    import java.sql.Connection;
    import java.sql.DriverManager;
    import java.sql.PreparedStatement;
    import java.sql.ResultSet;
    
    public class ThriftJDBCServerTest {
    	
    	public static void main(String[] args) {
    		String sql = "select name from users where id=?";
    		
    		Connection conn = null;
    		PreparedStatement pstmt = null;
    		ResultSet rs = null;
    		
    		try {
    			Class.forName("org.apache.hive.jdbc.HiveDriver");  
    			
    			conn = DriverManager.getConnection("jdbc:hive2://192.168.0.103:10001/default?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice", 
    					"root", 
    					"");
    			
    			pstmt = conn.prepareStatement(sql);
    			pstmt.setInt(1, 1);  
    			
    			rs = pstmt.executeQuery();
    			while(rs.next()) {
    				String name = rs.getString(1);
    				System.out.println(name);  
    			}
    		} catch (Exception e) {
    			e.printStackTrace(); 
    		}
    	}
    	
    }

## Spark SQL实战开发进阶-CLI命令行使用


Spark SQL CLI是一个很方便的工具，可以用来在本地模式下运行Hive的元数据服务，并且通过命令行执行针对Hive的SQL查询。但是

我们要注意的是，Spark SQL CLI是不能与Thrift JDBC server进行通信的。

如果要启动Spark SQL CLI，只要执行Spark的bin目录下的spark-sql命令即可

./bin/spark-sql --jars /usr/local/hive/lib/mysql-connector-java-5.1.17.jar

这里同样要注意的是，必须将我们的hive-site.xml文件放在Spark的conf目录下。

我们也可以通过执行./bin/spark-sql --help命令，来获取该命令的所有帮助选项。



## Spark SQL实战开发进阶-综合案例2：新闻网站关键指标离线统计

案例背景

    新闻网站
    1、版块
    2、新闻页面
    3、新用户注册
    4、用户跳出

案例需求分析

每天每个页面的PV：PV是Page View，是指一个页面被所有用户访问次数的总和，页面被访问一次就被记录1次PV

每天每个页面的UV：UV是User View，是指一个页面被多少个用户访问了，一个用户访问一次是1次UV，一个用户访问多次还是1次UV

新用户注册比率：当天注册用户数 / 当天未注册用户数

用户跳出率：IP只浏览了一个页面就离开网站的次数/网站总访问数（PV）

版块热度排行榜：根据每个版块每天被访问的次数，做出一个排行榜

网站日志格式

    date timestamp userid pageid section action 

日志字段说明

    date: 日期，yyyy-MM-dd格式
    timestamp: 时间戳
    userid: 用户id
    pageid: 页面id
    section: 新闻版块
    action: 用户行为，两类，点击页面和注册

模拟数据生成程序
模式数据演示

在hive中创建访问日志表

    create table news_access (
      date string,
      timestamp bigint,
      userid bigint,
      pageid bigint,
      section string,
      action string) 

将模拟数据导入hive表中

    load data local inpath '/usr/local/test/news_access.log' into table news_access;



代码：

    OfflineDataGenerator.java
    NewsOfflineStatSpark.java




## Spark Streaming实战开发进阶-接收flume实时数据流-flume风格的基于push的方式


Flume被设计为可以在agent之间推送数据，而不一定是从agent将数据传输到sink中。在这种方式下，Spark Streaming需要启动一个作为Avro Agent的Receiver，来让
flume可以推送数据过来。下面是我们的整合步骤：

### 前提需要

选择一台机器：

1、Spark Streaming与Flume都可以在这台机器上启动，Spark的其中一个Worker必须运行在这台机器上面

2、Flume可以将数据推送到这台机器上的某个端口

由于flume的push模型，Spark Streaming必须先启动起来，Receiver要被调度起来并且监听本地某个端口，来让flume推送数据。

### 配置flume

在flume-conf.properties文件中，配置flume的sink是将数据推送到其他的agent中

    agent1.sinks.sink1.type = avro
    agent1.sinks.sink1.channel = channel1
    agent1.sinks.sink1.hostname = 192.168.0.103
    agent1.sinks.sink1.port = 8888

### 配置spark streaming

在我们的spark工程的pom.xml中加入spark streaming整合flume的依赖

    groupId = org.apache.spark
    artifactId = spark-streaming-flume_2.10
    version = 1.5.0

在代码中使用整合flume的方式创建输入DStream

    import org.apache.spark.streaming.flume.*;
    
    JavaReceiverInputDStream<SparkFlumeEvent> flumeStream =
    	FlumeUtils.createStream(streamingContext, [chosen machine's hostname], [chosen port]);
	
这里有一点需要注意的是，这里监听的hostname，必须与cluster manager（比如Standalone Master、YARN ResourceManager）是同一台机器，这样cluster manager
才能匹配到正确的机器，并将receiver调度在正确的机器上运行。

### 部署spark streaming应用

打包工程为一个jar包，使用spark-submit来提交作业

启动flume agent

    flume-ng agent -n agent1 -c conf -f /usr/local/flume/conf/flume-conf.properties -Dflume.root.logger=DEBUG,console

什么时候我们应该用Spark Streaming整合Kafka去用，做实时计算？

什么使用应该整合flume？

看你的实时数据流的产出频率

1、如果你的实时数据流产出特别频繁，比如说一秒钟10w条，那就必须是kafka，分布式的消息缓存中间件，可以承受超高并发

2、如果你的实时数据流产出频率不固定，比如有的时候是1秒10w，有的时候是1个小时才10w，可以选择将数据用nginx日志来表示，每隔一段时间将日志文件
放到flume监控的目录中，然后呢，spark streaming来计算


代码：

    FlumePushWordCount.java


## Spark Streaming实战开发进阶-接收flume实时数据流-自定义sink的基于poll的方式


除了让flume将数据推送到spark streaming，还有一种方式，可以运行一个自定义的flume sink

1、Flume推送数据到sink中，然后数据缓存在sink中

2、spark streaming用一个可靠的flume receiver以及事务机制从sink中拉取数据

### 前提条件

1、选择一台可以在flume agent中运行自定义sink的机器

2、将flume的数据管道流配置为将数据传送到那个sink中

3、spark streaming所在的机器可以从那个sink中拉取数据

### 配置flume

1、加入sink jars，将以下jar加入flume的classpath中

    groupId = org.apache.spark
    artifactId = spark-streaming-flume-sink_2.10
    version = 1.5.1
    
    groupId = org.scala-lang
    artifactId = scala-library
    version = 2.10.4
    
    groupId = org.apache.commons
    artifactId = commons-lang3
    version = 3.3.2

2、修改配置文件
    
    agent.sinks.sink1.type = org.apache.spark.streaming.flume.sink.SparkSink
    agent.sinks.sink1.hostname = 192.168.0.103
    agent.sinks.sink1.port = 8888
    agent.sinks.sink1.channel = channel1

### 配置spark streaming

    import org.apache.spark.streaming.flume.*;
    
    JavaReceiverInputDStream<SparkFlumeEvent>flumeStream =
    	FlumeUtils.createPollingStream(streamingContext, [sink machine hostname], [sink port]);
    	
一定要先启动flume，再启动spark streaming

    flume-ng agent -n agent1 -c conf -f /usr/local/flume/conf/flume-conf.properties -Dflume.root.logger=DEBUG,console



代码：

    FlumePollWordCount.java
    

## Spark Streaming实战开发进阶-高阶技术之自定义Receiver


spark streaming可以从任何数据源来接收数据，哪怕是除了它内置支持的数据源以外的其他数据源（比如flume、kafka、socket等）。

如果我们想要从spark streaming没有内置支持的数据源中接收实时数据，那么我们需要自己实现一个receiver。

实现一个自定义的receiver

一个自定义的receiver必须实现以下两个方法：onStart()、onStop()。onStart()和onStop()方法必须不能阻塞数据，一般来说，

onStart()方法会启动负责接收数据的线程，onStop()方法会确保之前启动的线程都已经停止了。负责接收数据的线程可以调用

isStopped()方法来检查它们是否应该停止接收数据。

一旦数据被接收了，就可以调用store(data)方法，数据就可以被存储在Spark内部。有一系列的store()重载方法供我们调用，来将数据

每次一条进行存储，或是每次存储一个集合或序列化的数据。

接收线程中的任何异常都应该被捕获以及正确处理，从而避免receiver的静默失败。restart()方法会通过异步地调用onStop()和

onStart()方法来重启receiver。stop()方法会调用onStop()方法来停止receiver。reportError()方法会汇报一个错误消息给driver

，但是不停止或重启receiver。

    public class JavaCustomReceiver extends Receiver<String> {
    
      String host = null;
      int port = -1;
    
      public JavaCustomReceiver(String host_ , int port_) {
        super(StorageLevel.MEMORY_AND_DISK_2());
        host = host_;
        port = port_;
      }
    
      public void onStart() {
        // Start the thread that receives data over a connection
        new Thread()  {
          @Override public void run() {
            receive();
          }
        }.start();
      }
    
      public void onStop() {
    
      }
    
      /** Create a socket connection and receive data until receiver is stopped */
      private void receive() {
        Socket socket = null;
        String userInput = null;
    
        try {
          // connect to the server
          socket = new Socket(host, port);
    
          BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream()));
    
          // Until stopped or connection broken continue reading
          while (!isStopped() && (userInput = reader.readLine()) != null) {
            System.out.println("Received data '" + userInput + "'");
            store(userInput);
          }
          reader.close();
          socket.close();
    
          // Restart in an attempt to connect again when server is active again
          restart("Trying to connect again");
        } catch(ConnectException ce) {
          // restart if could not connect to server
          restart("Could not connect", ce);
        } catch(Throwable t) {
          // restart if there is any other error
          restart("Error receiving data", t);
        }
      }
    }

在spark streaming中使用自定义的receiver

    JavaDStream<String> lines = ssc.receiverStream(new JavaCustomReceiver(host, port));

安装网络服务工具

    rpm -ihv netcat-0.7.1-1.i386.rpm
    
    nc -lk 9999



## Spark Streaming实战开发进阶-综合案例3：新闻网站关键指标实时统计

    kafka安装略


    kafka-topics.sh --zookeeper 192.168.0.103:2181,192.168.0.104:2181 --topic news-access --replication-factor 1 --partitions 1 --create

    kafka-console-consumer.sh --zookeeper 192.168.0.103:2181,192.168.0.104:2181 --topic news-access --from-beginning



## Spark Streaming实战开发进阶-综合案例3：页面pv实时统计

代码：
    
    AccessProducer.java
    NewsRealtimeStatSpark.java


## Spark Streaming实战开发进阶-综合案例3：注册用户数实时统计


代码：

    NewsRealtimeStatSpark.java


## Spark Streaming实战开发进阶-综合案例3：用户跳出量实时统计


代码：

    NewsRealtimeStatSpark.java


## Spark Streaming实战开发进阶-综合案例3：版块pv实时统计

代码：

    NewsRealtimeStatSpark.java















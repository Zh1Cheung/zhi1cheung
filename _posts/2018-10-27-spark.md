---
title: Spark-User Behavior Analysis
categories:
- Spark
- Big Data
tags:
- Spark



---
 
  
  

# Spark用户行为分析

## 一、环境搭建
 
- CentOS 6.4
- hadoop-2.5.0-cdh5.3.6
- hive-0.13.1-cdh5.3.6
- zookeeper-3.4.5-cdh5.3.6
- kafka_2.9.2-0.8.1
- flume-ng-1.5.0-cdh5.3.6
- spark-1.5.1-bin-hadoop2.4

## 二、流程介绍

### 离线日志采集流程介绍

- 我们的数据从哪里来?
->发送请求到后台服务器，Nginx接收请求并进行转发
->1.Nginx来接收请求，并且后端接入Tomcat集群/Jetty集群，来进行高并发访问下的负载均衡。
  2.Nginx，或者是Tomcat，你进行适当配置之后，所有请求的数据都会作为log存储起来
  3.可能有多份日志文件，因为有多个web服务器。
  	1.一个日志转移的工具，比如自己用linux的crontab定时调度一个shell脚本/python脚本
  	2.或者自己用java开发一个后台服务，用quartz这样的框架进行定时调度
  	3.然后作为一份日志文件，给转移到flume agent正在监控的目录中。
->1.flume agent启动起来以后, 可以实时的监控linux系统上面的某一个目录,看其中是否有新的文件进来
  2.只要发现有新的日志文件进来，那么flume就会走后续的channel和sink。
  3.通常来说，sink都会配置为HDFS。
->flume负责将每天的一份log文件，传输到HDFS上
->HDFS用来存储每天的log数据
  1.使用Hadoop MapReduce，自己开发MR作业,可以用crontab定时调度工具来定时每天执行一次
  2.Oozie来进行定时调度
  3.针对HDFS里的原始日志进行数据清洗，写入HDFS中另外一个文件
  4.HDFS：存储一份经过数
  据清洗的日志文件。
->把HDFS中的清洗后的数据，给导入到Hive的某个表中。
  1.这里可以使用动态分区，Hive使用分区表，每个分区放一天的数据。
  2.Hive，底层也是基于HDFS，作为一个大数据的数据仓库
  3.数据仓库内部，再往后，其实就是一些数据仓库建模的ETL
  4.ETL会将原始日志所在的一个表，给转换成几十张，甚至上百张表。这些表，就是我们的数据仓库。
  5.公司的统计分析人员，就会针对数据仓库中的表，执行临时的，或者每天定时调度的Hive SQL ETL作业。来进行大数据的统计和分析。
->Spark/Hdoop/Storm，大数据平台/系统，可能都会使用Hive中的数据仓库内部的表
  1.我们的Spark大数据系统，数据来源都是Hive中的某些表
  2.开发特殊的，符合业务需求的大数据平台
  3.通过大数据平台来给公司里的用户进行使用，来提供大数据的支持，推动公司的发展



### 实时数据采集流程介绍

- 我们的数据从哪里来?
->1.Nginx，后台Web服务器（Tomcat、Jetty），后台系统（J2EE、PHP）。
  2.到这一步为止，其实还是可以跟我们之前的离线日志收集流程一样。
->flume，监控指定的文件夹
	->可以每天收集一份，放到flume，转移到HDFS里面，清洗后放入Hive，建立离线的数据仓库。
	->也可以每收集1分钟的数据，或者每收集一点数据，就放入文件，然后转移到flume中去,可以配置flume，将数据写入Kafka
->实时数据，通常都是从分布式消息队列集群中读取的，比如Kafka
->，再由我们后端的实时数据处理程序（Storm、Spark Streaming），实时从Kafka中读取数据，log日志。然后进行实时的计算和处理。
->1.大数据实时计算系统，比如说用Storm、Spark Streaming开发的，
  2.可以实时的从Kafka中拉取数据，然后对实时的数据进行处理和计算，
  3.这里可以封装大量复杂的业务逻辑，甚至调用复杂的机器学习、数据挖掘、智能推荐的算法，然后实现实时的车辆调度、实时推荐。(广告流量的实时统计)



## 三、用户访问session分析

### 介绍
#### 1.模块介绍

  - 1、对用户访问session进行分析
  - 2、JDBC辅助类封装
  - 3、用户访问session聚合统计
  - 4、按时间比例随机抽取session
  - 5、获取点击、下单和支付次数排名前10的品类
  - 6、获取top10品类的点击次数最多的10个session
  - 7、复杂性能调优全套解决方案
  - 8、十亿级数据troubleshooting经验总结
  - 9、数据倾斜全套完美解决方案
  - 10、模块功能演示


- 模块的目标：对用户访问session进行分析

1、可以根据使用者指定的某些条件，筛选出指定的一些用户（有特定年龄、职业、城市）；
2、对这些用户在指定日期范围内发起的session，进行聚合统计，比如，统计出访问时长在0~
3s的session占总session数量的比例；


1、可以根据使用者指定的某些条件，筛选出指定的一些用户（有特定年龄、职业、城市）；

2、对这些用户在指定日期范围内发起的session，进行聚合统计，比如，统计出访问时长在0~
3s的session占总session数量的比例；

3、按时间比例，比如一天有24个小时，其中12:00~
13:00的session数量占当天总session数量的50%，当天总session数
量是10000个，那么当天总共要抽取1000个session，ok，12:00-13:00的用户，就得抽取1000*
50%=500。而且这500个需要随机抽取。

4、获取点击量、下单量和支付量都排名10的商

---
title: Cache（九）
categories:
- Caching architecture
tags:
- Caching architecture
- Dynamic rendering system

---
 
## 大型电商网站的商品详情页的深入分析


商品详情页介绍

	商品详情页的多模板化
		多套模板：聚划算、天猫超市、淘抢购、电器城
		不同模板的元数据一样，只是展示方式不一样
		不同的业务，商品详情页的个性化需求很多，数据来源也很多
	
	商品详情页结构
		时效性比较低的数据
			一个商品详情包含了不同的维度
			商品维度：标题，图片，属性，等等
			主商品维度：商品介绍，规格参数，等等
			分类维度
			商家维度
			时效性比较低的数据
				其实后面会讲解，都是在一个商品详情页被访问到的时候，将数据动态直接渲染/填充到一个html模板中去的
				在浏览器展现的时候，数据写死在html里面，直接就出来了
				因为比如说，一个商品的数据变更了，可能是异步的去更新数据的，也许需要5分钟，或者10分钟的时间，才能将变更的数据反映的商品详情页中去
		实时性较高的数据
			实时价格、实时促销、广告词、配送至、预售、库存
			ajax异步加载
			在访问商品详情页的时候，价格、库存、促销活动、广告词，都没有直接写死到html中，直接是在html里放了一个js脚本
			然后在html在浏览器显示出来的时候，js脚本运行，直接发送ajax请求到后端
			后端接口，直接查询价格、库存、促销活动、广告词，最新的数据
			只要你变更了数据，那么在下一次商品详情页展示的时候，一定可以将最新的数据展示出来
	
	在淘宝网上展示一个通用商品模板，商品详情页结构拆解说明，分析一个商品详情页的多维度构成
		
	亿级流量电商网站的商品详情页访问情况
		访问量：比如双11活动，商品详情页的pv至少达到几亿次，但是经过良好设计的详情页系统，响应时间小于几十ms
		访问特点：离散访问多，热点数据少
		一般来说，访问的比较均匀，很少说集中式访问某个商品详情页，除非是那种秒杀活动，那是集中式访问某个商品详情页
		



## 大型电商网站的商品详情页系统架构是如何一步一步演进的


商品详情页系统架构演进历程

第一个版本

	架构设计
		J2EE+Tomcat+MySQL
		动态页面，每次请求都要调用多个依赖服务的接口，从数据库里查询数据，然后通过类似JSP的技术渲染到HTML模板中，返回最终HTML页面
	架构缺陷
		每次请求都是要访问数据库的，性能肯定很差
		每次请求都要调用大量的依赖服务，依赖服务不稳定导致商品详情页展示的性能经常抖动
	
第二个版本

	架构设计
		页面静态化技术
		通过MQ得到商品详情页涉及到的数据的变更消息
		通过Java Worker服务全量调用所有的依赖服务的接口，查询数据库，获取到构成一个商品详情页的完整数据，并通过velocity等模板技术生成静态HTML
		将静态HTML页面通过rsync工具直接推送到多台nginx服务器上，每台nginx服务器上都有全量的HTML静态页面
		nginx对商品详情页的访问请求直接返回本地的静态HTML页面
		在nginx服务器前加一层负载均衡设备，请求打到任何一台应用nginx服务器上，都有全量的HTML静态页面可以返回
		
	架构缺陷
		全量更新问题
			如果某一个商品分类、商家等信息变更了
			那么那个分类、店铺、商家下面所有的商品详情页都需要重新生成静态HTML页面
		更新速度过慢问题
			分类、店铺、商家、商品越来越多
			重新生成HTML的负载越来越高，rsync全量同步所有nginx的负载也越来越高
			从数据变更到生成静态HTML，再到全量同步到所有nginx，时间越来越慢
		扩容问题
			因为每个商品详情页都要全量同步到所有的nginx上，导致系统无法扩容，无法增加系统容量
	
	架构优化
		解决全量更新问题
			每次Java Worker收到某个维度的变更消息，不是拉去全量维度并生成完整HTML，而是按照维度拆分，生成一个变化维度的HTML片段
			nginx对多个HTML片段通过SSI合并html片段然后输出一个完整的html
		解决扩容问题
			每个商品详情页不是全量同步到所有的nginx
			而是根据商品id路由到某一台nginx上，同时接入层nginx按照相同的逻辑路由请求
		更新速度过慢问题
			增加更多机器资源
			多机房部署，每个机房部署一套Java Worker+应用Nginx，所有机房用一套负载均衡设备，在每个机房内部完成全流程，不跨机房
			
	架构优化后的缺陷
		更新速度还是不够快的问题
			商品的每个维度都有一个HTML片段，rsync推送大量的HTML片段，负载太高，性能较差
			Nginx基于机械硬盘进行SSI合并，性能太差
		还是存在全量更新的问题
			虽然解决了分类、商家、店铺维度的变更，只要增量重新生产较小的HTML片段即可，不用全量重新生成关联的所有商品详情页的HTML
			但是如果某个页面模板变更，或者新加入一个页面模板，还是会导致几亿个商品的HTML片段都要重新生成和rsync，要几天时间才能完成，无法响应需求
		还是存在容量问题
			nginx存储有限，不能无限存储几亿，以及增长的商品详情页的HTML文件
			如果nginx存储达到极限，需要删除部分商品详情页的HTML文件，改成nginx找不到HTML，则调用后端接口，回到动态页面的架构
			动态页面架构在高并发访问的情况下，会对依赖系统造成过大的压力，几乎扛不住
		
第三个版本

	需要支持的需求
		迅速响应各种页面模板的改版和个性化需求的新模板的加入
		页面模块化，页面中的某个区域变化，只要更新这个区域中的数据即可
		支持高性能访问
		支持水平扩容的伸缩性架构
		
	架构设计
		系统架构设计
			依赖服务有数据变更发送消息到MQ
			数据异构Worker服务监听MQ中的变更消息，调用依赖服务的接口，仅仅拉取有变更的数据即可，然后将数据存储到redis中
			数据异构Worker存储到redis中的，都是原子未加工数据，包括商品基本信息、商品扩展属性、商品其他信息、商品规格参数、商品分类、商家信息
			数据异构Worker发送消息到MQ，数据聚合Worker监听到MQ消息
			数据聚合Worker将原子数据从redis中取出，按照维度聚合后存储到redis中，包括三个维度
				基本信息维度：基本信息、扩展属性
				商品介绍：PC版、移动版
				其他信息：商品分类、商家信息
			nginx+lua，lua从redis读取商品各个维度的数据，通过nginx动态渲染到html模板中，然后输出最终的html
		如何解决所有的问题
			更新问题：不再是生成和推送html片段了，不再需要合成html，直接数据更新到redis，然后走动态渲染，性能大大提升
			全量更新问题：数据和模板分离，数据更新呢就更新数据，模板更新直接推送模板到nginx，不需要重新生成所有html，直接走动态渲染
			容量问题：不需要依赖nginx所在机器的磁盘空间存储大量的html，将数据放redis，html就存放模板，大大减少空间占用，而且redis集群可扩容

第一个版本的架构：
![image](http://i2.51cto.com/images/blog/201810/01/be449aea6b19867ca794e7fd27557bed.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


第二个版本优化后的架构:
![image](http://i2.51cto.com/images/blog/201810/01/a0cfcd304ed13f84f74cd765076a64a6.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


第二个版本架构以及其缺陷:
![image](http://i2.51cto.com/images/blog/201810/01/9f97014a6f4f83f88274652e9fae0c6c.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

第三个版本的架构:
![image](http://i2.51cto.com/images/blog/201810/01/cebe2d7235149198992cb999f8d27ee6.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)






## 亿级流量大型电商网站的商品详情页系统架构的整体设计


商品详情页整体架构组成

	动态渲染系统
		将页面中静的数据，直接在变更的时候推送到缓存，然后每次请求页面动态渲染新数据
		商品详情页系统（负责静的部分）：被动接收数据，存储redis，nginx+lua动态渲染
		商品详情页动态服务系统（对外提供数据接口）
			提供各种数据接口
			动态调用依赖服务的接口，产生数据并且返回响应
			从商品详情页系统处理出来的redis中，获取数据，并返回响应
			
	OneService系统
		动的部分，都是走ajax异步请求的，不是走动态渲染的
		商品详情页统一服务系统（负责动的部分）
		
	前端页面
		静的部分，直接被动态渲染系统渲染进去了
		动的部分，html一到浏览器，直接走js脚本，ajax异步加载
		商品详情页，分段存储，ajax异步分屏加载
		
	工程运维
		限流，压测，灰度发布





商品详情页系统整体架构设计:

![image](http://i2.51cto.com/images/blog/201810/01/e607cb4deeacc9b85b14d058625adbd3.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


## 商品详情页动态渲染系统：大型网站的多机房4级缓存架构设计


多级缓存架构
	本地缓存
		使用nginx shared dict作为local cache，http-lua-module的shared dict可以作为缓存，而且reload nginx不会丢失
		也可以使用nginx proxy cache做local cache
		双层nginx部署，一层接入，一层应用，接入层用hash路由策略提升缓存命中率
			比如库存缓存数据的TP99为5s，本地缓存命中率25%，redis命中率28%，回源命中率47%
			一次普通秒杀活动的命中率，本地缓存55%，分布式redis命中率15%，回源命中率27%
			最高可以提升命中率达到10%
		全缓存链路维度化存储，如果有3个维度的数据，只有其中1个过期了，那么只要获取那1个过期的数据即可
		nginx local cache的过期时间一般设置为30min，到后端的流量会减少至少3倍
		
	4级多级缓存
	
		nginx本地缓存，抗热点数据，小内存缓存访问最频繁的数据
		各个机房本地的redis从集群的数据，抗大量离线数据，采用一致性hash策略构建分布式redis缓存集群
		tomcat中的动态服务的本地jvm堆缓存
			支持在一个请求中多次读取一个数据，或者与该数据相关的数据
			作为redis崩溃的备用防线
			固定缓存一些较少访问频繁的数据，比如分类，品牌等数据
			堆缓存过期时间为redis过期时间的一半
		主redis集群
			命中率非常低，小于5%
			防止主从同步延迟导致的数据读取miss
			防止各个机房的从redis集群崩溃之后，全量走依赖服务会导致雪崩，主redis集群是后备防线
			
	主redis集群，采取多机房一主三从的高可用部署架构
		redis集群部署采取双机房一主三活的架构，机房A部署主集群+一个从集群，机房B部署一个从集群（从机房A主集群）+一个从集群（从机房B从集群）
		双机房一主三活的架构，保证了机房A彻底故障的时候，机房B还有一套备用的集群，可以升级为一主一从
		如果采取机房A部署一主一从，机房B一从，那么机房A故障时，机房B的一从承载所有读写压力，压力过大，很难承受


主集群的双机房一主三从部署架构:

![image](http://i2.51cto.com/images/blog/201810/01/631a56ab0d782490499d68e6d641db2f.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

## 商品详情页动态渲染系统：架构整体设计



我们先做动态渲染那套系统

（1）依赖服务 -> MQ -> 动态渲染服务 -> 多级缓存

（2）负载均衡 -> 分发层nginx -> 应用层nginx -> 多级缓存

（3）多级缓存 -> 数据直连服务

动态渲染系统
	
	数据闭环
		数据闭环架构
			依赖服务：商品基本信息，规格参数，商家/店铺，热力图，商品介绍，商品维度，品牌，分类，其他
			发送数据变更消息到MQ
			数据异构Worker集群，监听MQ，将原子数据存储到redis，发送消息到MQ
			数据聚合Worker集群，监听MQ，将原子数据按维度聚合后存储到redis，三个维度（商品基本信息、商品介绍、其他信息）
		数据闭环，就是数据的自我管理，所有数据原样同步后，根据自己的逻辑进行后续的数据加工，走系统流程，以及展示k
		数据形成闭环之后，依赖服务的抖动或者维护，不会影响到整个商品详情页系统的运行
		数据闭环的流程：数据异构（多种异构数据源拉取），数据原子化，数据聚合（按照维度将原子数据进行聚合），数据存储（Redis）
	
	数据维度化
		商品基本信息：标题、扩展属性、特殊属性、图片、颜色尺码、规格参数
		商品介绍
		非商品维度其他信息：分类，商家，店铺，品牌
		商品维度其他信息：采用ajax异步加载，价格，促销，配送至，广告，推荐，最佳组合，等等
		
		采取ssdb，这种基于磁盘的大容量/高性能的kv存储，保存商品维度、主商品维度、商品维度其他信息，数据量大，不能光靠内存去支撑
		采取redis，纯内存的kv存储，保存少量的数据，比如非商品维度的其他数据，商家数据，分类数据，品牌数据
		
		一个完整的数据，拆分成多个维度，每个维度独立存储，就避免了一个维度的数据变更就要全量更新所有数据的问题
		不同维度的数据，因为数据量的不一样，可以采取不同的存储策略
		
	系统拆分
		系统拆分更加细：依赖服务、MQ、数据异构Worker、数据同步Worker、Redis、Nginx+Lua
		每个部分的工作专注，影响少，适合团队多人协作
		异构Worker的原子数据，基于原子数据提供的服务更加灵活
		聚合Worker将数据聚合后，减少redis读取次数，提升性能
		前端展示分离为商品详情页前端展示系统和商品介绍前端展示系统，不同特点，分离部署，不同逻辑，互相不影响
		
	异步化
		异步化，提升并发能力，流量削峰
		消息异步化，让各个系统解耦合，如果使用依赖服务调用商品详情页系统接口同步推送，那么就是耦合的
		缓存数据更新异步化，数据异构Worker同步调用依赖服务接口，但是异步更新redis
		
	动态化
		数据获取动态化：nginx+lua获取商品详情页数据的时候，按照维度获取，比如商品基本数据、其他数据（分类、商家）
		模板渲染实时化：支持模板页面随时变化，因为采用的是每次从nginx+redis+ehcache缓存获取数据，渲染到模板的方式，因此模板变更不用重新静态化HTML
		重启应用秒级化：nginx+lua架构，重启在秒级
		需求上线快速化：使用nginx+lua架构开发商品详情页的业务逻辑，非常快速
			
	多机房多活
		Worker无状态，同时部署在各自的机房时采取不同机房的配置，来读取各自机房内部部署的数据集群（redis、mysql等）
			将数据异构Worker和数据聚合Worker设计为无状态化，可以任意水平扩展
			Worker无状态化，但是配置文件有状态，不同的机房有一套自己的配置文件，只读取自己机房的redis、ssdb、mysql等数据
		每个机房配置全链路：接入nginx、商品详情页nginx+商品基本信息redis集群+其他信息redis集群、商品介绍nginx+商品介绍redis集群
		部署统一的CDN以及LVS+KeepAlived负载均衡设备



![image](http://i2.51cto.com/images/blog/201810/01/d2bd11c7a55425b9d1f4c083be8d369b.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



## 商品详情页动态渲染系统：复杂的消息队列架构设计


队列化

	任务等待队列
	任务排重队列（异构Worker对一个时间段内的变更消息做排重）
	失败任务队列（失败重试机制）
	优先级队列，刷数据队列（依赖服务洗数据）、高优先级队列（活动商品优先级高）

复杂的消息队列架构设计:

![](http://i2.51cto.com/images/blog/201810/01/4b2421754186d35a3eb22865e85041ed.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



## 商品详情页动态渲染系统：使用多线程并发提升系统吞吐量的设计



并发化

	数据同步服务做并发化+合并，将多个变更消息合并在一起，调用依赖服务一次接口获取多个数据，采用多线程并发调用
	数据聚合服务做并发化，每次重新聚合数据的时候，对多个原子数据用多线程并发从redis查询
	
	
多线程提升吞吐量的设计:
![image](http://i2.51cto.com/images/blog/201810/01/40afbc3302c2459e8db43ae1d2a7c067.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


## 商品详情页动态渲染系统：redis批量查询性能优化设计


redis批量查询的性能优化:
![image](http://i2.51cto.com/images/blog/201810/01/02b5aebf8f07c15ed5368ead011247e9.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



## 商品详情页动态渲染系统：全链路高可用架构设计


高可用设计
	
	读链路多级降级：本机房从集群 -> 主集群 -> 直连
	
	全链路隔离
		基于hystrix的依赖调用隔离，限流，熔断，降级
		普通服务的多机房容灾冗余部署以及隔离
		
	
![image](http://i2.51cto.com/images/blog/201810/01/7f6b755e52f535c533dc97b08a6eee84.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


## 商品详情页动态渲染系统：微服务架构设计


1、领域驱动设计：我们需要对这个系统涉及到的领域模型机进行分析，然后进行领域建模，最后的话，设计出我们对应的微服务的模型

2、spring cloud：微服务的基础技术架构，我们用spring cloud来做

3、持续交付流水线，jenkins+git+自动化持续集成+自动化测试+自动化部署

4、docker：大量的微服务的部署与管理


## 商品详情页动态渲染系统：机房与机器的规划

    虚拟机，要弄几台，大概怎么来部署
    
    负载均衡：2台机器，lvs+keepalived，双机高可用
    
    两个机房，每个机房给1台机器，总共就是2台机器，分发层nginx+应用层nginx+缓存从集群
    
    缓存主集群：模拟跟上面的两个机房部署在一起，在实际生产环境中，的确可能是在相同的机房，但是肯定在不同的机器上
    
    我们这里不会有真正的机房，但是会模拟出来，有些机器会在某个机房里
    
    缓存集群分片中间件，跟缓存集群部署在一起
    
    rabbitmq和mysql：1台机器


## 商品详情页动态渲染系统：部署CentOS虚拟机集群



2台3G内存的虚拟机

1、在虚拟机中安装CentOS

（1）使用课程提供的CentOS
6.5镜像即可，CentOS-6.5-i386-minimal.iso。

（2）创建虚拟机：打开Virtual Box，点击“新建”按钮，点击“下一步”，输入虚拟机名称为eshop-cache01，选择操作系统为Linux，选择版本为Red Hat，分配1024MB内存，后面的选项全部用默认，在Virtual Disk File location and size中，一定要自己选择一个目录来存放虚拟机文件，最后点击“create”按钮，开始创建虚拟机。

（3）设置虚拟机网卡：选择创建好的虚拟机，点击“设置”按钮，在网络一栏中，连接方式中，选择“Bridged Adapter”。

（4）安装虚拟机中的CentOS 6.5操作系统：选择创建好的虚拟机，点击“开始”按钮，选择安装介质（即本地的CentOS 6.5镜像文件），选择第一项开始安装-Skip-欢迎界面Next-选择默认语言-Baisc Storage Devices-Yes, discard any data-主机名:spark2upgrade01-选择时区-设置初始密码为hadoop-Replace Existing Linux System-Write changes to disk-CentOS 6.5自己开始安装。

（5）安装完以后，CentOS会提醒你要重启一下，就是reboot，你就reboot就可以了。

（6）配置网络

    vi /etc/sysconfig/network-scripts/ifcfg-eth0
    
    DEVICE=eth0
    TYPE=Ethernet
    ONBOOT=yes
    BOOTPROTO=dhcp
    service network restart
    ifconfig
    
    BOOTPROTO=static
    IPADDR=192.168.0.X
    NETMASK=255.255.255.0
    GATEWAY=192.168.0.1
    service network restart

（7）配置hosts

    vi /etc/hosts
    配置本机的hostname到ip地址的映射

（8）配置SecureCRT

此时就可以使用SecureCRT从本机连接到虚拟机进行操作了

一般来说，虚拟机管理软件，virtual box，可以用来创建和管理虚拟机，但是一般不会直接在virtualbox里面去操作，因为比较麻烦，没有办法复制粘贴

比如后面我们要安装很多其他的一些东西，perl，java，redis，storm，复制一些命令直接去执行

SecureCRT，在windows宿主机中，去连接virtual box中的虚拟机

收费的，我这里有完美破解版，跟着课程一起给大家，破解

（9）关闭防火墙

    service iptables stop
    service ip6tables stop
    chkconfig iptables off
    chkconfig ip6tables off
    
    vi /etc/selinux/config
    SELINUX=disabled

关闭windows的防火墙

后面要搭建集群，有的大数据技术的集群之间，在本地你给了防火墙的话，可能会没有办法互相连接，会导致搭建失败

（10）配置yum

    yum clean all
    yum makecache
    yum install wget



2、在每个CentOS中都安装Java和Perl

（1）安装JDK

1、将jdk-7u60-linux-i586.rpm通过WinSCP上传到虚拟机中

2、安装JDK：rpm -ivh jdk-7u65-linux-i586.rpm

3、配置jdk相关的环境变量

    vi .bashrc
    export JAVA_HOME=/usr/java/latest
    export PATH=$PATH:$JAVA_HOME/bin
    source .bashrc

4、测试jdk安装是否成功：java -version

（2）安装Perl

    yum install -y gcc
    
    wget http://www.cpan.org/src/5.0/perl-5.16.1.tar.gz
    tar -xzf perl-5.16.1.tar.gz
    cd perl-5.16.1
    ./Configure -des -Dprefix=/usr/local/perl
    make && make test && make install
    perl -v



3、在另外一个虚拟机中安装CentOS集群

（1）按照上述步骤，再安装1台一模一样环境的linux机器

（2）另外三台机器的hostname分别设置为eshop-detail02

（3）安装好之后，在每台机器的hosts文件里面，配置好所有的机器的ip地址到hostname的映射关系


4、配置2台CentOS为ssh免密码互相通信

（1）首先在三台机器上配置对本机的ssh免密码登录

    ssh-keygen -t rsa
生成本机的公钥，过程中不断敲回车即可，ssh-keygen命令默认会将公钥放在/root/.ssh目录下

    cd /root/.ssh
    cp id_rsa.pub authorized_keys
    
将公钥复制为authorized_keys文件，此时使用ssh连接本机就不需要输入密码了

（2）接着配置三台机器互相之间的ssh免密码登录
使用ssh-copy-id -i hostname命令将本机的公钥拷贝到指定机器的authorized_keys文件中


## 商品详情页动态渲染系统：双机房部署接入层与应用层Nginx+Lua



openresty，我们之前都给大家讲解过了

部署了两台虚拟机

模拟的场景是什么，假设这两台虚拟机分别在不同的机房中，每个机房里都有一台机器，所以按照我们之前讲解的那套双机房的四级缓存架构

部署nginx，虚拟机，每台机器上，部署两个nginx，一个是分发层nginx，一个是应用层nginx

在实际生产环境中

1、部署第一个nginx

（1）部署openresty

    mkdir -p /usr/servers  
    cd /usr/servers/
    
    yum install -y readline-devel pcre-devel openssl-devel gcc
    
    wget http://openresty.org/download/ngx_openresty-1.7.7.2.tar.gz  
    tar -xzvf ngx_openresty-1.7.7.2.tar.gz  
    cd /usr/servers/ngx_openresty-1.7.7.2/
    
    cd bundle/LuaJIT-2.1-20150120/  
    make clean && make && make install  
    ln -sf luajit-2.1.0-alpha /usr/local/bin/luajit
    
    cd bundle  
    wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gz  
    tar -xvf 2.3.tar.gz  
    
    cd bundle  
    wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz  
    tar -xvf v0.3.0.tar.gz  
    
    cd /usr/servers/ngx_openresty-1.7.7.2  
    ./configure --prefix=/usr/servers --with-http_realip_module  --with-pcre  --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2  
    make && make install 
    
    cd /usr/servers/  
    ll
    
    /usr/servers/luajit
    /usr/servers/lualib
    /usr/servers/nginx
    /usr/servers/nginx/sbin/nginx -V 
    
    启动nginx: /usr/servers/nginx/sbin/nginx

（2）nginx+lua开发的hello world

    vi /usr/servers/nginx/conf/nginx.conf
    
    在http部分添加：
    
    lua_package_path "/usr/servers/lualib/?.lua;;";  
    lua_package_cpath "/usr/servers/lualib/?.so;;";  
    
    /usr/servers/nginx/conf下，创建一个lua.conf
    
    server {  
        listen       80;  
        server_name  _;  
    }  
    
    在nginx.conf的http部分添加：
    
    include lua.conf;
    
    验证配置是否正确：
    
    /usr/servers/nginx/sbin/nginx -t
    
    在lua.conf的server部分添加：
    
    location /lua {  
        default_type 'text/html';  
        content_by_lua 'ngx.say("hello world")';  
    } 
    
    /usr/servers/nginx/sbin/nginx -t  

重新nginx加载配置

    /usr/servers/nginx/sbin/nginx -s reload  
    
    访问http: http://192.168.31.187/lua
    
    vi /usr/servers/nginx/conf/lua/test.lua
    
    ngx.say("hello world"); 
    
    修改lua.conf
    
    location /lua {  
        default_type 'text/html';  
        content_by_lua_file conf/lua/test.lua; 
    }

查看异常日志

    tail -f /usr/servers/nginx/logs/error.log

（3）工程化的nginx+lua项目结构

项目工程结构

    hello
        hello.conf     
        lua              
          hello.lua
        lualib            
          *.lua
          *.so

放在/usr/hello目录下
    
    /usr/servers/nginx/conf/nginx.conf
    
    worker_processes  2;  
    
    error_log  logs/error.log;  
    
    events {  
        worker_connections  1024;  
    }  
    
    http {  
        include       mime.types;  
        default_type  text/html;  
      
        lua_package_path "/usr/hello/lualib/?.lua;;";  
        lua_package_cpath "/usr/hello/lualib/?.so;;"; 
        include /usr/hello/hello.conf;  
    }  
    
    /usr/hello/hello.conf
    
    server {  
        listen       80;  
        server_name  _;  
      
        location /lua {  
            default_type 'text/html';  
            lua_code_cache off;  
            content_by_lua_file /usr/example/lua/test.lua;  
        }  
    }  


部署结构图说明:
![image](http://i2.51cto.com/images/blog/201810/02/2eb3d105cbf9ce383c12060f97ea02d5.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)




## 商品详情页动态渲染系统：为什么是twemproxy+redis而不是redis cluster？


1、LVS那块不讲解

LVS+KeepAlived，负载均衡那块，讲一讲，还是不讲了，意义不是太大

MySQL+Atlas，分库分表，鸡肋



2、redis cluster的问题

twemproxy+redis去做集群，redis部署多个主实例，每个主实例可以挂载一些redis从实例，如果将不同的数据分片，写入不同的redis主实例中，twemproxy这么一个缓存集群的中间件

redis cluster

（1）不好做读写分离，读写请求全部落到主实例上的，如果要扩展写QPS，或者是扩展读QPS，都是需要扩展主实例的数量，从实例就是一个用做热备+高可用

（2）不好跟nginx+lua直接整合，lua->redis的client api，但是不太支持redis cluster，中间就要走一个中转的java服务

（3）不好做树状集群结构，比如redis主集群一主三从双机房架构，redis cluster不太好做成那种树状结构

（4）方便，相当于是上下线节点，集群扩容，运维工作，高可用自动切换，比较方便

3、twemproxy+redis

（1）上线下线节点，有一些手工维护集群的成本

（2）支持redis集群+读写分离，就是最基本的多个redis主实例，twemproxy这个中间件来决定的，java/nginx+lua客户端，是连接twemproxy中间件的。每个redis主实例就挂载了多个redis从实例，高可用->哨兵，redis cluster读写都要落到主实例的限制，你自己可以决定写主，读从，等等

（3）支持redis cli协议，可以直接跟nginx+lua整合

（4）可以搭建树状集群结构

4、如何选择？

（1）看你是否一定需要那3点了，如果不需要，那么用redis cluster也ok，大多数情况下，很多应用用redis就是比较简单的，做一个缓存

（2）如果你的架构里很需要那3点，那么用twemproxy比较好，商品详情页系统的整套架构


## 商品详情页动态渲染系统：redis复习以及twemproxy基础知识讲解



1、部署redis

    tar -zxvf redis-2.8.19.tar.gz --版本过旧，实际在做类似这种nginx+lua生产环境的部署的时候，不一定用最新的版本就是最好，老版本一般比较稳定，nginx+lua整合，用老点的版本，会比较保险一些
    cd redis-2.8.19
    make
    
    nohup /usr/local/redis-test/redis-2.8.19/src/redis-server  /usr/local/redis-test/redis-2.8.19/redis.conf &  
    
    ps -aux | grep redis
    
    /usr/local/redis-test/redis-2.8.19/src/redis-cli -p 6379  
    
    set k1 v1
    get k1

2、twemproxy部署

    yum install -y autoconf automake libtool
    
    yum remove -y autoconf --直接将autoconf和automake、libtool都删除掉了
    
    wget ftp://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz
    tar -zxvf autoconf-2.69.tar.gz
    cd autoconf-2.69 
    ./configure --prefix=/usr
    make && make install
    
    wget http://ftp.gnu.org/gnu/automake/automake-1.14.tar.gz
    tar -zxvf automake-1.14.tar.gz 
    cd automake-1.14
    ./bootstrap.sh
    ./configure --prefix=/usr
    make && make install
    
    wget http://ftpmirror.gnu.org/libtool/libtool-2.4.2.tar.gz
    tar -zxvf libtool-2.4.2.tar.gz
    cd libtool-2.4.2
    ./configure --prefix=/usr
    make && make install
    
    tar -zxvf twemproxy-0.4.0.tar.gz
    
    cd twemproxy-0.4.0
    
    autoreconf -fvi
    ./configure && make
    
    vi /usr/local/twemproxy-test/twemproxy-0.4.0/conf/nutcracker.yml  
    
    server1:  
      listen: 127.0.0.1:1111  
      hash: fnv1a_64  
      distribution: ketama  
      redis: true  
      servers:  
       - 127.0.0.1:6379:1 
    
    /usr/local/twemproxy-test/twemproxy-0.4.0/src/nutcracker -d -c /usr/local/twemproxy-test/twemproxy-0.4.0/conf/nutcracker.yml 
    
    ps -aux | grep nutcracker
    
    /usr/local/redis-test/redis-2.8.19/src/redis-cli -p 1111  
    
    get k1
    set k1 v2
    get k1

3、redis配置

    redis.conf
    
    port 6379
    logfile ""
    maxmemory 100mb
    maxmemory-policy volatile-lru
    maxmemory-samples 10

4、redis主从

    cp redis.conf redis_6379.conf
    cp redis.conf redis_6380.conf
    
    两份文件分别将端口设置为6379和6380
    
    分别启动两个redis实例
    
    nohup /usr/local/redis-2.8.19/src/redis-server  /usr/local/redis-2.8.19/redis_6379.conf &  
    nohup /usr/local/redis-2.8.19/src/redis-server  /usr/local/redis-2.8.19/redis_6380.conf &
    
    ps -aux | grep redis 
    
    通过从实例客户端挂载主从
    
    /usr/local/redis-2.8.19/src/redis-cli  -p 6380
    
    slaveof 127.0.0.1 6379
    
    info replication
    
    /usr/local/redis-2.8.19/src/redis-cli -p 6379
    
    set k2 v2
    
    /usr/local/redis-2.8.19/src/redis-cli -p 6380
    
    get k2

5、twemproxy讲解

    eshop-detail-test:  
      listen: 127.0.0.1:1111  
      hash: fnv1a_64  
      distribution: ketama  
      timeout:1000  
      redis: true  
      servers:  
       - 127.0.0.1:6379:1 test-redis-01 
       - 127.0.0.1:6380:1 test-redis-02

    eshop-detail-test: redis集群的逻辑名称
    listen：twemproxy监听的端口号
    hash：hash散列算法
    distribution：分片算法，一致性hash，取模，等等
    timeout：跟redis连接的超时时长
    redis：是否是redis，false的话是memcached
    servers：redis实例列表，一定要加别名，否则默认使用ip:port:weight来计算分片，如果宕机后更换机器，那么分片就不一样了，因此加了别名后，可以确保分片一定是准确的

你的客户端，java/nginx+lua，连接twemproxy，写数据的时候，twemproxy负责将数据分片，写入不同的redis实例
   
如果某个redis机器宕机，需要自动从一致性hash环上摘掉，等恢复后自动上线

    auto_eject_hosts: true，自动摘除故障节点
    server_retry_timeout: 30000，每隔30秒判断故障节点是否正常，如果正常则放回一致性hash环
    server_failure_limit: 2，多少次无响应，就从一致性hash环中摘除


## 商品详情页动态渲染系统：部署双机房一主三从架构的redis主集群





部署架构：
![image](http://i2.51cto.com/images/blog/201810/02/07e6ffc6c995a27745f8c0233a53bad5.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



在第一台虚拟机上，部署两个redis主实例+两个redis从实例，模拟一个机房的情况
在第二台虚拟机上，部署两个redis从实例，挂载到第一台虚拟机的redis从实例上; 再部署两个redis从实例，挂载到第二台虚拟机的从实例上

    tar -zxvf redis-2.8.19.tar.gz 
    cd redis-2.8.19
    make
    
    master01.conf
    master02.conf
    slave01.conf
    slave02.conf
    
    nohup src/redis-server  ../redis.conf &  
    
    src/redis-cli -p 6379  
    
    slaveof



## 商品详情页动态渲染系统：给每个机房部署一个redis从集群


部署架构:
![image](http://i2.51cto.com/images/blog/201810/02/1952acf63bdd7332af44eb38aa02f407.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



## 商品详情页动态渲染系统：为redis主集群部署twemproxy中间件

    yum install -y autoconf automake libtool
    
    yum remove -y autoconf --直接将autoconf和automake、libtool都删除掉了
    
    wget ftp://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz
    tar -zxvf autoconf-2.69.tar.gz
    cd autoconf-2.69 
    ./configure --prefix=/usr
    make && make install
    
    wget http://ftp.gnu.org/gnu/automake/automake-1.14.tar.gz
    tar -zxvf automake-1.14.tar.gz 
    cd automake-1.14
    ./bootstrap.sh
    ./configure --prefix=/usr
    make && make install
    
    wget http://ftpmirror.gnu.org/libtool/libtool-2.4.2.tar.gz
    tar -zxvf libtool-2.4.2.tar.gz
    cd libtool-2.4.2
    ./configure --prefix=/usr
    make && make install
    
    tar -zxvf twemproxy-0.4.0.tar.gz
    
    cd twemproxy-0.4.0
    
    autoreconf -fvi
    ./configure && make
    
    vi conf/nutcracker.yml  
    
    server1:  
      listen: 127.0.0.1:1111  
      hash: fnv1a_64  
      distribution: ketama  
      redis: true  
      servers:  
       - 127.0.0.1:6379:1 
    
    src/nutcracker -d -c ../conf/nutcracker.yml 
    
    ps -aux | grep nutcracker
    
    src/redis-cli -p 1111  
    
    get k1
    set k1 v2
    get k1



## 商品详情页动态渲染系统：为每个机房的redis从集群部署twemproxy中间件



    
    yum install -y autoconf automake libtool
    
    yum remove -y autoconf --直接将autoconf和automake、libtool都删除掉了
    
    wget ftp://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz
    tar -zxvf autoconf-2.69.tar.gz
    cd autoconf-2.69 
    ./configure --prefix=/usr
    make && make install
    
    wget http://ftp.gnu.org/gnu/automake/automake-1.14.tar.gz
    tar -zxvf automake-1.14.tar.gz 
    cd automake-1.14
    ./bootstrap.sh
    ./configure --prefix=/usr
    make && make install
    
    wget http://ftpmirror.gnu.org/libtool/libtool-2.4.2.tar.gz
    tar -zxvf libtool-2.4.2.tar.gz
    cd libtool-2.4.2
    ./configure --prefix=/usr
    make && make install
    
    tar -zxvf twemproxy-0.4.0.tar.gz
    
    cd twemproxy-0.4.0
    
    autoreconf -fvi
    ./configure && make
    
    vi conf/nutcracker.yml  
    
    server1:  
      listen: 127.0.0.1:1111  
      hash: fnv1a_64  
      distribution: ketama  
      redis: true  
      servers:  
       - 127.0.0.1:6379:1 
    
    src/nutcracker -d -c ../conf/nutcracker.yml 
    
    ps -aux | grep nutcracker
    
    src/redis-cli -p 1111  
    
    get k1
    set k1 v2
    get k1


## 商品详情页动态渲染系统：部署RabbitMQ消息中间件


实际上，现在最近几年，企业里比较流行的还是rabbitmq，因为性能等各方面都比传统的activemq要好很多

安装rabbitmq 3.6.12，当前为止，最新的rabbitmq的版本

1、安装编译工具

    yum install -y ncurses ncurses-base ncurses-devel ncurses-libs ncurses-static ncurses-term ocaml-curses ocaml-curses-devel
    yum install -y openssl-devel zlib-devel
    yum install -y make ncurses-devel gcc gcc-c++ unixODBC unixODBC-devel openssl openssl-devel

2、安装erlang
    
    下载erlang：http://erlang.org/download/otp_src_20.0.tar.gz
    
    tar -zxvf otp_src_20.0.tar.gz
    
    cd otp_src_20.0
    
    ./configure --prefix=/usr/local/erlang --with-ssl -enable-threads -enable-smmp-support -enable-kernel-poll --enable-hipe --without-javac
    
    make && make install
    
    ln -s /usr/local/erlang/bin/erl /usr/local/bin/erl
    
    vi ~/.bashrc
    
    ERLANG_HOME=/usr/local/erlang
    PATH=$ERLANG_HOME/bin:$PATH
    
    source ~/.bashrc
    
    erl

3、安装rabbitmq

    http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.12/rabbitmq-server-generic-unix-3.6.12.tar.xz
    
    yum install -y xz
    
    xz -d rabbitmq-server-generic-unix-3.6.12.tar.xz
    
    tar -xvf rabbitmq-server-generic-unix-3.6.12.tar
    
    mv rabbitmq_server-3.6.1 rabbitmq-3.6.12

开启管理页面的插件

    cd rabbitmq-3.6.1/sbin/
    ./rabbitmq-plugins enable rabbitmq_management

后台启动rabbitmq server

    ./rabbitmq-server -detached

关闭rabbitmq server

    ./rabbitmqctl stop

添加管理员账号

    ./rabbitmqctl add_user rabbitadmin 123456
    ./rabbitmqctl set_user_tags rabbitadmin administrator

进入管理页面

    15672端口号，输入用户名和密码

## 商品详情页动态渲染系统：部署MySQL数据库


用最简单的方式装一个mysql数据库

    yum install -y mysql-server
    
    chkconfig mysqld on
     
    service mysqld start 
    
    mysql -u root
    
    set password for root@localhost=password('root');
    
    mysql -uroot -proot

























































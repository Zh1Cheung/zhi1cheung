---
title: Spark、Scala基础
categories:
- Spark
- Big Data
tags:
- Spark
- Scala


---


##  Spark是什么
Spark，是一种通用的大数据计算框架，正如传统大数据技术Hadoop的MapReduce、Hive引擎，以及Storm流式实时计算引擎等。

Spark包含了大数据领域常见的各种计算框架：比如Spark Core用于离线计算，Spark SQL用于交互式查询，Spark Streaming用于实时流式计算，Spark MLlib用于机器学习，Spark GraphX用于图计算。

Spark主要用于大数据的计算，而Hadoop以后主要用于大数据的存储（比如HDFS、Hive、HBase等），以及资源调度（Yarn）。

Spark+Hadoop的组合，是未来大数据领域最热门的组合，也是最有前景的组合！

### Spark的介绍
Spark，是一种"One Stack to rule them all"的大数据计算框架，期望使用一个技术堆栈就完美地解决大数据领域的各种计算任务。Apache官方，对Spark的定义就是：通用的大数据快速处理引擎。

Spark使用Spark RDD、Spark SQL、Spark Streaming、MLlib、GraphX成功解决了大数据领域中，离线批处理、交互式查询、实时流计算、机器学习与图计算等最重要的任务和问题。

Spark除了一站式的特点之外，另外一个最重要的特点，就是基于内存进行计算，从而让它的速度可以达到MapReduce、Hive的数倍甚至数十倍！

现在已经有很多大公司正在生产环境下深度地使用Spark作为大数据的计算框架，包括eBay、Yahoo!、BAT、网易、京东、华为、大众点评、优酷土豆、搜狗等等。

Spark同时也获得了多个世界顶级IT厂商的支持，包括IBM、Intel等。

 Spark整体架构：
![image](http://i2.51cto.com/images/blog/201810/02/de485a7c681d7f7abc4aa81e5480b459.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


###  Spark的特点


速度快：Spark基于内存进行计算（当然也有部分计算基于磁盘，比如shuffle）。

容易上手开发：Spark的基于RDD的计算模型，比Hadoop的基于Map-Reduce的计算模型要更加易于理解，更加易于上手开发，实现各种复杂功能，比如二次排序、topn等复杂操作时，更加便捷。

超强的通用性：Spark提供了Spark RDD、Spark SQL、Spark Streaming、Spark MLlib、Spark GraphX等技术组件，可以一站式地完成大数据领域的离线批处理、交互式查询、流式计算、机器学习、图计算等常见的任务。

集成Hadoop：Spark并不是要成为一个大数据领域的“独裁者”，一个人霸占大数据领域所有的“地盘”，而是与Hadoop进行了高度的集成，两者可以完美的配合使用。Hadoop的HDFS、Hive、HBase负责存储，YARN负责资源调度；Spark复杂大数据计算。实际上，Hadoop+Spark的组合，是一种“double win”的组合。

极高的活跃度：Spark目前是Apache基金会的顶级项目，全世界有大量的优秀工程师是Spark的committer。并且世界上很多顶级的IT公司都在大规模地使用Spark。

大数据体系概览（Spark的地位）：

![](http://i2.51cto.com/images/blog/201810/02/2ba75bf375f05a73d8ecf80591524427.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)




### Spark VS MapReduce


MapReduce能够完成的各种离线批处理功能，以及常见算法（比如二次排序、topn等），基于Spark RDD的核心编程，都可以实现，并且可以更好地、更容易地实现。而且基于Spark RDD编写的离线批处理程序，运行速度是MapReduce的数倍，速度上有非常明显的优势。

Spark相较于MapReduce速度快的最主要原因就在于，MapReduce的计算模型太死板，必须是map-reduce模式，有时候即使完成一些诸如过滤之类的操作，也必须经过map-reduce过程，这样就必须经过shuffle过程。而MapReduce的shuffle过程是最消耗性能的，因为shuffle中间的过程必须基于磁盘来读写。而Spark的shuffle虽然也要基于磁盘，但是其大量transformation操作，比如单纯的map或者filter等操作，可以直接基于内存进行pipeline操作，速度性能自然大大提升。

但是Spark也有其劣势。由于Spark基于内存进行计算，虽然开发容易，但是真正面对大数据的时候（比如一次操作针对10亿以上级别），在没有进行调优的情况下，可能会出现各种各样的问题，比如OOM内存溢出等等。导致Spark程序可能都无法完全运行起来，就报错挂掉了，而MapReduce即使是运行缓慢，但是至少可以慢慢运行完。

此外，Spark由于是新崛起的技术新秀，因此在大数据领域的完善程度，肯定不如MapReduce，比如基于HBase、Hive作为离线批处理程序的输入输出，Spark就远没有MapReduce来的完善。实现起来非常麻烦。



![](http://i2.51cto.com/images/blog/201810/02/99c16e9dd6b1d43b874ba31a96a8db33.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

###  Spark SQL VS Hive


Spark SQL实际上并不能完全替代Hive，因为Hive是一种基于HDFS的数据仓库，并且提供了基于SQL模型的，针对存储了大数据的数据仓库，进行分布式交互查询的查询引擎。

严格的来说，Spark SQL能够替代的，是Hive的查询引擎，而不是Hive本身，实际上即使在生产环境下，Spark SQL也是针对Hive数据仓库中的数据进行查询，Spark本身自己是不提供存储的，自然也不可能替代Hive作为数据仓库的这个功能。

Spark SQL的一个优点，相较于Hive查询引擎来说，就是速度快，同样的SQL语句，可能使用Hive的查询引擎，由于其底层基于MapReduce，必须经过shuffle过程走磁盘，因此速度是非常缓慢的。很多复杂的SQL语句，在hive中执行都需要一个小时以上的时间。而Spark SQL由于其底层基于Spark自身的基于内存的特点，因此速度达到了Hive查询引擎的数倍以上。

但是Spark SQL由于与Spark一样，是大数据领域的新起的新秀，因此还不够完善，有少量的Hive支持的高级特性，Spark SQL还不支持，导致Spark SQL暂时还不能完全替代Hive的查询引擎。而只能在部分Spark SQL功能特性可以满足需求的场景下，进行使用。

而Spark SQL相较于Hive的另外一个优点，就是支持大量不同的数据源，包括hive、json、parquet、jdbc等等。此外，Spark SQL由于身处Spark技术堆栈内，也是基于RDD来工作，因此可以与Spark的其他组件无缝整合使用，配合起来实现许多复杂的功能。比如Spark SQL支持可以直接针对hdfs文件执行sql语句！


![](http://i2.51cto.com/images/blog/201810/02/5a8cc9907272f1357cfaa5cfe40c4f6c.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)









###  Spark Streaming VS Storm

Spark Streaming与Storm都可以用于进行实时流计算。但是他们两者的区别是非常大的。其中区别之一，就是，Spark Streaming和Storm的计算模型完全不一样，Spark Streaming是基于RDD的，因此需要将一小段时间内的，比如1秒内的数据，收集起来，作为一个RDD，然后再针对这个batch的数据进行处理。而Storm却可以做到每来一条数据，都可以立即进行处理和计算。因此，Spark Streaming实际上严格意义上来说，只能称作准实时的流计算框架；而Storm是真正意义上的实时计算框架。

此外，Storm支持的一项高级特性，是Spark Streaming暂时不具备的，即Storm支持在分布式流式计算程序（Topology）在运行过程中，可以动态地调整并行度，从而动态提高并发处理能力。而Spark Streaming是无法动态调整并行度的。

但是Spark Streaming也有其优点，首先Spark Streaming由于是基于batch进行处理的，因此相较于Storm基于单条数据进行处理，具有数倍甚至数十倍的吞吐量。

此外，Spark Streaming由于也身处于Spark生态圈内，因此Spark Streaming可以与Spark Core、Spark SQL，甚至是Spark MLlib、Spark GraphX进行无缝整合。流式处理完的数据，可以立即进行各种map、reduce转换操作，可以立即使用sql进行查询，甚至可以立即使用machine learning或者图计算算法进行处理。这种一站式的大数据处理功能和优势，是Storm无法匹敌的。

因此，综合上述来看，通常在对实时性要求特别高，而且实时数据量不稳定，比如在白天有高峰期的情况下，可以选择使用Storm。但是如果是对实时性要求一般，允许1秒的准实时处理，而且不要求动态调整并行度的话，选择Spark Streaming是更好的选择。


![](http://i2.51cto.com/images/blog/201810/02/465217d7fd022bd127b0ee6642a63e79.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)






###  Spark的使用体会

首先，Spark目前来说，相较于MapReduce来说，可以立即替代的，并且会产生非常理想的效果的场景，就是要求低延时的复杂大数据交互式计算系统。比如某些大数据系统，可以根据用户提交的各种条件，立即定制执行复杂的大数据计算系统，并且要求低延时（一小时以内）即可以出来结果，并通过前端页面展示效果。在这种场景下，对速度比较敏感的情况下，非常适合立即使用Spark替代MapReduce。因为Spark编写的离线批处理程序，如果进行了合适的性能调优之后，速度可能是MapReduce程序的十几倍。从而达到用户期望的效果。

其次，相对于Hive来说，对于某些需要根据用户选择的条件，动态拼接SQL语句，进行某类特定查询统计任务的系统，其实类似于上述的系统。此时也要求低延时，甚至希望达到几分钟之内。此时也可以使用Spark SQL替代Hive查询引擎。因此场景比较固定，SQL语句的语法比较固定，清楚肯定不会使用到Spark SQL所不支持的Hive语法特性。此时使用Hive查询引擎可以需要几十分钟执行一个复杂SQL。而使用Spark SQl，可能只需要使用几分钟。可以达到用户期望的效果

最后，对于Storm来说，如果仅仅要求对数据进行简单的流式计算处理，那么选择storm或者spark streaming都无可厚非。但是如果需要对流式计算的中间结果（RDD），进行复杂的后续处理，则使用Spark更好，因为Spark本身提供了很多原语，比如map、reduce、groupByKey、filter等等。


## 大纲

### Spark核心编程

    RDD介绍
    ・Spark基本工作原理
    ・Spark开发入门
        ・编写WordCount程序
        ・使用本地模式进行测试
        ・使用spark-submit提交到集群运行（spark-submit常用参数说明）
        ・Spark程序开发流程总结
        ・spark-shell的使用（编写wordcount程序）
    ・创建RDD：并行化集合、基于文件创建RDD
    ・操作RDD：transformation和action，java 8和旧版本的区别，操作key-value对
    ・RDD常用操作全程案例实战
    ・RDD持久化：cache()和persist()，几种持久化策略
    ・共享变量：broadcast variable、accumulator
    ・RDD高级编程：基于排序算法的WordCount、二次排序、topn、combineByKey
    
    
### 结合源码深度剖析Spark内核

    ・Spark内核概览
        ・Spark核心概念
        ・Spark工作流程
        ・Spark运行模式
    ・SparkContext原理剖析与源码分析
    ・job触发流程原理剖析与源码分析
    ・Master原理剖析（资源调度算法）
        ・高可用机制原理剖析
        ・注册机制原理剖析
        ・executor失败容错机制原理剖析
        ・资源调度算法剖析
    ・Worker原理剖析


    ・DAGScheduler原理剖析
        ・stage划分算法
    ・TaskScheduler原理剖析
        ・task分配算法
    ・Executor原理剖析
    ・ShuffleMapTask和ResultTask原理剖析
    ・Shuffle原理剖析
    ・Storage模块原理剖析
        ・BlockManager原理剖析
        ・Cache原理剖析
        ・Checkpoint原理剖析

###  Spark性能优化

    
    ・使用Kryo进行序列化
    ・优化数据结构
    ・对多次执行action operation的RDD进行持久化
    ・对RDD持久化进行序列化
    ・垃圾回收调优
    ・提高并行度
    ・广播大数据集
    ・数据本地化
    ・reduceByKey和groupByKey
    ・shuffle性能调优


### Spark SQL

    ・DataFrame的使用
    ・将RDD转化为DataFrame
    ・支持的数据源（parquet、json、hive、jdbc）
    ・工作原理
    ・性能调优


### Spark Streaming


    ・基本工作原理
    ・WordCount与开发流程
    ・输入DStream（hdfs、socket、kafka）
    ・DStream的transformation操作（updateStateByKey、transform、slide window）
    ・DStream的output操作（性能优化与最佳实践）
    ・Spark Streaming与Spark SQL整合
    ・Cache、Checkpoint、Ahead Write Log
    ・容错机制 
    ・源码剖析
    ・性能调优
  
  
## Scala：Trait

      
      1、trait基础知识
      1-1 将trait作为接口使用
      1-2 在trait中定义具体方法
      1-3 在trait中定义具体字段
      1-4 在trait中定义抽象字段
    
    2、trait高级知识
      2-1 为实例对象混入trait
      2-2 trait调用链
      2-3 在trait中覆盖抽象方法
      2-4 混合使用trait的具体方法和抽象方法
      2-5 trait的构造机制
      2-6 trait字段的初始化
      2-7 让trait继承类

###  将trait作为接口使用

 Scala中的Triat是一种特殊的概念

 首先我们可以将Trait作为接口来使用，此时的Triat就与Java中的接口非常类似

 在triat中可以定义抽象方法，就与抽象类中的抽象方法一样，只要不给出方法的具体实现即可

 类可以使用extends关键字继承trait，注意，这里不是implement，而是extends，在scala中没有implement的概念，无论继承类还是trait，统一都是extends

 类继承trait后，必须实现其中的抽象方法，实现时不需要使用override关键字

scala不支持对类进行多继承，但是支持多重继承trait，使用with关键字即可

    trait HelloTrait {
      def sayHello(name: String)
    }
    trait MakeFriendsTrait {
      def makeFriends(p: Person)
    }
    class Person(val name: String) extends HelloTrait with MakeFriendsTrait with Cloneable with Serializable {
      def sayHello(name: String) = println("Hello, " + name)
      def makeFriends(p: Person) = println("Hello, my name is " + name + ", your name is " + p.name)
    }



###  在Trait中定义具体方法



Scala中的Triat可以不是只定义抽象方法，还可以定义具体方法，此时trait更像是包含了通用工具方法的东西// 有一个专有的名词来形容这种情况，就是说trait的功能混入了类

举例来说，trait中可以包含一些很多类都通用的功能方法，比如打印日志等等，spark中就使用了trait来定义了通用的日志打印方法

    trait Logger {
      def log(message: String) = println(message)
    }
    
    class Person(val name: String) extends Logger {
      def makeFriends(p: Person) {
        println("Hi, I'm " + name + ", I'm glad to make friends with you, " + p.name)
        log("makeFriends methdo is invoked with parameter Person[name=" + p.name + "]")
      }
    }


###  在Trait中定义具体字段


 Scala中的Triat可以定义具体field，此时继承trait的类就自动获得了trait中定义的field

但是这种获取field的方式与继承class是不同的：如果是继承class获取的field，实际是定义在父类中的；而继承trait获取的field，就直接被添加到了类中

    trait Person {
      val eyeNum: Int = 2
    }
    
    class Student(val name: String) extends Person {
      def sayHello = println("Hi, I'm " + name + ", I have " + eyeNum + " eyes.")
    }




###  在Trait中定义抽象字段

 Scala中的Triat可以定义抽象field，而trait中的具体方法则可以基于抽象field来编写

 但是继承trait的类，则必须覆盖抽象field，提供具体的值
    
    trait SayHello {
      val msg: String
      def sayHello(name: String) = println(msg + ", " + name)
    }
    
    class Person(val name: String) extends SayHello {
      val msg: String = "hello"
      def makeFriends(p: Person) {
        sayHello(p.name)
        println("I'm " + name + ", I want to make friends with you!")
      }
    }




###  为实例混入trait



有时我们可以在创建类的对象时，指定该对象混入某个trait，这样，就只有这个对象混入该trait的方法，而类的其他对象则没有

    trait Logged {
      def log(msg: String) {}
    }
    trait MyLogger extends Logged {
      override def log(msg: String) { println("log: " + msg) }
    }  
    class Person(val name: String) extends Logged {
        def sayHello { println("Hi, I'm " + name); log("sayHello is invoked!") }
    }
    
    val p1 = new Person("leo")
    p1.sayHello
    val p2 = new Person("jack") with MyLogger
    p2.sayHello


###  trait调用链


 Scala中支持让类继承多个trait后，依次调用多个trait中的同一个方法，只要让多个trait的同一个方法中，在最后都执行super.方法即可

 类中调用多个trait中都有的这个方法时，首先会从最右边的trait的方法开始执行，然后依次往左执行，形成一个调用链条

 这种特性非常强大，其实就相当于设计模式中的责任链模式的一种具体实现依赖

    trait Handler {
      def handle(data: String) {}
    }
    trait DataValidHandler extends Handler {
      override def handle(data: String) {
        println("check data: " + data)
        super.handle(data)
      } 
    }
    trait SignatureValidHandler extends Handler {
      override def handle(data: String) {
        println("check signature: " + data)
        super.handle(data)
      }
    }
    class Person(val name: String) extends SignatureValidHandler with DataValidHandler {
      def sayHello = { println("Hello, " + name); handle(name) }
    }


###  在trait中覆盖抽象方法

在trait中，是可以覆盖父trait的抽象方法的

但是覆盖时，如果使用了super.方法的代码，则无法通过编译。因为super.方法就会去掉用父trait的抽象方法，此时子trait的该方法还是会被认为是抽象的

此时如果要通过编译，就得给子trait的方法加上abstract override修饰

    trait Logger {
      def log(msg: String)
    }
    
    trait MyLogger extends Logger {
      abstract override def log(msg: String) { super.log(msg) }
    }

###  混合使用trait的具体方法和抽象方法


 在trait中，可以混合使用具体方法和抽象方法

 可以让具体方法依赖于抽象方法，而抽象方法则放到继承trait的类中去实现

 这种trait其实就是设计模式中的模板设计模式的体现

    trait Valid {
      def getName: String
      def valid: Boolean = {
        getName == "leo"    
      }
    }
    class Person(val name: String) extends Valid {
      println(valid)
      def getName = name
    }


###  trait的构造机制


在Scala中，trait也是有构造代码的，也就是trait中的，不包含在任何方法中的代码

而继承了trait的类的构造机制如下：1、父类的构造函数执行；2、trait的构造代码执行，多个trait从左到右依次执行；3、构造trait时会先构造父trait，如果多个trait继承同一个父trait，则父trait只会构造一次；4、所有trait构造完毕之后，子类的构造函数执行

    class Person { println("Person's constructor!") }
    trait Logger { println("Logger's constructor!") }
    trait MyLogger extends Logger { println("MyLogger's constructor!") }
    trait TimeLogger extends Logger { println("TimeLogger's constructor!") }
    class Student extends Person with MyLogger with TimeLogger {
      println("Student's constructor!")
    }




###  trait field的初始化

在Scala中，trait是没有接收参数的构造函数的，这是trait与class的唯一区别，但是如果需求就是要trait能够对field进行初始化，该怎么办呢？只能使用Scala中非常特殊的一种高级特性――提前定义

    trait SayHello {
      val msg: String
      println(msg.toString)
    }
    
    class Person
    val p = new {
      val msg: String = "init"
    } with Person with SayHello
    
    class Person extends {
      val msg: String = "init"
    } with SayHello {}
    
    // 另外一种方式就是使用lazy value
    trait SayHello {
      lazy val msg: String = null
      println(msg.toString)
    }
    class Person extends SayHello {
      override lazy val msg: String = "init"
    }




###  trait继承class


在Scala中，trait也可以继承自class，此时这个class就会成为所有继承该trait的类的父类

    class MyUtil {
      def printMessage(msg: String) = println(msg)
    }
    
    trait Logger extends MyUtil {
      def log(msg: String) = printMessage("log: " + msg)
    }
    
    class Person(val name: String) extends Logger {
      def sayHello {
        log("Hi, I'm " + name)
        printMessage("Hi, I'm " + name)
      }
    }


## Scala：模式匹配

    1、模式匹配的基础语法（案例：成绩评价）
    2、对类型进行模式匹配（案例：异常处理）
    3、对Array和List的元素进行模式匹配（案例：对朋友打招呼）
    4、case class与模式匹配（案例：学校门禁）
    5、Option与模式匹配（案例：成绩查询）

    
###  模式匹配

    
 Scala是没有Java中的switch case语法的，相对应的，Scala提供了更加强大的match case语法，即模式匹配，类替代switch case，match case也被称为模式匹配

 Scala的match case与Java的switch case最大的不同点在于，Java的switch case仅能匹配变量的值，比1、2、3等；而Scala的match case可以匹配各种情况，比如变量的类型、集合的元素、有值或无值

 match case的语法如下：变量 match { case 值 => 代码 }。如果值为下划线，则代表了不满足以上所有情况下的默认情况如何处理。此外，match case中，只要一个case分支满足并处理了，就不会继续判断下一个case分支了。（与Java不同，java的switch case需要用break阻止）

 match case语法最基本的应用，就是对变量的值进行模式匹配
    
     案例：成绩评价
        def judgeGrade(grade: String) {
          grade match {
            case "A" => println("Excellent")
            case "B" => println("Good")
            case "C" => println("Just so so")
            case _ => println("you need work harder")
          }
        }



###  在模式匹配中使用if守卫


 Scala的模式匹配语法，有一个特点在于，可以在case后的条件判断中，不仅仅只是提供一个值，而是可以在值后面再加一个if守卫，进行双重过滤

    // 案例：成绩评价（升级版）
    def judgeGrade(name: String, grade: String) {
      grade match {
        case "A" => println(name + ", you are excellent")
        case "B" => println(name + ", you are good")
        case "C" => println(name + ", you are just so so")
        case _ if name == "leo" => println(name + ", you are a good boy, come on")
        case _ => println("you need to work harder")
      }
    }


###  在模式匹配中进行变量赋值


 Scala的模式匹配语法，有一个特点在于，可以将模式匹配的默认情况，下划线，替换为一个变量名，此时模式匹配语法就会将要匹配的值赋值给这个变量，从而可以在后面的处理语句中使用要匹配的值

 为什么有这种语法？？思考一下。因为只要使用用case匹配到的值，是不是我们就知道这个只啦！！在这个case的处理语句中，是不是就直接可以使用写程序时就已知的值！
 
 但是对于下划线_这种情况，所有不满足前面的case的值，都会进入_这种默认情况进行处理，此时如果我们在处理语句中需要拿到具体的值进行处理呢？那就需要使用这种在模式匹配中进行变量赋值的语法！！

     案例：成绩评价（升级版）
    def judgeGrade(name: String, grade: String) {
      grade match {
        case "A" => println(name + ", you are excellent")
        case "B" => println(name + ", you are good")
        case "C" => println(name + ", you are just so so")
        case _grade if name == "leo" => println(name + ", you are a good boy, come on, your grade is " + _grade)
        case _grade => println("you need to work harder, your grade is " + _grade)
      }
    }



###  对类型进行模式匹配




Scala的模式匹配一个强大之处就在于，可以直接匹配类型，而不是值！！！这点是java的switch case绝对做不到的。
// 理论知识：对类型如何进行匹配？其他语法与匹配值其实是一样的，但是匹配类型的话，就是要用“case 变量: 类型 => 代码”这种语法，而不是匹配值的“case 值 => 代码”这种语法。

    案例：异常处理
    import java.io._
    
    def processException(e: Exception) {
      e match {
        case e1: IllegalArgumentException => println("you have illegal arguments! exception is: " + e1)
        case e2: FileNotFoundException => println("cannot find the file you need read or write!, exception is: " + e2)
        case e3: IOException => println("you got an error while you were doing IO operation! exception is: " + e3)
        case _: Exception => println("cannot know which exception you have!" )
      }
    }



###  对Array和List进行模式匹配



 对Array进行模式匹配，分别可以匹配带有指定元素的数组、带有指定个数元素的数组、以某元素打头的数组
 
 对List进行模式匹配，与Array类似，但是需要使用List特有的::操作符

     案例：对朋友打招呼
    def greeting(arr: Array[String]) {
      arr match {
        case Array("Leo") => println("Hi, Leo!")
        case Array(girl1, girl2, girl3) => println("Hi, girls, nice to meet you. " + girl1 + " and " + girl2 + " and " + girl3)
        case Array("Leo", _*) => println("Hi, Leo, please introduce your friends to me.")
        case _ => println("hey, who are you?")
      }
    }
    
    def greeting(list: List[String]) {
      list match {
        case "Leo" :: Nil => println("Hi, Leo!")
        case girl1 :: girl2 :: girl3 :: Nil => println("Hi, girls, nice to meet you. " + girl1 + " and " + girl2 + " and " + girl3)
        case "Leo" :: tail => println("Hi, Leo, please introduce your friends to me.")
        case _ => println("hey, who are you?")
      }
    }



###  case class与模式匹配

 Scala中提供了一种特殊的类，用case class进行声明，中文也可以称作样例类。case class其实有点类似于Java中的JavaBean的概念。即只定义field，并且由Scala编译时自动提供getter和setter方法，但是没有method。

case class的主构造函数接收的参数通常不需要使用var或val修饰，Scala自动就会使用val修饰（但是如果你自己使用var修饰，那么还是会按照var来）

 Scala自动为case class定义了伴生对象，也就是object，并且定义了apply()方法，该方法接收主构造函数中相同的参数，并返回case class对象

    案例：学校门禁
    class Person
    case class Teacher(name: String, subject: String) extends Person
    case class Student(name: String, classroom: String) extends Person
    
    def judgeIdentify(p: Person) {
      p match {
        case Teacher(name, subject) => println("Teacher, name is " + name + ", subject is " + subject)
        case Student(name, classroom) => println("Student, name is " + name + ", classroom is " + classroom)
        case _ => println("Illegal access, please go out of the school!")
      }  
    }




###  Option与模式匹配



 Scala有一种特殊的类型，叫做Option。Option有两种值，一种是Some，表示有值，一种是None，表示没有值。

 Option通常会用于模式匹配中，用于判断某个变量是有值还是没有值，这比null来的更加简洁明了

 Option的用法必须掌握，因为Spark源码中大量地使用了Option，比如Some(a)、None这种语法，因此必须看得懂Option模式匹配，才能够读懂spark源码。

    案例：成绩查询
    val grades = Map("Leo" -> "A", "Jack" -> "B", "Jen" -> "C")
    
    def getGrade(name: String) {
      val grade = grades.get(name)
      grade match {
        case Some(grade) => println("your grade is " + grade)
        case None => println("Sorry, your grade information is not in the system")
      }
    }



## Scala：类型参数

类型参数是什么？类型参数其实就类似于Java中的泛型。先说说Java中的泛型是什么，比如我们有List a = new ArrayList()，接着a.add(1)，没问题，a.add("2")，然后我们a.get(1) == 2，对不对？肯定不对了，a.get(1)获取的其实是个String――"2"，String――"2"怎么可能与一个Integer类型的2相等呢？

所以Java中提出了泛型的概念，其实也就是类型参数的概念，此时可以用泛型创建List，List a = new ArrayList[Integer]()，那么，此时a.add(1)没问题，而a.add("2")呢？就不行了，因为泛型会限制，只能往集合中添加Integer类型，这样就避免了上述的问题。

那么Scala的类型参数是什么？其实意思与Java的泛型是一样的，也是定义一种类型参数，比如在集合，在类，在函数中，定义类型参数，然后就可以保证使用到该类型参数的地方，就肯定，也只能是这种类型。从而实现程序更好的健壮性。

此外，类型参数是Spark源码中非常常见的，因此同样必须掌握，才能看懂spark源码。


    
    1、泛型类（案例：新生报到）
    2、泛型函数（案例：卡片售卖机）
    3、上边界Bounds（案例：在派对上交朋友）
    4、下边界Bounds（案例：领身份证）
    5、View Bounds（案例：跟小狗交朋友）
    6、Context Bounds（案例：使用Scala内置的比较器比较大小）
    7、Manifest Context Bounds（案例：打包饭菜）
    8、协变和逆变（案例：进入会场）
    9、Existential Type


###  泛型类


 泛型类，顾名思义，其实就是在类的声明中，定义一些泛型类型，然后在类内部，比如field或者method，就可以使用这些泛型类型。

 使用泛型类，通常是需要对类中的某些成员，比如某些field和method中的参数或变量，进行统一的类型限制，这样可以保证程序更好的健壮性和稳定性。

 如果不使用泛型进行统一的类型限制，那么在后期程序运行过程中，难免会出现问题，比如传入了不希望的类型，导致程序出问题。

 在使用类的时候，比如创建类的对象，将类型参数替换为实际的类型，即可。

 Scala自动推断泛型类型特性：直接给使用了泛型类型的field赋值时，Scala会自动进行类型推断。

    案例：新生报到，每个学生来自不同的地方，id可能是Int，可能是String
    
    class Student[T](val localId: T) {
      def getSchoolId(hukouId: T) = "S-" + hukouId + "-" + localId
    }
    
    val leo = new Student[Int](111)


###  泛型函数



 泛型函数，与泛型类类似，可以给某个函数在声明时指定泛型类型，然后在函数体内，多个变量或者返回值之间，就可以使用泛型类型进行声明，从而对某个特殊的变量，或者多个变量，进行强制性的类型限制。

 与泛型类一样，你可以通过给使用了泛型类型的变量传递值来让Scala自动推断泛型的实际类型，也可以在调用函数时，手动指定泛型类型。

    案例：卡片售卖机，可以指定卡片的内容，内容可以是String类型或Int类型
    def getCard[T](content: T) = {
      if(content.isInstanceOf[Int]) "card: 001, " + content
      else if(content.isInstanceOf[String]) "card: this is your card, " + content
      else "card: " + content
    }
    
    getCard[String]("hello world")



###  上边界Bounds



 在指定泛型类型的时候，有时，我们需要对泛型类型的范围进行界定，而不是可以是任意的类型。比如，我们可能要求某个泛型类型，它就必须是某个类的子类，这样在程序中就可以放心地调用泛型类型继承的父类的方法，程序才能正常的使用和运行。此时就可以使用上下边界Bounds的特性。

 Scala的上下边界特性允许泛型类型必须是某个类的子类，或者必须是某个类的父类

    案例：在派对上交朋友
    class Person(val name: String) {
      def sayHello = println("Hello, I'm " + name)
      def makeFriends(p: Person) {
        sayHello
        p.sayHello
      }
    }
    class Student(name: String) extends Person(name)
    class Party[T <: Person](p1: T, p2: T) {
      def play = p1.makeFriends(p2)
    }




###  下边界Bounds


 除了指定泛型类型的上边界，还可以指定下边界，即指定泛型类型必须是某个类的父类

    案例：领身份证
    class Father(val name: String) 
    class Child(name: String) extends Father(name)
    
    def getIDCard[R >: Child](person: R) {
      if (person.getClass == classOf[Child]) println("please tell us your parents' names.")
      else if (person.getClass == classOf[Father]) println("sign your name for your child's id card.")
      else println("sorry, you are not allowed to get id card.")
    }




###  View Bounds

 上下边界Bounds，虽然可以让一种泛型类型，支持有父子关系的多种类型。但是，在某个类与上下边界Bounds指定的父子类型范围内的类都没有任何关系，则默认是肯定不能接受的。
 然而，View Bounds作为一种上下边界Bounds的加强版，支持可以对类型进行隐式转换，将指定的类型进行隐式转换后，再判断是否在边界指定的类型范围内

    案例：跟小狗交朋友
    class Person(val name: String) {
      def sayHello = println("Hello, I'm " + name)
      def makeFriends(p: Person) {
        sayHello
        p.sayHello
      }
    }
    class Student(name: String) extends Person(name)
    class Dog(val name: String) { def sayHello = println("Wang, Wang, I'm " + name) }
    
    implicit def dog2person(dog: Object): Person = if(dog.isInstanceOf[Dog]) {val _dog = dog.asInstanceOf[Dog]; new Person(_dog.name) } else Nil
    
    class Party[T <% Person](p1: T, p2: T)


###  Context Bounds


 Context Bounds是一种特殊的Bounds，它会根据泛型类型的声明，比如“T: 类型”要求必须存在一个类型为“类型[T]”的隐式值。其实个人认为，Context Bounds之所以叫Context，是因为它基于的是一种全局的上下文，需要使用到上下文中的隐式值以及注入。

    案例：使用Scala内置的比较器比较大小
    class Calculator[T: Ordering] (val number1: T, val number2: T) {
      def max(implicit order: Ordering[T]) = if(order.compare(number1, number2) > 0) number1 else number2
    }


###  Manifest Context Bounds

 在Scala中，如果要实例化一个泛型数组，就必须使用Manifest Context Bounds。也就是说，如果数组元素类型为T的话，需要为类或者函数定义[T: Manifest]泛型类型，这样才能实例化Array[T]这种泛型数组。

    案例：打包饭菜（一种食品打成一包）
    class Meat(val name: String)
    class Vegetable(val name: String)
    
    def packageFood[T: Manifest] (food: T*) = {
      val foodPackage = new Array[T](food.length)
      for(i <- 0 until food.length) foodPackage(i) = food(i)
      foodPackage 
    }


###  协变和逆变

 Scala的协变和逆变是非常有特色的！完全解决了Java中的泛型的一大缺憾！

 举例来说，Java中，如果有Professional是Master的子类，那么Card[Professionnal]是不是Card[Master]的子类？答案是：不是。因此对于开发程序造成了很多的麻烦。

 而Scala中，只要灵活使用协变和逆变，就可以解决Java泛型的问题。

    案例：进入会场
    class Master
    class Professional extends Master
    
     大师以及大师级别以下的名片都可以进入会场
    class Card[+T] (val name: String)
    def enterMeet(card: Card[Master]) {
      println("welcome to have this meeting!")
    }
    
     只要专家级别的名片就可以进入会场，如果大师级别的过来了，当然可以了！
    class Card[-T] (val name: String)
    def enterMeet(card: Card[Professional]) {
      println("welcome to have this meeting!")
    }




###  Existential Type


 在Scala里，有一种特殊的类型参数，就是Existential Type，存在性类型。这种类型务必掌握是什么意思，因为在spark源码实在是太常见了！
    
    Array[T] forSome { type T }
    Array[_]






## Scala：类型参数


Scala提供的隐式转换和隐式参数功能，是非常有特色的功能。是Java等编程语言所没有的功能。它可以允许你手动指定，将某种类型的对象转换成其他类型的对象。通过这些功能，可以实现非常强大，而且特殊的功能。

Scala的隐式转换，其实最核心的就是定义隐式转换函数，即implicit conversion function。定义的隐式转换函数，只要在编写的程序内引入，就会被Scala自动使用。Scala会根据隐式转换函数的签名，在程序中使用到隐式转换函数接收的参数类型定义的对象时，会自动将其传入隐式转换函数，转换为另外一种类型的对象并返回。这就是“隐式转换”。

隐式转换函数叫什么名字是无所谓的，因为通常不会由用户手动调用，而是由Scala进行调用。但是如果要使用隐式转换，则需要对隐式转换函数进行导入。因此通常建议将隐式转换函数的名称命名为“one2one”的形式。

Spark源码中有大量的隐式转换和隐式参数，因此必须精通这种语法。
    
    1、隐式转换（案例：特殊售票窗口）
    2、使用隐式转换加强现有类型（案例：超人变身）
    3、隐式转换函数的作用域与导入
    4、隐式转换的发生时机（案例：特殊售票窗口加强版）
    5、隐式参数（案例：考试签到）



###  隐式转换

要实现隐式转换，只要程序可见的范围内定义隐式转换函数即可。Scala会自动使用隐式转换函数。隐式转换函数与普通函数唯一的语法区别就是，要以implicit开头，而且最好要定义函数返回类型。

案例：特殊售票窗口（只接受特殊人群，比如学生、老人等）

    class SpecialPerson(val name: String)
    class Student(val name: String)
    class Older(val name: String)
    
    implicit def object2SpecialPerson (obj: Object): SpecialPerson = {
      if (obj.getClass == classOf[Student]) { val stu = obj.asInstanceOf[Student]; new SpecialPerson(stu.name) }
      else if (obj.getClass == classOf[Older]) { val older = obj.asInstanceOf[Older]; new SpecialPerson(older.name) }
      else Nil
    }
    
    var ticketNumber = 0
    def buySpecialTicket(p: SpecialPerson) = {
      ticketNumber += 1
      "T-" + ticketNumber
    }


###  使用隐式转换加强现有类型



 隐式转换非常强大的一个功能，就是可以在不知不觉中加强现有类型的功能。也就是说，可以为某个类定义一个加强版的类，并定义互相之间的隐式转换，从而让源类在使用加强版的方法时，由Scala自动进行隐式转换为加强类，然后再调用该方法。

    案例：超人变身
    
    class Man(val name: String)
    class Superman(val name: String) {
      def emitLaser = println("emit a laster!")
    }
    
    implicit def man2superman(man: Man): Superman = new Superman(man.name)
    
    val leo = new Man("leo")
    leo.emitLaser



###  隐式转换函数作用域与导入

Scala默认会使用两种隐式转换，一种是源类型，或者目标类型的伴生对象内的隐式转换函数；一种是当前程序作用域内的可以用唯一标识符表示的隐式转换函数。

如果隐式转换函数不在上述两种情况下的话，那么就必须手动使用import语法引入某个包下的隐式转换函数，比如import test._。通常建议，仅仅在需要进行隐式转换的地方，比如某个函数或者方法内，用iimport导入隐式转换函数，这样可以缩小隐式转换函数的作用域，避免不需要的隐式转换。


###  隐式转换的发生时机

 1、调用某个函数，但是给函数传入的参数的类型，与函数定义的接收参数类型不匹配（案例：特殊售票窗口）

 2、使用某个类型的对象，调用某个方法，而这个方法并不存在于该类型时（案例：超人变身）

 3、使用某个类型的对象，调用某个方法，虽然该类型有这个方法，但是给方法传入的参数类型，与方法定义的接收参数的类型不匹配（案例：特殊售票窗口加强版）

     案例：特殊售票窗口加强版
    class TicketHouse {
      var ticketNumber = 0
      def buySpecialTicket(p: SpecialPerson) = {
        ticketNumber += 1
        "T-" + ticketNumber
      }
    }

###  隐式参数


 所谓的隐式参数，指的是在函数或者方法中，定义一个用implicit修饰的参数，此时Scala会尝试找到一个指定类型的，用implicit修饰的对象，即隐式值，并注入参数。
 
 Scala会在两个范围内查找：一种是当前作用域内可见的val或var定义的隐式变量；一种是隐式参数类型的伴生对象内的隐式值

    案例：考试签到
    class SignPen {
      def write(content: String) = println(content)
    }
    implicit val signPen = new SignPen
    
    def signForExam(name: String) (implicit signPen: SignPen) {
      signPen.write(name + " come to exam in time.")
    }


## Scala：Actor入门



Scala的Actor类似于Java中的多线程编程。但是不同的是，Scala的Actor提供的模型与多线程有所不同。Scala的Actor尽可能地避免锁和共享状态，从而避免多线程并发时出现资源争用的情况，进而提升多线程编程的性能。此外，Scala Actor的这种模型还可以避免死锁等一系列传统多线程编程的问题。

Spark中使用的分布式多线程框架，是Akka。Akka也实现了类似Scala Actor的模型，其核心概念同样也是Actor。因此只要掌握了Scala Actor，那么在Spark源码研究时，至少即可看明白Akka Actor相关的代码。但是，换一句话说，由于Spark内部有大量的Akka Actor的使用，因此对于Scala Actor也至少必须掌握，这样才能学习Spark源码。


    1、Actor的创建、启动和消息收发（案例：Actor Hello World）
    2、收发case class类型的消息（案例：用户注册登录后台接口）
    3、Actor之间互相收发消息（案例：打电话）
    4、同步消息和Future


###  Actor的创建、启动和消息收发


 Scala提供了Actor trait来让我们更方便地进行actor多线程编程，就Actor trait就类似于Java中的Thread和Runnable一样，是基础的多线程基类和接口。我们只要重写Actor trait的act方法，即可实现自己的线程执行体，与Java中重写run方法类似。

 此外，使用start()方法启动actor；使用!符号，向actor发送消息；actor内部使用receive和模式匹配接收消息

    案例：Actor Hello World
    import scala.actors.Actor
    
    class HelloActor extends Actor {
      def act() {
        while (true) {
          receive {
            case name: String => println("Hello, " + name)
          }
        }
      }
    }
    
    val helloActor = new HelloActor
    helloActor.start()
    helloActor ! "leo"





###  收发case class类型的消息


 Scala的Actor模型与Java的多线程模型之间，很大的一个区别就是，Scala Actor天然支持线程之间的精准通信；即一个actor可以给其他actor直接发送消息。这个功能是非常强大和方便的。

 要给一个actor发送消息，需要使用“actor ! 消息”的语法。在scala中，通常建议使用样例类，即case class来作为消息进行发送。然后在actor接收消息之后，可以使用scala强大的模式匹配功能来进行不同消息的处理。

    案例：用户注册登录后台接口
    case class Login(username: String, password: String)
    case class Register(username: String, password: String)
    class UserManageActor extends Actor {
      def act() {
        while (true) {
          receive {
            case Login(username, password) => println("login, username is " + username + ", password is " + password)
            case Register(username, password) => println("register, username is " + username + ", password is " + password)
          }
        }
      }
    }
    val userManageActor = new UserManageActor
    userManageActor.start()
    userManageActor ! Register("leo", "1234"); userManageActor ! Login("leo", "1234")
    




###  Actor之间互相收发消息



 如果两个Actor之间要互相收发消息，那么scala的建议是，一个actor向另外一个actor发送消息时，同时带上自己的引用；其他actor收到自己的消息时，直接通过发送消息的actor的引用，即可以给它回复消息。

    案例：打电话
    case class Message(content: String, sender: Actor)
    class LeoTelephoneActor extends Actor {
      def act() {
        while (true) {
          receive {
            case Message(content, sender) => { println("leo telephone: " + content); sender ! "I'm leo, please call me after 10 minutes." }
          }
        }
      }
    }
    class JackTelephoneActor(val leoTelephoneActor: Actor) extends Actor {
      def act() {
        leoTelephoneActor ! Message("Hello, Leo, I'm Jack.", this)
        receive {
          case response: String => println("jack telephone: " + response)
        }
      }
    }




###  同步消息和Future




 默认情况下，消息都是异步的；但是如果希望发送的消息是同步的，即对方接受后，一定要给自己返回结果，那么可以使用!?的方式发送消息。即val reply = actor !? message。

 如果要异步发送一个消息，但是在后续要获得消息的返回值，那么可以使用Future。即!!语法。

    val future = actor !! message
    val reply = future()



















































































































































































---
title: elasticsearch(十三)
categories:
- ELK
tags:
- elasticsearch


---

### 基于全局锁实现悲观锁并发控制


1、悲观锁的简要说明

基于version的乐观锁并发控制

在数据建模，结合文件系统建模的这个案例，把悲观锁的并发控制，3种锁粒度，都给大家仔细讲解一下

最粗的一个粒度，全局锁

/workspace/projects/helloworld

如果多个线程，都过来，要并发地给/workspace/projects/helloworld下的README.txt修改文件名

实际上要进行并发的控制，避免出现多线程的并发安全问题，比如多个线程修改，纯并发，先执行的修改操作被后执行的修改操作给覆盖了

get current version

带着这个current version去执行修改，如果一旦发现数据已经被别人给修改了，version号跟之前自己获取的已经不一样了; 那么必须重新获取新的version号再次尝试修改

上来就尝试给这条数据加个锁，然后呢，此时就只有你能执行各种各样的操作了，其他人不能执行操作

第一种锁：全局锁，直接锁掉整个fs index

2、全局锁的上锁实验

    PUT /fs/lock/global/_create
    {}

fs: 你要上锁的那个index

lock: 就是你指定的一个对这个index上全局锁的一个type

global: 就是你上的全局锁对应的这个doc的id

_create：强制必须是创建，如果/fs/lock/global这个doc已经存在，那么创建失败，报错

利用了doc来进行上锁

    /fs/lock/global /index/type/id --> doc
    
    {
      "_index": "fs",
      "_type": "lock",
      "_id": "global",
      "_version": 1,
      "result": "created",
      "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
      },
      "created": true
    }

另外一个线程同时尝试上锁

    PUT /fs/lock/global/_create
    {}
    
    {
      "error": {
        "root_cause": [
          {
            "type": "version_conflict_engine_exception",
            "reason": "[lock][global]: version conflict, document already exists (current version [1])",
            "index_uuid": "IYbj0OLGQHmMUpLfbhD4Hw",
            "shard": "2",
            "index": "fs"
          }
        ],
        "type": "version_conflict_engine_exception",
        "reason": "[lock][global]: version conflict, document already exists (current version [1])",
        "index_uuid": "IYbj0OLGQHmMUpLfbhD4Hw",
        "shard": "2",
        "index": "fs"
      },
      "status": 409
    }

如果失败，就再次重复尝试上锁

执行各种操作。。。

    POST /fs/file/1/_update
    {
      "doc": {
        "name": "README1.txt"
      }
    }
    
    {
      "_index": "fs",
      "_type": "file",
      "_id": "1",
      "_version": 2,
      "result": "updated",
      "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
      }
    }
    
    DELETE /fs/lock/global
    
    {
      "found": true,
      "_index": "fs",
      "_type": "lock",
      "_id": "global",
      "_version": 2,
      "result": "deleted",
      "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
      }
    }

另外一个线程，因为之前发现上锁失败，反复尝试重新上锁，终于上锁成功了，因为之前获取到全局锁的那个线程已经delete /fs/lock/global全局锁了

    PUT /fs/lock/global/_create
    {}
    
    {
      "_index": "fs",
      "_type": "lock",
      "_id": "global",
      "_version": 3,
      "result": "created",
      "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
      },
      "created": true
    }
    
    POST /fs/file/1/_update 
    {
      "doc": {
        "name": "README.txt"
      }
    }
    
    {
      "_index": "fs",
      "_type": "file",
      "_id": "1",
      "_version": 3,
      "result": "updated",
      "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
      }
    }
    
    DELETE /fs/lock/global

3、全局锁的优点和缺点

优点：操作非常简单，非常容易使用，成本低

缺点：你直接就把整个index给上锁了，这个时候对index中所有的doc的操作，都会被block住，导致整个系统的并发能力很低

上锁解锁的操作不是频繁，然后每次上锁之后，执行的操作的耗时不会太长，用这种方式，方便


### 基于document锁实现悲观锁并发控制



1、对document level锁，详细的讲解

全局锁，一次性就锁整个index，对这个index的所有增删改操作都会被block住，如果上锁不频繁，还可以，比较简单

细粒度的一个锁，document锁，顾名思义，每次就锁你要操作的，你要执行增删改的那些doc，doc锁了，其他线程就不能对这些doc执行增删改操作了

但是你只是锁了部分doc，其他线程对其他的doc还是可以上锁和执行增删改操作的

document锁，是用脚本进行上锁

    POST /fs/lock/1/_update
    {
      "upsert": { "process_id": 123 },
      "script": "if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = 'noop';"
      "params": {
        "process_id": 123
      }
    }

/fs/lock，是固定的，就是说fs下的lock type，专门用于进行上锁

/fs/lock/id，比如1，id其实就是你要上锁的那个doc的id，代表了某个doc数据对应的lock（也是一个doc）

_update + upsert：执行upsert操作

params，里面有个process_id，process_id，是你的要执行增删改操作的进程的唯一id，比如说可以在java系统，启动的时候，给你的每个线程都用UUID自动生成一个thread id，你的系统进程启动的时候给整个进程也分配一个UUID。process_id + thread_id就代表了某一个进程下的某个线程的唯一标识。可以自己用UUID生成一个唯一ID

process_id很重要，会在lock中，设置对对应的doc加锁的进程的id，这样其他进程过来的时候，才知道，这条数据已经被别人给锁了

assert false，不是当前进程加锁的话，则抛出异常

ctx.op='noop'，不做任何修改

如果该document之前没有被锁，/fs/lock/1之前不存在，也就是doc id=1没有被别人上过锁; upsert的语法，那么执行index操作，创建一个/fs/lock/id这条数据，而且用params中的数据作为这个lock的数据。process_id被设置为123，script不执行。这个时候象征着process_id=123的进程已经锁了一个doc了。

如果document被锁了，就是说/fs/lock/1已经存在了，代表doc id=1已经被某个进程给锁了。那么执行update操作，script，此时会比对process_id，如果相同，就是说，某个进程，之前锁了这个doc，然后这次又过来，就可以直接对这个doc执行操作，说明是该进程之前锁的doc，则不报错，不执行任何操作，返回success; 如果process_id比对不上，说明doc被其他doc给锁了，此时报错

    /fs/lock/1
    {
      "process_id": 123
    }
    
    POST /fs/lock/1/_update
    {
      "upsert": { "process_id": 123 },
      "script": "if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = 'noop';"
      "params": {
        "process_id": 123
      }
    }


script：ctx._source.process_id，123
process_id：加锁的upsert请求中带过来额proess_id

如果两个process_id相同，说明是一个进程先加锁，然后又过来尝试加锁，可能是要执行另外一个操作，此时就不会block，对同一个process_id是不会block，ctx.op= 'noop'，什么都不做，返回一个success

如果说已经有一个进程加了锁了
    
    /fs/lock/1
    {
      "process_id": 123
    }
    
    POST /fs/lock/1/_update
    {
      "upsert": { "process_id": 123 },
      "script": "if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = 'noop';"
      "params": {
        "process_id": 234
      }
    }
    
    "script": "if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = 'noop';"
    
    ctx._source.process_id：123
    process_id: 234
    
    process_id不相等，说明这个doc之前已经被别人上锁了，process_id=123上锁了; process_id=234过来再次尝试上锁，失败，assert false，就会报错

此时遇到报错的process，就应该尝试重新上锁，直到上锁成功

有报错的话，如果有些doc被锁了，那么需要重试

直到所有锁定都成功，执行自己的操作。。。

释放所有的锁

2、上document锁的完整实验过程

    scripts/judge-lock.groovy: if ( ctx._source.process_id != process_id ) { assert false }; ctx.op = 'noop';
    
    POST /fs/lock/1/_update
    {
      "upsert": { "process_id": 123 },
      "script": {
        "lang": "groovy",
        "file": "judge-lock", 
        "params": {
          "process_id": 123
        }
      }
    }
    
    {
      "_index": "fs",
      "_type": "lock",
      "_id": "1",
      "_version": 1,
      "result": "created",
      "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
      }
    }
    
    GET /fs/lock/1
    
    {
      "_index": "fs",
      "_type": "lock",
      "_id": "1",
      "_version": 1,
      "found": true,
      "_source": {
        "process_id": 123
      }
    }
    
    POST /fs/lock/1/_update
    {
      "upsert": { "process_id": 234 },
      "script": {
        "lang": "groovy",
        "file": "judge-lock", 
        "params": {
          "process_id": 234
        }
      }
    }
    
    {
      "error": {
        "root_cause": [
          {
            "type": "remote_transport_exception",
            "reason": "[4onsTYV][127.0.0.1:9300][indices:data/write/update[s]]"
          }
        ],
        "type": "illegal_argument_exception",
        "reason": "failed to execute script",
        "caused_by": {
          "type": "script_exception",
          "reason": "error evaluating judge-lock",
          "caused_by": {
            "type": "power_assertion_error",
            "reason": "assert false\n"
          },
          "script_stack": [],
          "script": "",
          "lang": "groovy"
        }
      },
      "status": 400
    }
    
    POST /fs/lock/1/_update
    {
      "upsert": { "process_id": 123 },
      "script": {
        "lang": "groovy",
        "file": "judge-lock", 
        "params": {
          "process_id": 123
        }
      }
    }
    
    {
      "_index": "fs",
      "_type": "lock",
      "_id": "1",
      "_version": 1,
      "result": "noop",
      "_shards": {
        "total": 0,
        "successful": 0,
        "failed": 0
      }
    }
    
    POST /fs/file/1/_update
    {
      "doc": {
        "name": "README1.txt"
      }
    }
    
    {
      "_index": "fs",
      "_type": "file",
      "_id": "1",
      "_version": 4,
      "result": "updated",
      "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
      }
    }
    
    POST /fs/_refresh 
    
    GET /fs/lock/_search?scroll=1m
    {
      "query": {
        "term": {
          "process_id": 123
        }
      }
    }
    
    {
      "_scroll_id": "DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACPkFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAj5RY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAAI-YWNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAACPnFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAj6BY0b25zVFlWWlRqR3ZJajlfc3BXejJ3",
      "took": 51,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 1,
        "max_score": 1,
        "hits": [
          {
            "_index": "fs",
            "_type": "lock",
            "_id": "1",
            "_score": 1,
            "_source": {
              "process_id": 123
            }
          }
        ]
      }
    }
    
    PUT /fs/lock/_bulk
    { "delete": { "_id": 1}}
    
    {
      "took": 20,
      "errors": false,
      "items": [
        {
          "delete": {
            "found": true,
            "_index": "fs",
            "_type": "lock",
            "_id": "1",
            "_version": 2,
            "result": "deleted",
            "_shards": {
              "total": 2,
              "successful": 1,
              "failed": 0
            },
            "status": 200
          }
        }
      ]
    }
    
    POST /fs/lock/1/_update
    {
      "upsert": { "process_id": 234 },
      "script": {
        "lang": "groovy",
        "file": "judge-lock", 
        "params": {
          "process_id": 234
        }
      }
    }

process_id=234上锁就成功了



### 基于共享锁和排他锁实现悲观锁并发控制



1、共享锁和排他锁的说明

共享锁：这份数据是共享的，然后多个线程过来，都可以获取同一个数据的共享锁，然后对这个数据执行读操作

排他锁：是排他的操作，只能一个线程获取排他锁，然后执行增删改操作

读写锁的分离

如果只是要读取数据的话，那么任意个线程都可以同时进来然后读取数据，每个线程都可以上一个共享锁

但是这个时候，如果有线程要过来修改数据，那么会尝试上排他锁，排他锁会跟共享锁互斥，也就是说，如果有人已经上了共享锁了，那么排他锁就不能上，就得等

如果有人在读数据，就不允许别人来修改数据

反之，也是一样的

如果有人在修改数据，就是加了排他锁
那么其他线程过来要修改数据，也会尝试加排他锁，此时会失败，锁冲突，必须等待，同时只能有一个线程修改数据

如果有人过来同时要读取数据，那么会尝试加共享锁，此时会失败，因为共享锁和排他锁是冲突的

如果有在修改数据，就不允许别人来修改数据，也不允许别人来读取数据

2、共享锁和排他锁的实验

第一步：有人在读数据，其他人也能过来读数据

    judge-lock-2.groovy: if (ctx._source.lock_type == 'exclusive') { assert false }; ctx._source.lock_count++
    
    POST /fs/lock/1/_update 
    {
      "upsert": { 
        "lock_type":  "shared",
        "lock_count": 1
      },
      "script": {
      	"lang": "groovy",
      	"file": "judge-lock-2"
      }
    }
    
    POST /fs/lock/1/_update 
    {
      "upsert": { 
        "lock_type":  "shared",
        "lock_count": 1
      },
      "script": {
      	"lang": "groovy",
      	"file": "judge-lock-2"
      }
    }
    
    GET /fs/lock/1
    
    {
      "_index": "fs",
      "_type": "lock",
      "_id": "1",
      "_version": 3,
      "found": true,
      "_source": {
        "lock_type": "shared",
        "lock_count": 3
      }
    }

就给大家模拟了，有人上了共享锁，你还是要上共享锁，直接上就行了，没问题，只是lock_count加1

2、已经有人上了共享锁，然后有人要上排他锁

    PUT /fs/lock/1/_create
    { "lock_type": "exclusive" }

排他锁用的不是upsert语法，create语法，要求lock必须不能存在，直接自己是第一个上锁的人，上的是排他锁

    {
      "error": {
        "root_cause": [
          {
            "type": "version_conflict_engine_exception",
            "reason": "[lock][1]: version conflict, document already exists (current version [3])",
            "index_uuid": "IYbj0OLGQHmMUpLfbhD4Hw",
            "shard": "3",
            "index": "fs"
          }
        ],
        "type": "version_conflict_engine_exception",
        "reason": "[lock][1]: version conflict, document already exists (current version [3])",
        "index_uuid": "IYbj0OLGQHmMUpLfbhD4Hw",
        "shard": "3",
        "index": "fs"
      },
      "status": 409
    }

如果已经有人上了共享锁，明显/fs/lock/1是存在的，create语法去上排他锁，肯定会报错

3、对共享锁进行解锁

    POST /fs/lock/1/_update
    {
      "script": {
      	"lang": "groovy",
      	"file": "unlock-shared"
      }
    }

连续解锁3次，此时共享锁就彻底没了

每次解锁一个共享锁，就对lock_count先减1，如果减了1之后，是0，那么说明所有的共享锁都解锁完了，此时就就将/fs/lock/1删除，就彻底解锁所有的共享锁

3、上排他锁，再上排他锁

    PUT /fs/lock/1/_create
    { "lock_type": "exclusive" }

其他线程

    PUT /fs/lock/1/_create
    { "lock_type": "exclusive" }
    
    {
      "error": {
        "root_cause": [
          {
            "type": "version_conflict_engine_exception",
            "reason": "[lock][1]: version conflict, document already exists (current version [7])",
            "index_uuid": "IYbj0OLGQHmMUpLfbhD4Hw",
            "shard": "3",
            "index": "fs"
          }
        ],
        "type": "version_conflict_engine_exception",
        "reason": "[lock][1]: version conflict, document already exists (current version [7])",
        "index_uuid": "IYbj0OLGQHmMUpLfbhD4Hw",
        "shard": "3",
        "index": "fs"
      },
      "status": 409
    }

4、上排他锁，上共享锁

    POST /fs/lock/1/_update 
    {
      "upsert": { 
        "lock_type":  "shared",
        "lock_count": 1
      },
      "script": {
      	"lang": "groovy",
      	"file": "judge-lock-2"
      }
    }
    
    {
      "error": {
        "root_cause": [
          {
            "type": "remote_transport_exception",
            "reason": "[4onsTYV][127.0.0.1:9300][indices:data/write/update[s]]"
          }
        ],
        "type": "illegal_argument_exception",
        "reason": "failed to execute script",
        "caused_by": {
          "type": "script_exception",
          "reason": "error evaluating judge-lock-2",
          "caused_by": {
            "type": "power_assertion_error",
            "reason": "assert false\n"
          },
          "script_stack": [],
          "script": "",
          "lang": "groovy"
        }
      },
      "status": 400
    }

5、解锁排他锁
    
    DELETE /fs/lock/1




### 基于nested object实现博客与评论嵌套关系

1、做一个实验，引出来为什么需要nested object

冗余数据方式的来建模，其实用的就是object类型，我们这里又要引入一种新的object类型，nested object类型

博客，评论，做的这种数据模型

    PUT /website/blogs/6
    {
      "title": "花无缺发表的一篇帖子",
      "content":  "我是花无缺，大家要不要考虑一下投资房产和买股票的事情啊。。。",
      "tags":  [ "投资", "理财" ],
      "comments": [ 
        {
          "name":    "小鱼儿",
          "comment": "什么股票啊？推荐一下呗",
          "age":     28,
          "stars":   4,
          "date":    "2016-09-01"
        },
        {
          "name":    "黄药师",
          "comment": "我喜欢投资房产，风，险大收益也大",
          "age":     31,
          "stars":   5,
          "date":    "2016-10-22"
        }
      ]
    }

被年龄是28岁的黄药师评论过的博客，搜索

    GET /website/blogs/_search
    {
      "query": {
        "bool": {
          "must": [
            { "match": { "comments.name": "黄药师" }},
            { "match": { "comments.age":  28      }} 
          ]
        }
      }
    }
    
    {
      "took": 102,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 1,
        "max_score": 1.8022683,
        "hits": [
          {
            "_index": "website",
            "_type": "blogs",
            "_id": "6",
            "_score": 1.8022683,
            "_source": {
              "title": "花无缺发表的一篇帖子",
              "content": "我是花无缺，大家要不要考虑一下投资房产和买股票的事情啊。。。",
              "tags": [
                "投资",
                "理财"
              ],
              "comments": [
                {
                  "name": "小鱼儿",
                  "comment": "什么股票啊？推荐一下呗",
                  "age": 28,
                  "stars": 4,
                  "date": "2016-09-01"
                },
                {
                  "name": "黄药师",
                  "comment": "我喜欢投资房产，风，险大收益也大",
                  "age": 31,
                  "stars": 5,
                  "date": "2016-10-22"
                }
              ]
            }
          }
        ]
      }
    }

结果是。。。好像不太对啊？？？

object类型数据结构的底层存储。。。
    
    {
      "title":            [ "花无缺", "发表", "一篇", "帖子" ],
      "content":             [ "我", "是", "花无缺", "大家", "要不要", "考虑", "一下", "投资", "房产", "买", "股票", "事情" ],
      "tags":             [ "投资", "理财" ],
      "comments.name":    [ "小鱼儿", "黄药师" ],
      "comments.comment": [ "什么", "股票", "推荐", "我", "喜欢", "投资", "房产", "风险", "收益", "大" ],
      "comments.age":     [ 28, 31 ],
      "comments.stars":   [ 4, 5 ],
      "comments.date":    [ 2016-09-01, 2016-10-22 ]
    }

object类型底层数据结构，会将一个json数组中的数据，进行扁平化

所以，直接命中了这个document，name=黄药师，age=28，正好符合

2、引入nested object类型，来解决object类型底层数据结构导致的问题

修改mapping，将comments的类型从object设置为nested

    PUT /website
    {
      "mappings": {
        "blogs": {
          "properties": {
            "comments": {
              "type": "nested", 
              "properties": {
                "name":    { "type": "string"  },
                "comment": { "type": "string"  },
                "age":     { "type": "short"   },
                "stars":   { "type": "short"   },
                "date":    { "type": "date"    }
              }
            }
          }
        }
      }
    }
    
    { 
      "comments.name":    [ "小鱼儿" ],
      "comments.comment": [ "什么", "股票", "推荐" ],
      "comments.age":     [ 28 ],
      "comments.stars":   [ 4 ],
      "comments.date":    [ 2014-09-01 ]
    }
    { 
      "comments.name":    [ "黄药师" ],
      "comments.comment": [ "我", "喜欢", "投资", "房产", "风险", "收益", "大" ],
      "comments.age":     [ 31 ],
      "comments.stars":   [ 5 ],
      "comments.date":    [ 2014-10-22 ]
    }
    { 
      "title":            [ "花无缺", "发表", "一篇", "帖子" ],
      "body":             [ "我", "是", "花无缺", "大家", "要不要", "考虑", "一下", "投资", "房产", "买", "股票", "事情" ],
      "tags":             [ "投资", "理财" ]
    }

再次搜索，成功了。。。

    GET /website/blogs/_search 
    {
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "title": "花无缺"
              }
            },
            {
              "nested": {
                "path": "comments",
                "query": {
                  "bool": {
                    "must": [
                      {
                        "match": {
                          "comments.name": "黄药师"
                        }
                      },
                      {
                        "match": {
                          "comments.age": 28
                        }
                      }
                    ]
                  }
                }
              }
            }
          ]
        }
      }
    }

score_mode：max，min，avg，none，默认是avg

如果搜索命中了多个nested document，如何讲个多个nested document的分数合并为一个分数



### 对嵌套的博客评论数据进行聚合分析

我们讲解一下基于nested object中的数据进行聚合分析

聚合数据分析的需求1：按照评论日期进行bucket划分，然后拿到每个月的评论的评分的平均值

    GET /website/blogs/_search 
    {
      "size": 0, 
      "aggs": {
        "comments_path": {
          "nested": {
            "path": "comments"
          }, 
          "aggs": {
            "group_by_comments_date": {
              "date_histogram": {
                "field": "comments.date",
                "interval": "month",
                "format": "yyyy-MM"
              },
              "aggs": {
                "avg_stars": {
                  "avg": {
                    "field": "comments.stars"
                  }
                }
              }
            }
          }
        }
      }
    }
    
    {
      "took": 52,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 2,
        "max_score": 0,
        "hits": []
      },
      "aggregations": {
        "comments_path": {
          "doc_count": 4,
          "group_by_comments_date": {
            "buckets": [
              {
                "key_as_string": "2016-08",
                "key": 1470009600000,
                "doc_count": 1,
                "avg_stars": {
                  "value": 3
                }
              },
              {
                "key_as_string": "2016-09",
                "key": 1472688000000,
                "doc_count": 2,
                "avg_stars": {
                  "value": 4.5
                }
              },
              {
                "key_as_string": "2016-10",
                "key": 1475280000000,
                "doc_count": 1,
                "avg_stars": {
                  "value": 5
                }
              }
            ]
          }
        }
      }
    }
    
    
    
    GET /website/blogs/_search 
    {
      "size": 0,
      "aggs": {
        "comments_path": {
          "nested": {
            "path": "comments"
          },
          "aggs": {
            "group_by_comments_age": {
              "histogram": {
                "field": "comments.age",
                "interval": 10
              },
              "aggs": {
                "reverse_path": {
                  "reverse_nested": {}, 
                  "aggs": {
                    "group_by_tags": {
                      "terms": {
                        "field": "tags.keyword"
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
    
    {
      "took": 5,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 2,
        "max_score": 0,
        "hits": []
      },
      "aggregations": {
        "comments_path": {
          "doc_count": 4,
          "group_by_comments_age": {
            "buckets": [
              {
                "key": 20,
                "doc_count": 1,
                "reverse_path": {
                  "doc_count": 1,
                  "group_by_tags": {
                    "doc_count_error_upper_bound": 0,
                    "sum_other_doc_count": 0,
                    "buckets": [
                      {
                        "key": "投资",
                        "doc_count": 1
                      },
                      {
                        "key": "理财",
                        "doc_count": 1
                      }
                    ]
                  }
                }
              },
              {
                "key": 30,
                "doc_count": 3,
                "reverse_path": {
                  "doc_count": 2,
                  "group_by_tags": {
                    "doc_count_error_upper_bound": 0,
                    "sum_other_doc_count": 0,
                    "buckets": [
                      {
                        "key": "大侠",
                        "doc_count": 1
                      },
                      {
                        "key": "投资",
                        "doc_count": 1
                      },
                      {
                        "key": "理财",
                        "doc_count": 1
                      },
                      {
                        "key": "练功",
                        "doc_count": 1
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    }


### 研发中心管理案例以及父子关系数据建模

nested object的建模，有个不好的地方，就是采取的是类似冗余数据的方式，将多个数据都放在一起了，维护成本就比较高

parent child建模方式，采取的是类似于关系型数据库的三范式类的建模，多个实体都分割开来，每个实体之间都通过一些关联方式，进行了父子关系的关联，各种数据不需要都放在一起，父doc和子doc分别在进行更新的时候，都不会影响对方

一对多关系的建模，维护起来比较方便，而且我们之前说过，类似关系型数据库的建模方式，应用层join的方式，会导致性能比较差，因为做多次搜索。父子关系的数据模型，不会，性能很好。因为虽然数据实体之间分割开来，但是我们在搜索的时候，由es自动为我们处理底层的关联关系，并且通过一些手段保证搜索性能。

父子关系数据模型，相对于nested数据模型来说，优点是父doc和子doc互相之间不会影响

要点：父子关系元数据映射，用于确保查询时候的高性能，但是有一个限制，就是父子数据必须存在于一个shard中

父子关系数据存在一个shard中，而且还有映射其关联关系的元数据，那么搜索父子关系数据的时候，不用跨分片，一个分片本地自己就搞定了，性能当然高咯

案例背景：研发中心员工管理案例，一个IT公司有多个研发中心，每个研发中心有多个员工

    PUT /company
    {
      "mappings": {
        "rd_center": {},
        "employee": {
          "_parent": {
            "type": "rd_center" 
          }
        }
      }
    }

父子关系建模的核心，多个type之间有父子关系，用_parent指定父type

    POST /company/rd_center/_bulk
    { "index": { "_id": "1" }}
    { "name": "北京研发总部", "city": "北京", "country": "中国" }
    { "index": { "_id": "2" }}
    { "name": "上海研发中心", "city": "上海", "country": "中国" }
    { "index": { "_id": "3" }}
    { "name": "硅谷人工智能实验室", "city": "硅谷", "country": "美国" }

shard路由的时候，id=1的rd_center doc，默认会根据id进行路由，到某一个shard

    PUT /company/employee/1?parent=1 
    {
      "name":  "张三",
      "birthday":   "1970-10-24",
      "hobby": "爬山"
    }

维护父子关系的核心，parent=1，指定了这个数据的父doc的id

此时，parent-child关系，就确保了说，父doc和子doc都是保存在一个shard上的。内部原理还是doc routing，employee和rd_center的数据，都会用parent id作为routing，这样就会到一个shard

就不会根据id=1的employee doc的id进行路由了，而是根据parent=1进行路由，会根据父doc的id进行路由，那么就可以通过底层的路由机制，保证父子数据存在于一个shard中
    
    POST /company/employee/_bulk
    { "index": { "_id": 2, "parent": "1" }}
    { "name": "李四", "birthday": "1982-05-16", "hobby": "游泳" }
    { "index": { "_id": 3, "parent": "2" }}
    { "name": "王二", "birthday": "1979-04-01", "hobby": "爬山" }
    { "index": { "_id": 4, "parent": "3" }}
    { "name": "赵五", "birthday": "1987-05-11", "hobby": "骑马" }



### 根据员工信息和研发中心互相搜索父子数据


我们已经建立了父子关系的数据模型之后，就要基于这个模型进行各种搜索和聚合了

1、搜索有1980年以后出生的员工的研发中心

    GET /company/rd_center/_search
    {
      "query": {
        "has_child": {
          "type": "employee",
          "query": {
            "range": {
              "birthday": {
                "gte": "1980-01-01"
              }
            }
          }
        }
      }
    }
    
    {
      "took": 33,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 2,
        "max_score": 1,
        "hits": [
          {
            "_index": "company",
            "_type": "rd_center",
            "_id": "1",
            "_score": 1,
            "_source": {
              "name": "北京研发总部",
              "city": "北京",
              "country": "中国"
            }
          },
          {
            "_index": "company",
            "_type": "rd_center",
            "_id": "3",
            "_score": 1,
            "_source": {
              "name": "硅谷人工智能实验室",
              "city": "硅谷",
              "country": "美国"
            }
          }
        ]
      }
    }

2、搜索有名叫张三的员工的研发中心

    GET /company/rd_center/_search
    {
      "query": {
        "has_child": {
          "type":       "employee",
          "query": {
            "match": {
              "name": "张三"
            }
          }
        }
      }
    }
    
    {
      "took": 2,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 1,
        "max_score": 1,
        "hits": [
          {
            "_index": "company",
            "_type": "rd_center",
            "_id": "1",
            "_score": 1,
            "_source": {
              "name": "北京研发总部",
              "city": "北京",
              "country": "中国"
            }
          }
        ]
      }
    }

3、搜索有至少2个以上员工的研发中心

    GET /company/rd_center/_search
    {
      "query": {
        "has_child": {
          "type":         "employee",
          "min_children": 2, 
          "query": {
            "match_all": {}
          }
        }
      }
    }
    
    {
      "took": 5,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 1,
        "max_score": 1,
        "hits": [
          {
            "_index": "company",
            "_type": "rd_center",
            "_id": "1",
            "_score": 1,
            "_source": {
              "name": "北京研发总部",
              "city": "北京",
              "country": "中国"
            }
          }
        ]
      }
    }

4、搜索在中国的研发中心的员工
    
    GET /company/employee/_search 
    {
      "query": {
        "has_parent": {
          "parent_type": "rd_center",
          "query": {
            "term": {
              "country.keyword": "中国"
            }
          }
        }
      }
    }
    
    {
      "took": 5,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 3,
        "max_score": 1,
        "hits": [
          {
            "_index": "company",
            "_type": "employee",
            "_id": "3",
            "_score": 1,
            "_routing": "2",
            "_parent": "2",
            "_source": {
              "name": "王二",
              "birthday": "1979-04-01",
              "hobby": "爬山"
            }
          },
          {
            "_index": "company",
            "_type": "employee",
            "_id": "1",
            "_score": 1,
            "_routing": "1",
            "_parent": "1",
            "_source": {
              "name": "张三",
              "birthday": "1970-10-24",
              "hobby": "爬山"
            }
          },
          {
            "_index": "company",
            "_type": "employee",
            "_id": "2",
            "_score": 1,
            "_routing": "1",
            "_parent": "1",
            "_source": {
              "name": "李四",
              "birthday": "1982-05-16",
              "hobby": "游泳"
            }
          }
        ]
      }
    }
    
    
    
### 对每个国家的员工兴趣爱好进行聚合统计


统计每个国家的喜欢每种爱好的员工有多少个

    GET /company/rd_center/_search 
    {
      "size": 0,
      "aggs": {
        "group_by_country": {
          "terms": {
            "field": "country.keyword"
          },
          "aggs": {
            "group_by_child_employee": {
              "children": {
                "type": "employee"
              },
              "aggs": {
                "group_by_hobby": {
                  "terms": {
                    "field": "hobby.keyword"
                  }
                }
              }
            }
          }
        }
      }
    }
    
    {
      "took": 15,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 3,
        "max_score": 0,
        "hits": []
      },
      "aggregations": {
        "group_by_country": {
          "doc_count_error_upper_bound": 0,
          "sum_other_doc_count": 0,
          "buckets": [
            {
              "key": "中国",
              "doc_count": 2,
              "group_by_child_employee": {
                "doc_count": 3,
                "group_by_hobby": {
                  "doc_count_error_upper_bound": 0,
                  "sum_other_doc_count": 0,
                  "buckets": [
                    {
                      "key": "爬山",
                      "doc_count": 2
                    },
                    {
                      "key": "游泳",
                      "doc_count": 1
                    }
                  ]
                }
              }
            },
            {
              "key": "美国",
              "doc_count": 1,
              "group_by_child_employee": {
                "doc_count": 1,
                "group_by_hobby": {
                  "doc_count_error_upper_bound": 0,
                  "sum_other_doc_count": 0,
                  "buckets": [
                    {
                      "key": "骑马",
                      "doc_count": 1
                    }
                  ]
                }
              }
            }
          ]
        }
      }
    }

### 祖孙三层数据关系建模以及搜索实战

父子关系，祖孙三层关系的数据建模，搜索

    PUT /company
    {
      "mappings": {
        "country": {},
        "rd_center": {
          "_parent": {
            "type": "country" 
          }
        },
        "employee": {
          "_parent": {
            "type": "rd_center" 
          }
        }
      }
    }

country -> rd_center -> employee，祖孙三层数据模型

    POST /company/country/_bulk
    { "index": { "_id": "1" }}
    { "name": "中国" }
    { "index": { "_id": "2" }}
    { "name": "美国" }
    
    POST /company/rd_center/_bulk
    { "index": { "_id": "1", "parent": "1" }}
    { "name": "北京研发总部" }
    { "index": { "_id": "2", "parent": "1" }}
    { "name": "上海研发中心" }
    { "index": { "_id": "3", "parent": "2" }}
    { "name": "硅谷人工智能实验室" }
    
    PUT /company/employee/1?parent=1&routing=1
    {
      "name":  "张三",
      "dob":   "1970-10-24",
      "hobby": "爬山"
    }

routing参数的讲解，必须跟grandparent相同，否则有问题

country，用的是自己的id去路由;

rd_center，parent，用的是country的id去路由;

employee，如果也是仅仅指定一个parent，那么用的是rd_center的id去路由，这就导致祖孙三层数据不会在一个shard上

孙子辈儿，要手动指定routing，指定为爷爷辈儿的数据的id

搜索有爬山爱好的员工所在的国家

    GET /company/country/_search
    {
      "query": {
        "has_child": {
          "type": "rd_center",
          "query": {
            "has_child": {
              "type": "employee",
              "query": {
                "match": {
                  "hobby": "爬山"
                }
              }
            }
          }
        }
      }
    }
    
    {
      "took": 10,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 1,
        "max_score": 1,
        "hits": [
          {
            "_index": "company",
            "_type": "country",
            "_id": "1",
            "_score": 1,
            "_source": {
              "name": "中国"
            }
          }
        ]
      }
    }



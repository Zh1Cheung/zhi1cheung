---
title: RPC(2)
categories:
- RPC
tags:
- RPC
---


## RPC的通信流程

- RPC 的作用就是体现在这样两个方面
  - 屏蔽远程调用跟本地调用的区别，让我们感觉就是调用项目内的方法；
  - 隐藏底层网络通信的复杂性，让我们更专注于业务逻辑
- 那一个完整的 RPC 会涉及到哪些步骤
  -  RPC 是一个远程调用，那肯定就需要通过网络来传输数据，并且 RPC 常用于业务系统之间的数据交互，需要保证其可靠性，所以 RPC 一般默认采用 TCP 来传输。我们常用的 HTTP 协议也是建立在 TCP 之上的。 
  - 网络传输的数据必须是二进制数据，但调用方请求的出入参数都是对象。对象是肯定没法直 接在网络中传输的，需要提前把它转成可传输的二进制，并且要求转换算法是可逆的，这个过程我们一般叫做“序列化”。
  - 服务提供方从 TCP 通道里面收到二进制数据，那如何知道一个请求的数据到哪里结束，是一个什么类型的请求呢？
    - 把数据格式的约定内容叫做“协议”。
    - 大多数的协议会分成两部分，分别是数据头和消息体。
  - 反序列化
  - 服务提供方再根据反序列化出来的请求对象找到对应的实现类，完成真正的方法调用，然后 把执行结果序列化后，回写到对应的 TCP 通道里面。调用方获取到应答的数据包后，再反序列化成应答对象，这样调用方就完成了一次 RPC 调用。
- 有什么办法来简化 API，屏蔽掉 RPC 细节，让使用方只需要关注业务接口，像调用本地一样来调用远程呢
  - 由服务提供者给出业务接口声明，在调用方的程序里面，RPC 框架根据调用的服务接口提前生成动态代理实现类，并通过依赖注入等技术注入到声明了该接口的相关业务逻辑里面。 该代理实现类会拦截所有的方法调用，在提供的方法处理逻辑里面完成一整套的远程调用， 并把远程调用结果返回给调用方，这样调用方在调用远程方法的时候就获得了像调用本地接口一样的体验









## **协议：怎么设计可扩展且向后兼容的协议**

- HTTP 协议跟 RPC 协议有一个共性就是都属于应用层协议。 

  - RPC 不直接用 HTTP 协议的一个原因是无法实现请求跟响应关联，每次请求都需要重新建立连接，响应完成后再关闭连接，所以我们要设计私有协议。
    - rpc为了吞吐量，会异步并发发送请求，等待应答，所以需要知道哪个应答对应那个请求 
    - 调用方需要维护消息ID列表，然后和返回结果中的消息ID做匹配
  - 在 RPC 里面，协议的作用就类似于文字中的符号，作为应用拆解请求消息的边界，保证二进制数据经过网络传输后，还能被正确地还原语义
  - 举个具体例子，调用方发送 AB、CD、EF 3 个消息，如果没有边界的话，接收端就可能收到 ABCDEF 或者ABC、DEF 这样的消息，这就会导致接收的语义跟发送的时候不一致了。
  - 所以呢，为了避免语义不一致的事情发生，我们就需要在发送请求的时候设定一个边界，然 后在收到请求的时候按照这个设定的边界进行数据分割。这个边界语义的表达，就是我们所说的协议。 

- 相对于 HTTP 的用处，RPC 更多的是负责应用间的通信，所以性能要求相对更高

  - 在协议里面需要放哪些内容

    - 在协议头里面，我们除了会放协议长度、序列化方式，还会放一些像协议标示、消息 ID、消息类型这样的参数，而协议体一般只放请求接口方法、请求的业务参数值和一些扩展属性。

  - 为了保证能平滑地升级改造前后的协议，我们有必要设计一种支持可扩展的协议

    - 整体协议就变成了三部分内容：固定部分、协议头内容、协议体内容，前两部分我们还是可以统称为“协议头”

      







## **序列化：对象怎么在网络中传输**

- 序列化过程就是在读取对象数据的时候，不断加入一些特殊分隔符，这些特殊分隔符用于在反序列化过程中截断用。
- Protobuf
  - 混合语言数据标准，是一种轻便、高效的结构化数据存储格式，可以用于结构化数据序列化
  - 优点
    - 序列化后体积相比 JSON、Hessian 小很多； 
    - IDL 能清晰地描述语义，所以足以帮助并保证应用程序之间的类型不会丢失，无需类似XML 解析器； 
    - 序列化反序列化速度很快，不需要通过反射获取类型； 
    - 消息格式升级和兼容性不错，可以做到向后兼容
- RPC 框架中如何选择序列化
  - 性能和效率
  - 空间开销
  - 序列化协议的通用性和兼容性
  - 安全性
- RPC 框架在使用时要注意哪些问题
  - 对象构造得过于复杂
  - 对象过于庞大
  - 使用序列化框架不支持的类作为入参类
  - 对象有复杂的继承关系







##  网络通信：RPC框架在网络通信上更倾向于哪种网络IO模型

- 常见的网络IO模型
  - 阻塞 IO（blocking IO）
    - 首先，应用进程发起 IO 系统调用后，应用进程被阻塞，转到内核空间处理。之后，内核开始等待数据，等待到数据之后，再将内核中的数据拷贝到用户内存中，整个 IO 处理完毕后返回进程。最后应用的进程解除阻塞状态，运行业务逻辑。 
  - IO 多路复用（IO multiplexing）
    - 多个网络连接的 IO 可以注册到一个复用器（select）上，当用户进程调用了 select，那么整个进程会被阻塞。同时，内核会“监视”所有 select 负责的 socket，当任何一个 socket 中的数据准备好了，select 就会返回。这个时候用户进程再调用 read 操作，将数据从内核中拷贝到用户进程。
    - 最大的优势在于，用户可以在一个线程内同时处理多个 socket 的 IO 请求。

- 为什么说阻塞 IO 和IO多路复用最为常用？

  - 实际在网络 IO 的应用上，需要的是系统内核的支持以及编程语言的支持。

- RPC框架在网络通信上倾向选择哪种网络IO模型

  - IO 多路复用更适合高并发的场景
  - RPC 调用在大多数的情况下，是一个高并发调用的场景

- 零拷贝

  - 应用进程的每一次写操作，都会把数据写到用户空间的缓冲区中，再由 CPU 将数据拷贝到系统内核的缓冲区中，之后再由 DMA 将这份数据拷贝到网卡中，最后由网卡发送出去。 这里我们可以看到，一次写操作数据要拷贝两次才能通过网卡发送出去，而用户进程的读操作则是将整个流程反过来，数据同样会拷贝两次才能让应用程序读取到数据
    - 每一次拷贝，都需要 CPU 进行一次上下文切换（由用户进程切换到系统内核，或由系统内核切换到用户进程）
  - 所谓的零拷贝，就是取消用户空间与内核空间之间的数据拷贝操作
    - 零拷贝有两种解决方式，分别是 mmap+write 方式和 sendfile 方式，其核心原理都是通过虚拟内存来解决的

  





## 动态代理：面向接口编程，屏蔽RPC处理流程

- RPC 会自动给接口生成一个代理类，当我们在项目中注入接口的时候，运行过程中实际绑定的是这个接口生成的代理类。在生成的代理类里面，加入远程调用逻辑
- 通过 Javassist 生成字节码，不 需要通过反射完成方法调用，所以性能肯定是更胜一筹的





## **设计一个灵活的**RPC框架

- RPC 框架就包含了两大核心体系——核心功能体系与插件体系
  - 入口层
    - 动态代理
    - 链路追踪
    - 过滤链
  - 集群层
    - 服务发现
    - 连接管理
    - 负载均衡
    - 路由
    - 容错
    - 配置管理
  - 协议层
    - 协议
    - 序列化
    - 解压缩
  - 传输层
    - TCP传输
    - HTTP传输







## 服务发现：到底是要CP还是AP

- 服务发现的作用就是 实时感知集群 IP 的变化，实现接口跟服务集群节点 IP 的映射。
- 基于消息总线的最终一致性的注册中心
  - 注册数据可以全量缓存在每个注册中心内存中，通过消息总线来同步数据。当有一个注册中心节点接收到服务节点注册时，会 产生一个消息推送给消息总线，再通过消息总线通知给其它注册中心节点更新数据并进行服务下发，从而达到注册中心间数据最终一致性
  - 当有服务上线，注册中心节点收到注册请求，服务列表数据发生变化，会生成一个消息，推送给消息总线，每个消息都有整体递增的版本。
  - 消息总线会主动推送消息到各个注册中心，同时注册中心也会定时拉取消息。对于获取 到消息的在消息回放模块里面回放，只接受大于本地版本号的消息，小于本地版本号的消息直接丢弃，从而实现最终一致性
  - 消费者订阅可以从注册中心内存拿到指定接口的全部服务实例，并缓存到消费者的内存里面。
  - 采用推拉模式，消费者可以及时地拿到服务实例增量变化情况，并和内存中的缓存数据 进行合并。





## **健康检测：这个节点都挂了，为啥还要疯狂发请求**

- **让调用方实时感知到节点的状态变化**
  - 业内常用的检测方法就是用心跳机制
  - “连续”心跳失败次数必须到达某一个阈值，比如 3 次
- 节点的心跳日志只是间歇性失败，也就是时好时坏
  - 第一，像前面说的那样，调用方跟服务节点之间网络状况瞬息万变，出现网络波动的时候会导致误判。第二，在负载高情况，服务端来不及处理心跳请求，由于心跳时间很短，会导致调用方很快触发连续心跳失败而造成断开连接
  - 调用方每个接口的调用频次不一样，有的接口可能 1 秒内调用上百次，有的接口可能半个小时才会调用一次，所以我们不能把简单的把总失败的次数当作判断条件。
  - 服务的接口响应时间也是不一样的，有的接口可能 1ms，有的接口可能是 10s，所以我们也不能把 TPS 至来当作判断条件。 
- **可用率**
  - 某一个时间窗口内接口调用成功次数的百分比（成功次数 / 总调用次数）。当可用率低于某个比例就认为这个节点存在问题
- 误判：检测程序所在的机器和目标机器之间的网络可能还会出现故障
  - 把检测程序部署在多个机器里面，分布在不同的机架，甚至不同的机房。因为网络同时故障的概率非常低，所以只要任意一个检测程序实例访问目标机器正常，就可以说明该目标机器正常。





## **路由策略：怎么让请求按照设定的规则发到不同的节点上**

- 灰度发布功能作为 RPC 路由功能的一个典型应用场景，我们可以通过路由功能完成像定点调用、黑白名单等一些高级服务治理功能。在 RPC 里面，不管是哪种路由策略，其核心思想都是一样的，就是让请求按照我们设定的规则发送到目标节点上，从而实现流量隔离的效果。 
- 路由本质是节点分组、隔离流量
-  用路由策略实现过灰度发布、定点调用、并行开发的时候，隔离出不同联调环境等功能







## **负载均衡：节点负载差距这么大，为什么收到的流量还一样**

- 当我们的一个服务节点无法支撑现有的访问量时，我们会部署多个节点，组成一个集群，然后通过负载均衡，将请求分发给这个集群下的每个服务节点，从而达到多个服务节点共同分担请求压力的目的。 

- 传统负载均衡面临这样几个问题
  - 搭建负载均衡设备或 TCP/IP 四层代理，需要额外成本；
  - 请求流量都经过负载均衡设备，多经过一次网络传输，会额外浪费一些性能；
  - 负载均衡添加节点和摘除节点，一般都要手动添加
  - 我们在服务治理的时候，针对不同接口服务、服务的不同分组，我们的负载均衡策略是需要可配的，如果大家都经过这一个负载均衡设备，就不容易根据不同的场景来配置不同的负载均衡策略了。

- RPC 的负载均衡完全由 RPC 框架自身实现，RPC 的服务调用者会与“注册中心”下发的所 有服务节点建立长连接，在每次发起 RPC 调用时，服务调用者都会通过配置的负载均衡插件，自主选择一个服务节点，发起 RPC 调用请求。
- 如何设计自适应的负载均衡
  - 可以采用一种根据指标打分的策略
  - 然后我们可以配合随机权重的负载均衡策略去控制，通过最终的指标分数修改服务节点最终的权重。

- 以 Dubbo 为例，常用的负载均衡方法有：
  - 基于权重随机算法
  - 基于最少活跃调用数算法
  - 基于 hash 一致性
  - 基于加权轮询算法
- 路由策略使用的场景是流量隔离，比如在灰度验证过程。负载均衡使用的场景则是请求智能调度，尽可能保证选择一个最合适的节点来调用





## **异常重试：在约定时间内安全可靠地重试**

- 在使用 RPC 框架的时候，我们要确保被调用的服务的业务逻辑是幂等的，这样我们才能考虑根据事件情况开启 RPC 框架的异常重试功能
- 如何在约定时间内安全可靠地重试
  - 解决这个问题最直接的方式就是，在每次重试后都重置一下请求的超时时间。
  - 在所有发起重试、负载均衡选择节点的时候，去掉重试之前出现过问题的那个节点，以保证重试的成功率
  - RPC 框架是不会知道哪些业务异常能够去进行异常重试的，我们可以加个重试异常的白名单，用户可以将允许重试的异常加入到这个白名单中。
- 在整个 RPC 调用的流程中，异常重试发生在动态代理发起invoke，紧接着的一步的环节




---
title: Spark2.0新特性介绍
categories:
- Spark
- Big Data
tags:
- Spark



---


## 新特性介绍



### Spark Core&Spark SQL API

* dataframe与dataset统一，dataframe只是dataset[Row]的类型别名
* SparkSession：统一SQLContext和HiveContext，新的上下文入口
* 为SparkSession开发的一种新的流式调用的configuration api
* accumulator功能增强：便捷api、web ui支持、性能更高
* dataset的增强聚合api



### Spark Core&Spark SQL 

* 支持sql 2003标准
* 支持ansi-sql和hive ql的sql parser
* 支持ddl命令
* 支持子查询：in/not in、exists/not exists


### Spark Core&Spark SQL new feature

* 支持csv文件
* 支持缓存和程序运行的堆外内存管理
* 支持hive风格的bucket表
* 支持近似概要统计，包括近似分位数、布隆过滤器、最小略图


### Spark Core&Spark SQL性能
* 通过whole-stage code generation技术将spark sql和dataset的性能提升2~10倍
* 通过vectorization技术提升parquet文件的扫描吞吐量
* 提升orc文件的读写性能
* 提升catalyst查询优化器的性能
* 通过native实现方式提升窗口函数的性能
* 对某些数据源进行自动文件合并


### Spark MLlib


* spark mllib未来将主要基于dataset api来实现，基于rdd的api转为维护阶段
* 基于dataframe的api，支持持久化保存和加载模型和pipeline
* 基于dataframe的api，支持更多算法，包括二分kmeans、高斯混合、maxabsscaler等
* spark R支持mllib算法，包括线性回归、朴素贝叶斯、kmeans、多元回归等
* pyspark支持更多mllib算法，包括LDA、高斯混合、泛化线性回顾等
* 基于dataframe的api，向量和矩阵使用性能更高的序列化机制


### Spark Streaming

* 发布测试版的structured streaming
	* 基于spark sql和catalyst引擎构建
	* 支持使用dataframe风格的api进行流式计算操作
	* catalyst引擎能够对执行计划进行优化
* 基于dstream的api支持kafka 0.10版本










### 依赖管理、打包和操作

* 不再需要在生产环境部署时打包fat jar，可以使用provided风格
* 完全移除了对akka的依赖
* mesos粗粒度模式下，支持启动多个executor
* 支持kryo 3.0版本
* 使用scala 2.11替代了scala 2.10



### 移除的功能


* bagel模块
* 对hadoop 2.1以及之前版本的支持
* 闭包序列化配置的支持
* HTTPBroadcast支持
* 基于TTL模式的元数据清理支持
* 半私有的org.apache.spark.Logging的使用支持
* SparkContext.metricsSystem API
* 与tachyon的面向block的整合支持
* spark 1.x中标识为过期的所有api
* python dataframe中返回rdd的方法
* 使用很少的streaming数据源支持：twitter、akka、MQTT、ZeroMQ
* hash-based shuffle manager
* standalone master的历史数据支持功能
* dataframe不再是一个类，而是dataset[Row]的类型别名


### 变化的机制

* 要求基于scala 2.11版本进行开发，而不是scala 2.10版本
* SQL中的浮点类型，使用decimal类型来表示，而不是double类型
* kryo版本升级到了3.0
* java的flatMap和mapPartitions方法，从iterable类型转变为iterator类型
* java的countByKey返回<K,Long>类型，而不是<K,Object>类型
* 写parquet文件时，summary文件默认不会写了，需要开启参数来启用
* spark mllib中，基于dataframe的api完全依赖于自己，不再依赖mllib包


### 过期的API

* mesos的细粒度模式
* java 7支持标识为过期，可能2.x未来版本会移除支持
* python 2.6的支持


## 新特性介绍-易用性：标准化SQL支持以及更合理的API


Spark最引以为豪的几个特点就是简单、直观、表达性好。Spark 2.0为了继续加强这几个特点，做了两件事情：1、提供标准化的SQL支持；2、统一了Dataframe和Dataset两套API。

在标准化SQL支持方面，引入了新的ANSI-SQL解析器，提供标准化SQL的解析功能，而且还提供了子查询的支持。Spark现在可以运行完整的99个TPC-DS查询，这就要求Spark包含大多数SQL 2003标准的特性。这么做的好处在于，SQL一直是大数据应用领域的一个最广泛接受的标准，比如说Hadoop，做大数据的企业90%的时间都在用Hive，写SQL做各种大数据的统计和分析。因此Spark SQL提升对SQL的支持，可以大幅度减少用户将应用从其他技术（比如Oracle、Hive等）迁移过来的成本。


### 统一Dataframe和Dataset API

从Spark 2.0开始，Dataframe就只是Dataset[Row]的一个别名，不再是一个单独的类了。无论是typed方法（map、filter、groupByKey等）还是untyped方法（select、groupBy等），都通过Dataset来提供。而且Dataset API将成为Spark的新一代流式计算框架——structured streaming的底层计算引擎。但是由于Python和R这两个语言都不具备compile-time type-safety的特性，所以就没有引入Dataset API，所以这两种语言中的主要编程接口还是Dataframe。


### SparkSession

SparkSession是新的Spark上下文以及入口，用于合并SQLContext和HiveContext，并替代它们。因为以前提供了SQLContext和HiveContext两种上下文入口，因此用户有时会有些迷惑，到底该使用哪个接口。现在好了，只需要使用一个统一的SparkSession即可。但是为了向后兼容性，SQLContext和HiveContext还是保留下来了。

### 新版本Accumulator API

Spark 2.0提供了新版本的Accumulator，提供了各种方便的方法，比如说直接通过一个方法的调用，就可以创建各种primitive data type（原始数据类型，int、long、double）的Accumulator。并且在spark web ui上也支持查看spark application的accumulator，性能也得到了提升。老的Accumulator API还保留着，主要是为了向后兼容性。


### 基于Dataframe/Dataset的Spark MLlib

Spark 2.0中，spark.ml包下的机器学习API，主要是基于Dataframe/Dataset来实现的，未来将会成为主要发展的API接口。原先老的基于RDD的spark.mllib包的机器学习API还会保留着，为了向后兼容性，但是未来主要会基于spark.ml包下的接口来进行开发。而且用户使用基于Dataframe/Dataset的新API，还能够对算法模型和pipeline进行持久化保存以及加载。


### SparkR中的分布式机器学习算法以及UDF函数

Spark 2.0中，为SparkR提供了分布式的机器学习算法，包括经典的Generalized Linear Model，朴素贝叶斯，Survival Regression，K-means等。此外SparkR还支持用户自定义的函数，即UDF。



## 新特性介绍-高性能：让Spark作为编译器来运行


###  让Spark作为编译器来运行
在一个2015年的spark调查中显示，91%的spark用户是因为spark的高性能才选择使用它的。所以spark的性能优化也就是社区的一个重要的努力方向了。spark 1.x相较于hadoop mapreduce来说，速度已经快了数倍了，但是spark 2.x中，还能不能相较于spark 1.x来说，速度再提升10倍呢？

带着这个疑问，我们可以重新思考一下spark的物理执行机制。对于一个现代的大数据处理引擎来说，CPU的大部分时间都浪费在了一些无用的工作上，比如说virtual function call，或者从CPU缓冲区中读写数据。现代的编译器为了减少cpu浪费在上述工作的时间，付出了大量的努力。


Spark 2.0的一个重大的特点就是搭载了最新的第二代tungsten引擎。第二代tungsten引擎吸取了现代编译器以及并行数据库的一些重要的思想，并且应用在了spark的运行机制中。其中一个核心的思想，就是在运行时动态地生成代码，在这些自动动态生成的代码中，可以将所有的操作都打包到一个函数中，这样就可以避免多次virtual function call，而且还可以通过cpu register来读写中间数据，而不是通过cpu cache来读写数据。上述技术整体被称作“whole-stage code generation”，中文也可以叫“全流程代码生成”。

之前有人做过测试，用单个cpu core来处理一行数据，对比了spark 1.6和spark 2.0的性能。spark 2.0搭载的是whole-stage code generation技术，spark 1.6搭载的是第一代tungsten引擎的expression code generation技术。测试结果显示，spark 2.0的性能相较于spark 1.6得到了一个数量级的提升。

除了刚才那个简单的测试以外，还有人使用完整的99个SQL基准测试来测试过spark 1.6和spark 2.0的性能。测试结果同样显示，spark 2.0的性能比spark 1.6来说，提升了一个数量级。

spark 2.0中，除了whole-stage code generation技术以外，还使用了其他一些新技术来提升性能。比如说对Spark SQL的catalyst查询优化器做了一些性能优化，来提升对一些常见查询的优化效率，比如null值处理等。再比如说，通过vectarization技术将parquet文件扫描的吞吐量提升了3倍以上。



## 新特性介绍-智能化：Structured Streaming介绍

### Structured Streaming介绍


Spark Streaming应该说是将离线计算操作和流式计算操作统一起来的大数据计算框架之一。从Spark 0.7开始引入的Spark Streaming，为开发人员提供了很多有用的特性：一次且仅一次的语义支持、容错性、强一致性保证、高吞吐量。

但是实际上在真正工业界的流式计算项目中，并不仅仅只是需要一个流式计算引擎。这些项目实际上需要深度地使用批处理计算以及流式处理技术，与外部存储系统进行整合，还有应对业务逻辑变更的能力。因此，企业实际上不仅仅只是需要一个流式计算引擎，他们需要的是一个全栈式的技术，让他们能够开发end-to-end的持续计算应用（continuous application）。

Spark 2.0为了解决上述流式计算的痛点和需求，开发了新的模块——Structured Streaming。

Structured Streaming提供了与批处理计算类似的API。要开发一个流式计算应用，开发人员只要使用Dataframe/Dataset API编写与批处理计算一样的代码即可，Structured Streaming会自动将这些类似批处理的计算代码增量式地应用到持续不断进入的新数据上。这样，开发人员就不需要花太多时间考虑状态管理、容错、与离线计算的同步等问题。Structured Streaming可以保证，针对相同的数据，始终与离线计算产出完全一样的计算结果。

Structured Streaming还提供了与存储系统的事务整合。它会进行自动的容错管理以及数据一致性的管理，如果开发人员要写一个应用程序来更新数据库，进而提供一些实时数据服务，与静态数据进行join，或者是在多个存储系统之间移动数据，那么Structured Streaming可以让这些事情更加简单。

Structured Streaming与Spark其余的组件都能够进行完美的整合。比如可以通过Spark SQL对实时数据进行统计分析，与静态数据进行join，还有其他的使用dataframe/dataset的组件，这样就可以让开发人员构建完整的流式计算引用，而不仅仅只是一个流式计算引擎而已。在未来，Spark会将Structured Streaming与Spark MLlib的整合做的更好。


Spark 2.0搭载了一个beta版本的Structured Streaming，目前是作为Dataframe/Dataset的一个小的附加组件。主要是让Spark用户可以先尝试使用一下Structured Streaming，比如做一些实验和测试。Structured Streaming的一些关键特性，比如基于时间的处理，延迟数据的处理，交互式的查询，以及与非流式的数据源和存储进行整合，可能会基于未来的版本来实现。



### 新特性介绍-Spark 1.x的Volcano Iterator Model技术缺陷分析

### Volcano Iterator Model


深入剖析Spark 2.x的第二代tungsten引擎原理之前，先看一下当前的Spark的工作原理。我们可以通过一个SQL来举例，这个SQL扫描了单个表，然后对属性等于指定值的记录进行汇总计数。SQL语句如下：select count(*) from store_sales where ss_item_sk=1000。

要执行这个查询，Spark 1.x会使用一种最流行、最经典的查询求值策略，该策略主要基于
Volcano Iterator Model。在这种模型中，一个查询会包含多个operator，每个operator都会实现
一个接口，提供一个next()方法，该方法返回operator tree中的下一个operator。

![](http://i2.51cto.com/images/blog/201810/04/a02b80d6d8a3f329eecb8534a602edc8.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

举例来说，上面那个查询中的filter operator的代码大致如下所示：
![](http://i2.51cto.com/images/blog/201810/04/6cf3e007fd8bf804ea047fa816aeceb1.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)
让每一个operator都实现一个iterator接口，
可以让查询引擎优雅的组装任意operator在
一起。而不需要查询引擎去考虑每个operator
具体的一些处理逻辑，比如数据类型等。

Vocano Iterator Model也因此成为了数据库SQL执行引擎领域内内的20年中最流行的一种标准。而且Spark 
SQL最初的SQL执行引擎也是基于这个思想来实现的。




### Volcano Iterator Model vs 手写代码

举例来说，上面那个查询中的filter operator的代码大致如下所示：
![](http://i2.51cto.com/images/blog/201810/04/90daf09f9475221b84fd72891aac0702.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



让每一个operator都实现一个iterator接口，
可以让查询引擎优雅的组装任意operator在
一起。而不需要查询引擎去考虑每个operator
具体的一些处理逻辑，比如数据类型等。

Vocano Iterator Model也因此成为了数据库SQL执行引擎领域内内的20年中最流行的一种标准。而且Spark 
SQL最初的SQL执行引擎也是基于这个思想来实现的。

![](http://i2.51cto.com/images/blog/201810/04/a361458562656f65f9dfd2f39ef72597.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


我们可以清晰地看到，手写的代码的性能比Volcano Iterator Model高了一整个数量级，而这其中的原因包含以下几点：

1、避免了virtual function dispatch：在Volcano Iterator Model中，至少需要调用一次next()函数来获取下一个operator。这些函数调用在操作系统层面，会被编译为virtual function dispatch。而手写代码中，没有任何的函数调用逻辑。虽然说，现代的编译器已经对虚函数调用进行了大量的优化，但是该操作还是会执行多个CPU指令，并且执行速度较慢，尤其是当需要成百上千次地执行虚函数调用时。

2、通过CPU Register存取中间数据，而不是内存缓冲：在Volcano Iterator Model中，每次一个operator将数据交给下一个operator，都需要将数据写入内存缓冲中。然而在手写代码中，JVM JIT编译器会将这些数据写入CPU Register。CPU从内存缓冲种读写数据的性能比直接从CPU Register中读写数据，要低了一个数量级。

3、Loop Unrolling和SIMD：现代的编译器和CPU在编译和执行简单的for循环时，性能非常地高。编译器通常可以自动对for循环进行unrolling，并且还会生成SIMD指令以在每次CPU指令执行时处理多条数据。CPU也包含一些特性，比如pipelining，prefetching，指令reordering，可以让for循环的执行性能更高。然而这些优化特性都无法在复杂的函数调用场景中施展，比如Volcano Iterator Model。

    loop unrolling解释（小白的方式）
    for(int i = 0; i < 10; i++) { System.out.println(i) }
    System.out.println(1)
    System.out.println(2)
    System.out.println(3)
    ......

手写代码的好处就在于，它是专门为实现这个功能而编写的，代码简单，因此可以吸收上述所有优点，包括避免虚函数调用，将中间数据保存在CPU寄存器中，而且还可以被底层硬件进行for循环的自动优化。


## 新特性介绍-whole-stage code generation技术和vectorization技术


### Whole-stage code generation

之前讲解了手工编写的代码的性能，为什么比Volcano Iterator Model要好。所以如果要对Spark进行性能优化，一个思路就是在运行时动态生成代码，以避免使用Volcano模型，转而使用性能更高的代码方式。要实现上述目的，就引出了Spark第二代Tungsten引擎的新技术，whole-stage code generation。通过该技术，SQL语句编译后的operator-treee中，每个operator执行时就不是自己来执行逻辑了，而是通过whole-stage code generation技术，动态生成代码，生成的代码中会尽量将所有的操作打包到一个函数中，然后再执行动态生成的代码。

就以上一讲的SQL语句来作为示例，Spark会自动生成以下代码。如果只是一个简单的查询，那么Spark会尽可能就生成一个stage，并且将所有操作打包到一起。但是如果是复杂的操作，就可能会生成多个stage。


![](http://i2.51cto.com/images/blog/201810/04/03f2778836a62b4794c2bab584a97ecf.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



Spark提供了explain()方法来查看一个SQL的执行计划，而且这里面是可以看到通过whole-stage code generation生成的代码的执行计划的。如果看到一个步骤前面有个*符号，那么就代表这个步骤是通过该技术自动生成的。在这个例子中，Range、Filter和Aggregation都是自动生成的，Exchange不是自动生成的，因为这是一个网络传输数据的过程。


![](http://i2.51cto.com/images/blog/201810/04/456a86a4d0acfe13088fa410d643cfe0.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

很多用户会疑惑，从Spark 1.1版本开始，就一直听说有code generation类的feature引入，这跟spark 2.0中的这个技术有什么不同呢。实际上在spark 1.x版本中，code generation技术仅仅被使用在了expression evoluation方面（比如a + 1），即表达式求值，还有极其少数几个算子上（比如filter等）。而spark 2.0中的whole-stage code generation技术是应用在整个spark运行流程上的。

### Vectorization
对于很多查询操作，whole-stage code generation技术都可以很好地优化其性能。但是有一些特殊的操作，却无法很好的使用该技术，比如说比较复杂一些操作，如parquet文件扫描、csv文件解析等，或者是跟其他第三方技术进行整合。

如果要在上述场景提升性能，spark引入了另外一种技术，称作“vectorization”，即向量化。向量化的意思就是避免每次仅仅处理一条数据，相反，将多条数据通过面向列的方式来组织成一个一个的batch，然后对一个batch中的数据来迭代处理。每次next()函数调用都返回一个batch的数据，这样可以减少virtual function dispatch的开销。同时通过循环的方式来处理，也可以使用编译器和CPU的loop unrolling等优化特性。


![](http://i2.51cto.com/images/blog/201810/04/03eaf91033ef629639b111fca4531b97.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

这种向量化的技术，可以使用到之前说的3个点中的2个点。即，减少virtual function dispatch，以及进行loop unrolling优化。但是还是需要通过内存缓冲来读写中间数据的。所以，仅仅当实在无法使用whole-stage code generation时，才会使用vectorization技术。有人做了一个parquet文件读取的实验，采用普通方式以及向量化方式，性能也能够达到一个数量级的提升：


![](http://i2.51cto.com/images/blog/201810/04/51067644cb510460b2c941d8993a2231.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

### 总结
上述的whole-stage code generation技术，能否保证将spark 2.x的性能比spark 1.x来说提升10倍以上呢？这是无法完全保证的。虽然说目前的spark架构已经搭载了目前世界上最先进的性能优化技术，但是并不是所有的操作都可以大幅度提升性能的。简单来说，CPU密集型的操作，可以通过这些新技术得到性能的大幅度提升，但是很多IO密集型的操作，比如shuffle过程的读写磁盘，是无法通过该技术提升性能的。在未来，spark会花费更多的精力在优化IO密集型的操作的性能上。

## Spark  2.x与1.x对比以及分析、学习建议以及使用建议



### Spark 2.x与1.x对比

Spark 1.x：Spark Core（RDD）、Spark SQL（SQL+Dataframe+Dataset）、Spark Streaming、Spark MLlib、Spark Graphx

Spark 2.x：Spark Core（RDD）、Spark SQL（ANSI-SQL+Subquery+Dataframe/Dataset）、Spark Streaming、Structured Streaming、Spark MLlib（Dataframe/Dataset）、Spark Graphx、Second Generation Tungsten Engine（Whole-stage code generation+Vectorization）


###  Spark Core（RDD）


从Spark诞生之日开始，RDD就是Spark最主要的编程接口，重要程度类似于Hadoop中的MapReduce。RDD，简单来说，就是一个不可变的分布式数据集，被分为多个partition从而在一个集群上分布式地存储。我们可以使用RDD提供的各种transformation和action算子，对RDD执行分布式的计算操作。

可能很多人会问，Spark 2.0开始，包括Structured Streaming、Spark MLlib、Spark SQL底层都开始基于Dataframe/Dataset来作为基础计算引擎，那么Spark Core/RDD是不是就要被淘汰了？

回答是：错误！

Spark官方社区对于这个问题也是这个态度，Spark Core绝对不会被淘汰掉。因为Spark Core/RDD作为一种low-level的API有它的较为底层的应用场景，虽然后续这种场景会越来越少，Dataframe/Dataset API会逐渐替代原先Spark Core的一些场景，但是不可否认的是，这种场景还是存在的。此外，Dataframe/Dataset实际上底层也是基于Spark Core/RDD构建的。所以说，Spark Core/RDD是Spark生态中，不可替代的基础API和引擎，其他所有的组件几乎都是构建在它之上。未来它不会被淘汰，只是应用场景会减少而已。


Spark 2.x中，在离线批处理计算中，编程API，除了RDD以外，还增强了Dataframe/Dataset API。那么，我们到底什么时候应该使用Spark Core/RDD来进行编程呢？实际上，RDD和Dataset最大的不同在于，RDD是底层的API和内核，Dataset实际上基于底层的引擎构建的high-level的计算引擎。

1、如果我们需要对数据集进行非常底层的掌控和操作，比如说，手动管理RDD的分区，或者根据RDD的运行逻辑来结合各种参数和编程来进行较为底层的调优。因为实际上Dataframe/Dataset底层会基于whole-stage code generation技术自动生成很多代码，那么就意味着，当我们在进行线上报错的troubleshooting以及性能调优时，对Spark的掌控能力就会降低。而使用Spark Core/RDD，因为其运行完全遵循其源码，因此我们完全可以在透彻阅读Spark Core源码的基础之上，对其进行troubleshooting和底层调优。（最重要的一点）

2、我们要处理的数据是非结构化的，比如说多媒体数据，或者是普通文本数据。

3、我们想要使用过程式编程风格来处理数据，而不想使用domain-specific language的编程风格来处理数据。

4、我们不关心数据的schema，即元数据。

5、我们不需要Dataframe/Dataset底层基于的第二代tungsten引擎提供的whole-stage code generation等性能优化技术。




### Spark SQL（ANSI-SQL+Subquery）


Spark 2.x中的Spark SQL，提供了标准化SQL的支持，以及子查询的支持，大幅度提升了Spark在SQL领域的应用场景。而且本身在大数据领域中，SQL就是一个最广泛使用的用户入口，据不完全统计以及讲师的行业经验，做大数据的公司里，90%的应用场景都是基于SQL的。最典型的例子就是Hadoop，几乎用Hadoop的公司，90%都是基于Hive进行各种大数据的统计和分析。剩下10%是实时计算、机器学习、图计算。之所以有这种现象，主要就是因为SQL简单、易学、易用、直观。无论是研发人员，还是产品经理，还是运营人员，还是其他的人，都能在几天之内入门和学会SQL的使用，然后就可以基于大数据SQL引擎（比如Hive）基于企业积累的海量数据，根据自己的需求进行各种统计和分析。

此外，据Spark官方社区所说，Spark 2.x一方面对SQL的支持做了大幅度的增强，另一方面，也通过优化了底层的计算引擎（第二代tungsten引擎，whole-stage code generation等），提升了SQL的执行性能以及稳定性。

所以在Spark 2.x中，一方面，开始鼓励大家多使用Spark SQL的SQL支持，采用Spark SQL来编写SQL进行最常见的大数据统计分析。比如可以尝试将Hive中的运行的一些SQL语句慢慢迁移到Spark SQL上来。另外一方面，也提醒大家，一般一个新的大版本，都是不太稳定的，因此Spark SQL虽然在功能、性能和稳定性上做了很多的增强，但是难免还是会有很多的坑。因此建议大家在做Hive/RDBMS（比如Oracle）到Spark SQL的迁移时，要小心谨慎，一点点迁移，同时做好踩坑的准备。

### Spark SQL（Dataframe/Dataset）

就像RDD一样，Dataframe也代表一个不可变的分布式数据集。与RDD不同的一点是，Dataframe引入了schema的概念，支持以复杂的类型作为元素类型，同时指定schema，比如Row。因此Dataframe更像是传统关系型数据库中的表的概念。为了提升开发人员对大数据的处理能力，Dataframe除了提供schema的引入，还基于Schema提供了很多RDD所不具备的high-level API，以及一些domain-specific language（特定领域编程语言）。但是在Spark 2.0中，Dataframe和Dataset合并了，Dataframe已经不是一个单独的概念了，目前仅仅只是Dataset[Row]的一个类型别名而已，你可以理解为Dataframe就是Dataset。


![](http://i2.51cto.com/images/blog/201810/04/8089c2e8a9bca4c8134299f07a47e098.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)


从Spark 2.0开始，Dataset有两种表现形式：typed API和untyped API。我们可以认为，Dataframe就是Dataset[Row]的别名，Row就是一个untyped类型的对象，因为Row是类似于数据库中的一行，我们只知道里面有哪些列，但是有些列即使不存在，我们也可以这对这些不存在的列进行操作。因此其被定义为untyped，就是弱类型。

而Dataset[T]本身，是一种typed类型的API，其中的Object通常都是我们自己自定义的typed类型的对象，因为对象是我们自己定义的，所以包括字段命名以及字段类型都是强类型的。目前Scala支持Dataset和Dataframe两种类型，Java仅仅支持Dataset类型，Python和R因为不具备compile-time type-safety特性，因此仅仅支持Dataframe。


Dataset API有哪些优点呢？

1、静态类型以及运行时的类型安全性

SQL语言具有最不严格的限制，而Dataset具有最严格的限制。SQL语言在只有在运行时才能发现一些错误，比如类型错误，但是由于Dataframe/Dataset目前都是要求类型指定的（静态类型），因此在编译时就可以发现类型错误，并提供运行时的类型安全。比如说，如果我们调用了一个不属于Dataframe的API，编译时就会报错。但是如果你使用了一个不存在的列，那么也只能到运行时才能发现了。而最严格的就是Dataset了，因为Dataset是完全基于typed API来设计的，类型都是严格而且强类型的，因此如果你使用了错误的类型，或者对不存在的列进行了操作，都能在编译时就发现。


-|SQL | Dataframe|Dataset
---|---|---|---
Syntax Error|	Runtime	 | Compile Time	|Compile Time
Analysis Error|	Runtime	|Runtime|	Compile Time


2、将半结构化的数据转换为typed自定义类型

举例来说，如果我们现在有一份包含了学校中所有学生的信息，是以JSON字符串格式定义的，比如：{“name”: “leo”, “age”, 19, “classNo”: 1}。我们可以自己定义一个类型，比如case class Student(name: String, age: Integer, classNo: Integer)。接着我们就可以加载指定的json文件，并将其转换为typed类型的Dataset[Student]，比如val ds = spark.read.json("students.json").as[Student]。

在这里，Spark会执行三个操作：
1、Spark首先会读取json文件，并且自动推断其schema，然后根据schema创建一个Dataframe。

2、在这里，会创建一个Dataframe=Dataset[Row]，使用Row来存放你的数据，因为此时还不知道具体确切的类型。

3、接着将Dataframe转换为Dataset[Student]，因为此时已经知道具体的类型是Student了。

这样，我们就可以将半结构化的数据，转换为自定义的typed结构化强类型数据集。并基于此，得到之前说的编译时和运行时的类型安全保障。


3、API的易用性

Dataframe/Dataset引入了很多的high-level API，并提供了domain-specific language风格的编程接口。这样的话，大部分的计算操作，都可以通过Dataset的high-level API来完成。通过typed类型的Dataset，我们可以轻松地执行agg、select、sum、avg、map、filter、groupBy等操作。使用domain-specific language也能够轻松地实现很多计算操作，比如类似RDD算子风格的map()、filter()等。

4、性能

除了上述的优点，Dataframe/Dataset在性能上也有很大的提升。首先，Dataframe/Dataset是构建在Spark SQL引擎之上的，它会根据你执行的操作，使用Spark SQL引擎的Catalyst来生成优化后的逻辑执行计划和物理执行计划，可以大幅度节省内存或磁盘的空间占用的开销（相对于RDD来说，Dataframe/Dataset的空间开销仅为1/3~1/4），也能提升计算的性能。其次，Spark 2.x还引入第二代Tungsten引擎，底层还会使用whole-stage code generation、vectorization等技术来优化性能。

什么时候应该使用Dataframe/Dataset，而不是RDD呢？
    
    1、如果需要更加丰富的计算语义，high-level的抽象语义，以及domain-specific API。
    2、如果计算逻辑需要high-level的expression、filter、map、aggregation、average、sum、SQL、列式存储、lambda表达式等语义，来处理半结构化，或结构化的数据。
    3、如果需要高度的编译时以及运行时的类型安全保障。
    4、如果想要通过Spark SQL的Catalyst和Spark 2.x的第二代Tungsten引擎来提升性能。
    5、如果想要通过统一的API来进行离线、流式、机器学习等计算操作。
    6、如果是R或Python的用户，那么只能使用Dataframe。

最后，实际上，Spark官方社区对RDD和Dataframe/Dataset的建议时，按照各自的特点，根据的需求场景，来灵活的选择最合适的引擎。甚至说，在一个Spark应用中，也可以将两者结合起来一起使用。


### Spark Streaming&Structured Streaming

Spark Streaming是老牌的Spark流式计算引擎，底层基于RDD计算引擎。除了类似RDD风格的计算API以外，也提供了更多的流式计算语义，比如window、updateStateByKey、transform等。同时对于流式计算中重要的数据一致性、容错性等也有一定的支持。

但是Spark 2.x中也推出了全新的基于Dataframe/Dataset的Structured Streaming流式计算引擎。相较于Spark Streaming来说，其最大的不同之处在于，采用了全新的逻辑模型，提出了real-time incremental table的概念，更加统一了流式计算和离线计算的概念，减轻了用户开发的负担。同时还提供了（可能在未来提供）高度封装的特性，比如双流的全量join、与离线数据进行join的语义支持、内置的自动化容错机制、内置的自动化的一次且仅一次的强一致性语义、time-based processing、延迟数据达到的自动处理、与第三方外部存储进行整合的sink概念，等等高级特性。大幅度降低了流式计算应用的开发成本。

这里要提的一句是，首先，目前暂时建议使用Spark Streaming，因为Spark Streaming基于RDD，而且经过过个版本的考验，已经趋向于稳定。对于Structured Streaming来说，一定要强调，在Spark 2.0版本刚推出的时候，千万别在生产环境使用，因为目前官方定义为beta版，就是测试版，里面可能有很多的bug和问题，而且上述的各种功能还不完全，很多功能还没有。因此Structured Streaming的设计理念虽然非常好，但是个人建议在后续的版本中再考虑使用。目前可以保持关注和学习，并做一些实验即可。

### Spark MLlib&GraphX

Spark MLlib未来将主要基于Dataframe/Dataset API来开发。而且还会提供更多的机器学习算法。因此可以主要考虑使用其spark.ml包下的API即可。

Spark GraphX，目前发展较为缓慢，如果有图计算相关的应用，可以考虑使用。


### Spark 2.x学习建议


纵观之前讲的内容，Spark 2.0本次，其实主要就是提升了底层的性能，搭载了第二代Tungsten引擎；同时大幅度调整和增强了ANSI-SQL支持和Dataframe/Dataset API，并将该API作为Spark未来重点发展的发现；此外，为了提供更好的流式计算解决方案，发布了一个测试版的Structured Streaming模块。

而且之前也讲解了Spark 1.x和Spark 2.x中的每一个模块。大家可以明确看到：

第一，Spark 1.x没有任何一个组件是被淘汰的；

第二，Spark这次重点改造的是Tungsten Engine、Dataframe/Dataset以及Structured Streaming，对于之前Spark 1.x课程中讲解的Spark Core、Spark SQL以及Spark Streaming，包括Spark Core的性能调优和源码剖析，集群运维管理，几乎没有做太多的调整；

第三，Spark Core、Spark SQL、Spark Streaming、Dataframe/Dataset、Structured Streaming、Spark MLlib和GraphX，每个组件目前都有其特点和用途，任何一个不是积累和过时的技术；

第五，Spark 2.0的新东西中，ANSI-SQL和Dataframe/Dataset API是可以重点尝试使用的，但是Structured Streaming还停留在实验阶段，完全不能应用到生产项目中。因此目前流式计算主要还是使用Spark Streaming。个人预计，至少要在2017年春节过后，Structured Streaming才有可能进入稳定状态，可以尝试使用。

### Spark 2.x使用建议


在透彻学习了Spark 1.x和Spark 2.x的知识体系之后，对于Spark的使用，建议如下

1、建议开始大量尝试使用Spark SQL的标准化SQL支持以及子查询支持的特性，大部分的大数据统计分析应用，采用Spark SQL来实现。

2、其次，对于一些无法通过SQL来实现的复杂逻辑，比如一些算法的实施，或者一些跟DB、缓存打交道的大数据计算应用，建议采用Dataframe/Dataset API来实施。

3、接着，对于一些深刻理解课程中讲解的Spark Core/RDD，以及内核源码的高阶同学，如果遇到了因为Spark SQL和Dataframe/Dataset导致的线上的莫名其妙的报错，始终无法解决，或者是觉得有些性能，通过第二代Tungsten引擎也无法很好的调优，需要自己手工通过RDD控制底层的分区以及各种参数来进行调优，那么建议使用Spark Core/RDD来重写SQL类应用。

4、对于流式计算应用，建议目前还是使用Spark Streaming，因为其稳定；Structured Streaming目前是beta版本，很不稳定，因此目前建议仅仅是学习和实验即可。个人预计和建议，估计至少要到2017年春节后，Structured Streaming才可能具备部署生产环境的能力。

5、对于机器学习应用，建议使用spark.ml包下的机器学习API，因为其基于Dataframe/Dataset API实现，性能更好，而且未来是社区重点发展方向














































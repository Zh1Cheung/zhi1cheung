---
title: Java 业务开发常见错误
categories:
- JAVA
tags:
- Java 业务开发常见错误
---



## 判等问题

- 注意 equals 和 == 的区别

  - 对基本类型，比如 int、long，进行判等，只能使用 ==，比较的是直接值。因为基本类型的值就是其数值。
  - 对引用类型，比如 Integer、Long 和 String，进行判等，需要使用 equals 进行内容判等。因为引用类型的直接值是指针，使用 == 的话，比较的是指针，也就是两个对象在内存中的地址，即比较它们是不是同一个对象，而不是比较对象的内容。
    - Integer默认情况下会缓存[-128,127]的数值
    - 使用 == 对一个值为 128 的直接赋值的 Integer 对象和另一个值为 128 的 int 基本类型判等,我们把装箱的 Integer 和基本类型 int 比较，前者会先拆箱再比较，比较的肯定是数值而不是引用
    - 对两个 new 出来的值都为 2 的 String 使用 == 判等：new 出来的两个 String 是不同对象，引用当然不同，所以得到 false 的结果。
  - 业务代码中滥用 intern，可能会产生性能问题
    - 其实，原因在于字符串常量池是一个固定容量的 Map。如果容量太小（Number of buckets=60013）、字符串太多（1000 万个字符串），那么每一个桶中的字符串数量会非常多，所以搜索起来就很慢。输出结果中的 Average bucket size=167，代表了 Map中桶的平均长度是 167

- 实现一个 equals 没有这么简单

  - 考虑到性能，可以先进行指针判等，如果对象是同一个那么直接返回 true；
  - 需要对另一方进行判空，空对象和自身进行比较，结果一定是 fasle（if (this == o) return true）；
  - 需要判断两个对象的类型，如果类型都不同，那么直接返回 false；
  - 确保类型相同的情况下再进行类型强制转换，然后逐一判断所有字段。

- hashCode 和 equals 要配对实现

  - 散列表需要使用 hashCode 来定位元素放到哪个桶。如果自定义对象没有实现自定义的 hashCode 方法，就会使用 Object 超类的默认实现，得到的两个hashCode 是不同的，导致无法满足需求。

- 注意 compareTo 和 equals 的逻辑一致性

  - binarySearch 方法内部调用了元素的 compareTo 方法进行比较；
  - 修复方式很简单，确保 compareTo 的比较逻辑和 equals 的实现一致即可。通过 Comparator.comparing 这个便捷的方法来实现两个字段的比较
  - 其实，这个问题容易被忽略的原因在于两方面：
    - 我们使用了 Lombok 的 @Data 标记了 Student,其实包含了 @EqualsAndHashCode 注解的作用，也就是默认情况下使用类型所有的字段（不包括 static 和 transient 字段）参与到 equals 和 hashCode 方法的实现中。因为这两个方法的实现不是我们自己实现的，所以容易忽略其逻辑。
    - compareTo 方法需要返回数值，作为排序的依据，容易让人使用数值类型的字段随意实现
  - 对于自定义的类型，如果要实现 Comparable，请记得 equals、hashCode、compareTo 三者逻辑一致。

- 小心 Lombok 生成代码的“坑”

  - 使用 @EqualsAndHashCode.Exclude 注解来修饰 name 字段，从 equals 和 hashCode的实现中排除 name 字段

  - 为解决这个问题，我们可以手动设置 callSuper 开关为 true，来覆盖这种默认行为

  - ```java
    @Data 
    @EqualsAndHashCode(callSuper = true) 
    class Employee extends Person
    ```

- 在实现 equals 时，我是先通过 getClass 方法判断两个对象的类型，你可能会想到还可以使用 instanceof 来判断。你能说说这两种实现方式的区别吗

  - getclass需要具体一种类型才能做比较
  - instanceof 涉及到继承的子类是都属于父类的判断

- 可以通过 HashSet 的 contains 方法判断元素是否在HashSet 中，同样是 Set 的 TreeSet 其 contains 方法和 HashSet 有什么区别吗

  - HashSet就是使用HashMap调用equals，判断两对象的HashCode是否相等
  - TreeSet因为是一个树形结构，则需要考虑树的左右。则需要通过compareTo计算正负值，看最后能否找到compareTo为0的值，找到则返回true

- quals比较的对象除了所谓的相等外，还有一个非常重要的因素，就是该对象的类加载器也必须是同一个，不然equals返回的肯定是false

  - 重启后，两个对象相等，
    结果是true，但是修改了某些东西后，热加载（不用重启即可生效）后，再次执行equals，返回就是false，因为热加载使用的类加载器和程序正常启动的类加载器不同

  

  

  

## 数值计算

- “危险”的 Double

  - 对 2.15-1.10 和 1.05 判等，结果判等不成立
    - 出现这种问题的主要原因是，计算机是以二进制存储数值的，浮点数也不例外
    - 对于计算机而言，0.1 无法精确表达，这是浮点数计算造成精度损失的根源
  - 我们大都听说过 BigDecimal 类型，浮点数精确表达和运算的场景，一定要使用这个类型。不过，在使用 BigDecimal 时有几个坑需要避开。
    - 浮点数运算避坑第一原则：使用 BigDecimal 表示和计算浮点数，且务必使用字符串的构造方法来初始化BigDecimal
    - 如果一定要用 Double 来初始化 BigDecimal 的话，可以使用 BigDecimal.valueOf 方法，以确保其表现和字符串形式的构造方法一致
    - BigDecimal 有 scale 和 precision 的概念，scale 表示小数点右边的位数，而 precision 表示精度，也就是有效数字的长度。
      - 调试一下可以发现，new BigDecimal(Double.toString(100)) 得到的 BigDecimal 的scale=1、precision=4；而 new BigDecimal(“100”) 得到的 BigDecimal 的 scale=0、precision=3。对于 BigDecimal 乘法操作，返回值的 scale 是两个数的 scale 相加。

- 考虑浮点数舍入和格式化的方式

  - String.format 采用四舍五入的方式进行舍入，取 1 位小数

    - 这就是由精度问题和舍入方式共同导致的，double 和 float 的 3.35 其实相当于 3.350xxx和 3.349xxx

    - 第二原则：浮点数的字符串格式化也要通过BigDecimal 进行。

      - ```java
        // 这次得到的结果是 3.3 和 3.4，符合预期
        BigDecimal num1 = new BigDecimal("3.35"); 
        BigDecimal num2 = num1.setScale(1, BigDecimal.ROUND_DOWN); 
        System.out.println(num2); 
        BigDecimal num3 = num1.setScale(1, BigDecimal.ROUND_HALF_UP); 
        System.out.println(num3);
        ```

- 用 equals 做判等，就一定是对的吗？

  - BigDecimal 的 equals 方法的注释中说明了原因，equals 比较的是 BigDecimal 的 value 和 scale，1.0 的 scale 是 1，1 的scale 是 0，所以结果一定是 false
  - 如果我们希望只比较 BigDecimal 的 value，可以使用 compareTo 方法
    - 你可能会意识到 BigDecimal 的 equals 和 hashCode 方法会同时考虑 value和 scale，如果结合 HashSet 或 HashMap 使用的话就可能会出现麻烦。比如，我们把值为 1.0 的 BigDecimal 加入 HashSet，然后判断其是否存在值为 1 的 BigDecimal，得到的结果是 false
    - 解决这个问题的办法有两个
      - 第一个方法是，使用 TreeSet 替换 HashSet。TreeSet 不使用 hashCode 方法，也不使用 equals 比较元素，而是使用 compareTo 方法，所以不会有问题	
      - 第二个方法是，把 BigDecimal 存入 HashSet 或 HashMap 前，先使用stripTrailingZeros 方法去掉尾部的零，比较的时候也去掉尾部的 0，确保 value 相同的BigDecimal，scale 也是一致的

- 小心数值溢出问题

  - 方法一是，考虑使用 Math 类的 addExact、subtractExact 等 xxExact 方法进行数值运算，这些方法可以在数值溢出时主动抛出异常。（执行后，可以得到 ArithmeticException，这是一个 RuntimeException）
  - 方法二是，使用大数类 BigInteger。BigDecimal 是处理浮点数的专家，而 BigInteger 则是对大数进行科学计算的专家。





## 线程池

- 线程池的声明需要手动进行

  - newFixedThreadPool 和 newCachedThreadPool，可能因为资源耗尽导致OOM 问题。

    - ```java
      // 虽然使用 newFixedThreadPool 可以把工作线程控制在固定的数量上，但任务队列是无界的。如果任务较多并且执行较慢的话，队列可能会快速积压，撑爆内存导致 OOM。
      public void oom() throws InterruptedException {
              ThreadPoolExecutor threadpool =(ThreadPoolExecutor) Executors.newFixedThreadPool(1);
              for (int i = 0; i < 10000000; i++) {
                  threadpool.execute(()->{
                      String payload = IntStream.rangeClosed(1,10000)
                              .mapToObj(__->"A")
                              .collect(Collectors.joining(""))+ UUID.randomUUID().toString();
                      try{
                          TimeUnit.HOURS.sleep(1);
                      }catch (InterruptedException e){
                          
                      }
                  });
                  threadpool.shutdown();
                  threadpool.awaitTermination(1,TimeUnit.HOURS);
      
              }
      ```

    - newCachedThreadPool 

      - 从日志中可以看到，这次 OOM 的原因是无法创建线程
      - 翻看 newCachedThreadPool 的源码可以看到，这种线程池的最大线程数是 Integer.MAX_VALUE，可以认为是没有上限的，而其工作队列 SynchronousQueue 是一个没有存储空间的阻塞队列
      - 这意味着，只要有请求到来，就必须找到一条工作线程来处理，如果当前没有空闲的线程就再创建一条新的。由于我们的任务需要 1 小时才能执行完成，大量的任务进来后会创建大量的线程。我们知道线程是需要分配一定的内存空间作为线程栈的，比如 1MB，因此无限制创建线程必然会
        导致 OOM

  - 应该手动 new ThreadPoolExecutor 来创建线程池。

- 线程池线程管理策略

  - ```java
    // 通过java在做定时任务的时候最好使用scheduleThreadPoolExecutor的方式
    // scheduleAtFixedRate(commod,initialDelay,period,unit)
    // initialDelay是说系统启动后，需要等待多久才开始执行。
    // period为固定周期时间，按照一定频率来重复执行任务。
    // 如果period设置的是3秒，系统执行要5秒；那么等上一次任务执行完就立即执行，也就是任务与任务之间的差异是5s；
    // 如果period设置的是3s，系统执行要2s；那么需要等到3S后再次执行下一次任务。
    
    // 最简陋的监控，每秒输出一次线程池的基本内部信息，包括线程数、活跃线程数、完成了多少任务，以及队列中还有多少积压任务等信息
    ThreadPoolExecutor threadpool =(ThreadPoolExecutor) Executors.newFixedThreadPool(1);
            Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(()-> {
                threadpool.getPoolSize();
                threadpool.getActiveCount();
                threadpool.getCompletedTaskCount();
                threadpool.getQueue().size();
            },0,1,TimeUnit.SECONDS);
    ```

- 让线程池更激进一点，优先开启更多的线程，而把队列当成一个后备方案呢？

  - 不知道你有没有想过：Java 线程池是先用工作队列来存放来不及处理的任务，满了之后再扩容线程池。当我们的工作队列设置得很大时，最大线程数这个参数显得没有意义，因为队列很难满，或者到满的时候再去扩容线程池已经于事无补了。

  - 

  - ```java
    // 使用Java 7 LinkedTransferQueue并进行offer()调用tryTransfer()。当有一个正在等待的使用者线程时，任务将被传递给该线程。否则，offer()将返回false ThreadPoolExecutor并将产生一个新线程。
    // submit()方法是调用了workQueue的offer()方法来塞入task，而offer()方法是非阻塞的，当workQueue已经满的时候，offer()方法会立即返回false，并不会阻塞在那里等待workQueue有空出位置，所以要让submit()阻塞，关键在于改变向workQueue添加task的行为
    // 这就达到了想要的效果：当workQueue满时，submit()一个task会导致调用我们自定义的RejectedExecutionHandler，而我们自定义的RejectedExecutionHandler会保证该task继续被尝试用阻塞式的put()到workQueue中。
    BlockingQueue<Runnable> queue = new LinkedTransferQueue<Runnable>() {
            @Override
            public boolean offer(Runnable e) {
                return tryTransfer(e);
            }
        };
        ThreadPoolExecutor threadPool = new ThreadPoolExecutor(1, 50, 60, TimeUnit.SECONDS, queue);
        threadPool.setRejectedExecutionHandler(new RejectedExecutionHandler() {
            @Override
            public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) {
                try {
                    executor.getQueue().put(r);
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                }
            }
        });
    ```

- 务必确认清楚线程池本身是不是复用的

  - 这样一个事故：某项目生产环境时不时有报警提示线程数过多，超过2000 个，收到报警后查看监控发现，瞬时线程数比较多但过一会儿又会降下来，线程数抖动很厉害，而应用的访问量变化不大。
    - 但是，来到 ThreadPoolHelper 的实现让人大跌眼镜，getThreadPool 方法居然是每次都使用 Executors.newCachedThreadPool 来创建一个线程池。
    - 我们可以想到 newCachedThreadPool 会在需要时创建必要多的线程，业务代码的一次业务操作会向线程池提交多个慢任务，这样执行一次业务操作就会开启多个线程。如果业务操作并发量较大的话，的确有可能一下子开启几千个线程。
  - 那，为什么我们能在监控中看到线程数量会下降，而不会撑爆内存呢？
    - 回到 newCachedThreadPool 的定义就会发现，它的核心线程数是 0，而 keepAliveTime是 60 秒，也就是在 60 秒之后所有的线程都是可以回收的。好吧，就因为这个特性，我们的业务程序死得没太难看
    - 要修复这个 Bug 也很简单，使用一个静态字段来存放线程池的引用，返回线程池的代码直接返回这个静态字段即可。这里一定要记得我们的最佳实践，手动创建线程池。

- 需要仔细斟酌线程池的混用策略

  - 对于执行比较慢、数量不大的 IO 任务，或许要考虑更多的线程数，而不需要太大的队列。
  - 而对于吞吐量较大的计算型任务，线程数量不宜过多，可以是 CPU 核数或核数 *2（理由是，线程一定调度到某个 CPU 进行执行，那么过多的线程只会增加线程切换的开销，并不能提升吞吐量），但可能需要较长的队列来做缓冲。
  - 遇到过这么一个问题，业务代码使用了线程池异步处理一些内存中的数据，但通过监控发现处理得非常慢，整个处理过程都是内存中的计算不涉及 IO 操作，也需要数秒的处理时间，应用程序 CPU 占用也不是特别高，有点不可思议。
    - 经排查发现，业务代码使用的线程池，还被一个后台的文件批处理任务用到了
    - 或许是够用就好的原则，这个线程池只有 2 个核心线程，最大线程也是 2，使用了容量为100 的 ArrayBlockingQueue 作为工作队列，使用了 CallerRunsPolicy 拒绝策略
    - CallerRunsPolicy    -- 当任务添加到线程池中被拒绝时，会在线程池当前正在运行的Thread线程池中处理被拒绝的任务。
    - 因为开启了CallerRunsPolicy 拒绝处理策略，所以当线程满载队列也满的情况下，任务会在提交任务的线程，或者说调用 execute 方法的线程执行，也就是说不能认为提交到线程池的任务就一定是异步处理的。如果使用了CallerRunsPolicy 策略，那么有可能异步任务变为同步执行。
    - 细想一下，问题其实没有这么简单。因为原来执行 IO 任务的线程池使用的是CallerRunsPolicy 策略，所以直接使用这个线程池进行异步计算的话，当线程池饱和的时候，计算任务会在执行 Web 请求的 Tomcat 线程执行，这时就会进一步影响到其他同步处理的线程，甚至造成整个应用程序崩溃。

- Java 8 的 parallel stream 功能

  - 可以让我们很方便地并行处理集合中的元素，其背后是共享同一个 ForkJoinPool，默认并行度是CPU 核数 -1
  - 对于 CPU 绑定的任务来说，使用这样的配置比较合适，但如果集合操作涉及同步 IO 操作的话（比如数据库操作、外部服务调用等），建议自定义一个ForkJoinPool（或普通线程池）

- 我们改进了 ThreadPoolHelper 使其能够返回复用的线程池。如果我们不小心每次都创建了这样一个自定义的线程池（10 核心线程，50 最大线程，2 秒回收的），反复执行测试接口线程，最终可以被回收吗？会出现 OOM 问题吗？

  - 每次请求都新建线程池，每个线程池的核心数都是10, 虽然自定义线程池设置2秒回收，但是没超过线程池核心数10是不会被回收的, 不间断的请求过来导致创建大量线程，最终OOM
  - ThreadPoolExecutor回收不了，工作线程Worker是内部类，只要它活着，换句话说线程在跑，就会阻止ThreadPoolExecutor回收，所以其实ThreadPoolExecutor是无法回收的，并不能认为ThreadPoolExecutor没有引用就能回收







## 缓存

- 不要把 Redis 当作数据库
  - Redis 的特点是，处理请求很快，但无法保存超过内存大小的数据
  - 常用的数据淘汰策略
    - 其实，这些算法是 Key 范围 +Key 选择算法的搭配组合，其中范围有 allkeys 和 volatile 两种，算法有 LRU、TTL 和 LFU 三种
    - 首先，从算法角度来说，Redis 4.0 以后推出的 LFU 比 LRU 更“实用”。试想一下，如果一个 Key 访问频率是 1 天一次，但正好在 1 秒前刚访问过，那么 LRU 可能不会选择优先淘汰这个 Key，反而可能会淘汰一个 5 秒访问一次但最近 2 秒没有访问过的 Key，而 LFU 算法不会有这个问题。而 TTL 会比较“头脑简单”一点，优先删除即将过期的 Key，但有可能这个 Key 正在被大量访问。
    - 然后，从 Key 范围角度来说，allkeys 可以确保即使 Key 没有 TTL 也能回收，如果使用的时候客户端总是“忘记”设置缓存的过期时间，那么可以考虑使用这个系列的算法。而 volatile 会更稳妥一些，万一客户端把 Redis 当做了长效缓存使用，只是启动时候初始化一次缓存，那么一旦删除了此类没有 TTL 的数据，可能就会导致客户端出错。
- 注意缓存雪崩问题
  - 从广义上说，产生缓存雪崩的原因有两种
    - 第一种是，缓存系统本身不可用，导致大量请求直接回源到数据库；
    - 第二种是，应用设计层面大量的 Key 在同一时间过期，导致大量的数据回源。
  - 如何确保大量 Key 不在同一时间被动过期
    - 差异化缓存过期时间，不要让大量的 Key 在同一时间过期。比如，在初始化缓存的时候，设置缓存的过期时间是 30 秒 +10 秒以内的随机延迟（扰动值）。这样，这些Key 不会集中在 30 秒这个时刻过期，而是会分散在 30~40 秒之间过期
- 注意缓存击穿问题
  - 在某些 Key 属于极端热点数据，且并发量很大的情况下，如果这个 Key 过期，可能会在某个瞬间出现大量的并发请求同时回源，相当于大量的并发请求直接打到了数据库。
  - 使用 Redisson 来获取一个基于 Redis 的分布式锁，在查询数据库之前先尝试获取锁，这样，可以把回源到数据库的并发限制在 1
  - 在真实的业务场景下，不一定要这么严格地使用双重检查分布式锁进行全局的并发限制，因为这样虽然可以把数据库回源并发降到最低，但也限制了缓存失效时的并发。可以考虑的方式是
    - 方案一，使用进程内的锁进行限制，这样每一个节点都可以以一个并发回源数据库；
    - 方案二，不使用锁进行限制，而是使用类似 Semaphore 的工具限制并发数，比如限制为 10，这样既限制了回源并发数不至于太大，又能使得一定量的线程可以同时回源。
- 注意缓存穿透问题
  - 缓存中没有数据不一定代表数据没有缓存，还有一种可能是原始数据压根就不存在
  - 解决缓存穿透有以下两种方案
    - 方案一，对于不存在的数据，同样设置一个特殊的 Value 到缓存中，比如当数据库中查出的用户信息为空的时候，设置 NODATA 这样具有特殊含义的字符串到缓存中。这样下次请求缓存的时候还是可以命中缓存，即直接从缓存返回结果，不查询数据库
    - 方案二，即使用布隆过滤器做前置过滤
      - 布隆过滤器是一种概率型数据库结构，由一个很长的二进制向量和一系列随机映射函数组成。它的原理是，当一个元素被加入集合时，通过 k 个散列函数将这个元素映射成一个 m位 bit 数组中的 k 个点，并置为 1。
      - 要用上布隆过滤器，我们可以使用 Google 的 Guava 工具包提供的 BloomFilter 类改造一
        下程序：启动时，初始化一个具有所有有效用户 ID 的、10000 个元素的 BloomFilter，在
        从缓存查询数据之前调用其 mightContain 方法，来检测用户 ID 是否可能存在；如果布隆
        过滤器说值不存在，那么一定是不存在的
- 注意缓存数据同步策略
  - Cache Aside 更新模式
  - 前面提到的 3 个案例，其实都属于缓存数据过期后的被动删除。在实际情况下，修改了原始数据后，考虑到缓存数据更新的及时性，我们可能会采用主动更新缓存的策略
  - 先更新缓存，再更新数据库；
    - “先更新缓存再更新数据库”策略不可行。数据库设计复杂，压力集中，数据库因为超时等原因更新操作失败的可能性较大，此外还会涉及事务，很可能因为数据库更新失败，导致缓存和数据库的数据不一致。
  - 先更新数据库，再更新缓存；
    - “先更新数据库再更新缓存”策略不可行。一是，如果线程 A 和 B 先后完成数据库更新，但更新缓存时却是 B 和 A 的顺序，那很可能会把旧数据更新到缓存中引起数据不一致；二是，我们不确定缓存中的数据是否会被访问，不一定要把所有数据都更新到缓存中去。
  - 先删除缓存，再更新数据库，访问的时候按需加载数据到缓存；
    - “先删除缓存再更新数据库，访问的时候按需加载数据到缓存”策略也不可行。在并发的情况下，很可能删除缓存后还没来得及更新数据库，就有另一个线程先读取了旧值到缓存中，如果并发量很大的话这个概率也会很大。
  - 先更新数据库，再删除缓存，访问的时候按需加载数据到缓存。
    - “先更新数据库再删除缓存，访问的时候按需加载数据到缓存”策略是最好的。
    - 这种做法其实不能算是坑，在实际的系统中也推荐使用这种方式。但是这种方式理论上还是可能存在问题。以Redis和Mysql为例，查询操作没有命中缓存，然后查询出数据库的老数据。此时有一个并发的更新操作，更新操作在读操作之后更新了数据库中的数据并且删除了缓存中的数据。然而读操作将从数据库中读取出的老数据更新回了缓存。这样就会造成数据库和缓存中的数据不一致，应用程序中读取的都是原来的数据（脏数据）
    - 但是，仔细想一想，这种并发的概率极低。因为这个条件需要发生在读缓存时缓存失效，而且有一个并发的写操作。实际上数据库的写操作会比读操作慢得多，而且还要加锁，而读操作必需在写操作前进入数据库操作，又要晚于写操作更新缓存，所有这些条件都具备的概率并不大。但是为了避免这种极端情况造成脏数据所产生的影响，我们还是要为缓存设置过期时间。
    - 需要注意的是，更新数据库后删除缓存的操作可能失败，如果失败则考虑把任务加入延迟队列进行延迟重试，确保数据可以删除，缓存可以及时更新。因为删除操作是幂等的，所以即使重复删问题也不是太大，这又是删除比更新好的一个原因。
  - Write Behind Caching 更新模式
    - Write Behind Caching 更新模式就是在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是直接操作内存速度快。因为异步，Write Behind Caching 更新模式还可以合并对同一个数据的多次操作到数据库，所以性能的提高是相当可观的。
    - 但其带来的问题是，数据不是强一致性的，而且可能会丢失。另外，Write Behind Caching 更新模式实现逻辑比较复杂，因为它需要确认有哪些数据是被更新了的，哪些数据需要刷到持久层上。只有在缓存需要失效的时候，才会把它真正持久起来。







##  Spring

- 如果以容器为依托来管理所有的框架、业务对象，我们不仅可以无侵入地调整对象的关系，还可以无侵入地随时调整对象的属性，甚至是实现对象的替换。

- 单例的 Bean 如何注入 Prototype 的 Bean

  - 定义了这么一个SayService 抽象类，其中维护了一个类型是 ArrayList 的字段 data，用于保存方法处理的中间数据。每次调用 say 方法都会往 data 加入新数据，可以认为 SayService 是有状态，如果 SayService 是单例的话必然会 OOM

    - 正确的方式是，在为类标记上 @Service 注解把类型交由容器管理前，首先评估一下类是否有状态，然后为 Bean 设置合适的 Scope
    - 但，上线后还是出现了内存泄漏，证明修改是无效的。

  - Bean 默认是单例的，所以单例的 Controller 注入的 Service 也是一次性创建的，即使Service 本身标识了 prototype 的范围也没用。

    - 修复方式是，让 Service 以代理方式注入。这样虽然 Controller 本身是单例的，但每次都能从代理获取 Service。这样一来，prototype 范围的配置才能真正生效

    - ```java
      @Scope(value = WebApplicationContext.SCOPE_SESSION,proxyMode = ScopedProxyMode.INTERFACES)
      ```

    - 当然，如果不希望走代理的话还有一种方式是，每次直接从 ApplicationContext 中获取Bean

- 监控切面因为顺序问题导致 Spring 事务失效

  - 我们知道，切面本身是一个 Bean，Spring 对不同切面增强的执行顺序是由 Bean 优先级决定的，具体规则是
    - 入操作（Around（连接点执行前）Before），切面优先级越高，越先执行。一个切面的入操作执行完，才轮到下一切面，所有切面入操作执行完，才开始执行连接点（方法）。
    - 出操作（Around（连接点执行后）、After、AfterReturning、AfterThrowing），切面优先级越低，越先执行。一个切面的出操作执行完，才轮到下一切面，直到返回到调用点。
    - 同一切面的 Around 比 After、Before 先执行。
    - 对于 Bean 可以通过 @Order 注解来设置优先级，值越大优先级反而越低

- Spring 程序配置的优先级问题

  - Spring 通过环境 Environment 抽象出的 Property 和 Profile
    - 针对 Property，又抽象出各种 PropertySource 类代表配置源。一个环境下可能有多个配置源，每个配置源中有诸多配置项。在查询配置信息时，需要按照配置源优先级进行查询。
    - Profile 定义了场景的概念。通常，我们会定义类似 dev、test、stage 和 prod 等环境作为不同的 Profile
  - 对于非 Web 应用，Spring 对于 Environment 接口的实现是 StandardEnvironment 类
    - MutablePropertySources 类型的字段 propertySources，看起来代表了所有配置源；
      - propertySourceList 字段用来真正保存 PropertySource 的 List，且这个 List 是一个CopyOnWriteArrayList
      - 类中定义了 addFirst、addLast、addBefore、addAfter 等方法，来精确控制PropertySource 加入 propertySourceList 的顺序。这也说明了顺序的重要性。
    - getProperty 方法，通过 PropertySourcesPropertyResolver 类进行查询配置；
    - 实例化 PropertySourcesPropertyResolver 的时候，传入了当前的MutablePropertySources。





## Spring声明式事务

- 小心 Spring 的事务可能没有生效

  - 在使用 @Transactional 注解开启声明式事务时， 第一个最容易忽略的问题是，很可能事务并没有生效。
  - @Transactional 生效原则 1，除非特殊配置（比如使用 AspectJ 静态织入实现AOP），否则只有定义在 public 方法上的 @Transactional 才能生效。
  - @Transactional 生效原则 2，必须通过代理过的类从外部调用目标方法才能生效。
    - this 自调用、通过 self 调用（自己的类内部注入自己调用自己的 方法 ），以及在 Controller 中调用UserService 三种实现的区别
      - 通过 this 自调用，没有机会走到 Spring 的代理类
      - 后两种改进方案调用的是 Spring 注入的 XXXService，通过代理调用才有机会对XXXService的方法进行动态增强。
      - CGLIB 通过继承方式实现代理类，private 方法在子类不可见，无法进行事务增强；
  - 一个小技巧，强烈建议你在开发时打开相关的 Debug 日志，以方便了解Spring 事务实现的细节，并及时判断事务的执行情况
    - 我们的 Demo 代码使用 JPA 进行数据库访问，可以这么开启 Debug 日志
    - logging.level.org.springframework.orm.jpa=DEBUG

- 事务即便生效也不一定能回滚

  - 通过 AOP 实现事务处理可以理解为，使用 try…catch…来包裹标记了 @Transactional 注解的方法，当方法出现了异常并且满足一定条件的时候，在 catch 里面我们可以设置事务回滚，没有异常则直接提交事务。

    - 第一，只有异常传播出了标记了 @Transactional 注解的方法，事务才能回滚。
    - 第二，默认情况下，出现 RuntimeException（非受检异常）或 Error 的时候，Spring才会回滚事务。

  - 在 createUserWrong1 方法中会抛出一个 RuntimeException，但由于方法内 catch 了所有异常，异常无法从方法传播出去，事务自然无法回滚

  - 现在，我们来看下修复方式，以及如何通过日志来验证是否修复成功。

    - 第一，如果你希望自己捕获异常进行处理的话，也没关系，可以手动设置让当前事务处于回滚状态

    - 第二，在注解中声明，期望遇到所有的 Exception 都回滚事务（来突破默认不回滚受检异常的限制）

    - ```java
      TransactionAspectSupprot.currentTransactionStatus().setRollbackonly();
      
      @Transaction(rollbackFor=Exception.class)
      ```

- 请确认事务传播配置是否符合自己的业务逻辑

  - 在有些业务逻辑中，可能会包含多次数据库操作，我们不一定希望将两次操作作为一个事务来处理，这时候就需要仔细考虑事务传播的配置了，否则也可能踩坑。

  - 因为运行时异常逃出了 @Transactional 注解标记的createUserWrong 方法，Spring 当然会回滚事务了。如果我们希望主方法不回滚，应该把子方法抛出的异常捕获了。

  - 看到这里，修复方式就很明确了，想办法让子逻辑在独立事务中运行，也就是改一下子用户的方法，为注解加上 propagation =Propagation.REQUIRES_NEW 来设置 REQUIRES_NEW 方式的事务传播策略，也就是执行到这个方法时需要开启新的事务，并挂起当前事务

  - ```java
    @Transactional(propagation = Propagation.REQUIRES_NEW) 
    ```

- @Transactional 与 @Async注解不能同时在一个方法上使用, 这样会导致事物不生效。




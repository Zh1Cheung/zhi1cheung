---
title: Java集合
categories:
- JAVA
tags:
- Java集合
---





## 概述

- List , Set,  Map都是接口，前两个继承至collection接口，Map为独立接口
- collection接口下还有个Queue接口，有PriorityQueue类
- Set下有HashSet，LinkedHashSet，TreeSet
- List下有ArrayList，Vector，LinkedList
- Map下有Hashtable，LinkedHashMap，HashMap，TreeMap                     







## ArrayList	

- 介绍

  - ArrayList的底层数据结构为数组（数组是一组连续的内存空间），默认容量为10，线程不安全，可以存储null值。

  - arrayList由于本质是数组，所以它在数据的查询方面会很快，而在插入删除这些方面，性能下降很多，有移动很多数据才能达到应有的效果（需要扩容）

  - ```java
    ArrayList extends AbstractList
    AbstractList extends AbstractCollection 
    
    ```

  - ArrayList实现了List<E>接口、RandomAccess接口、Cloneable接口、Serializable接口

- add(E)

  - ensureCapacityInternal(size + 1);确定内部容量的方法。判断size+1的这个个数数组能否放得下，就在这个方法中去判断是否数组.length是否够用了
    - ensureCapacityInternal方法，主要是有两个目的：1.如果没初始化则进行初始化；2.校验添加元素后是否需要扩容
    - ArrayList中储存数据的其实就是一个数组，这个数组就是elementData
  - calculateCapacity
    - 如果elementData是空的话,将minCapacity变成10，也就是默认大小，但是在这里，还没有真正的初始化这个elementData的大小。
  - ensureExplicitCapacity(xxx)；确认实际的容量，这个方法就是真正的判断elementData是否够用
    - minCapacity如果大于了实际elementData的长度，那么就说明elementData数组的长度不够用，不够用那么就要增加elementData的length
  - grow(xxx); 扩展数组大小
    - newCapacity就是1.5倍的oldCapacity
    - elementData为空数组的时候，真正的初始化elementData的大小了，就是为10.前面的工作都是准备工作。
    - 新的容量大小已经确定好了，就copy数组，改变容量大小

- 循环遍历并删除元素的陷阱

  - ```java
    // 在遍历第二个元素字符串bb时因为符合删除条件，所以将该元素从数组中删除，并且将后一个元素移动（也是字符串bb）至当前位置，导致下一次循环遍历时后一个字符串bb并没有遍历到，所以无法删除
    public static void remove(ArrayList<String> list) {  
        for (int i = 0; i < list.size(); i++) {  
            String s = list.get(i);  
            if (s.equals("bb")) {  
                list.remove(s);  
            }  
        }  
    }  
    
    list.add("a");
    list.add("bb");
    list.add("bb");
    list.add("ccc");
    list.add("ccc");
    list.add("ccc");
    
    // 针对这种情况可以倒序删除的方式来避免
    
    ```

- ConcurrentModificationException

  - 在单线程环境下的解决办法
    - 在迭代器中如果要删除元素的话，需要调用Itr类的remove方法（iterator.remove()）
    - 在这个方法中，删除元素实际上调用的就是list.remove()方法，但是多了一个操作：expectedModCount = modCount;
  - 在多线程环境下的解决方法
    - 通过Iterator访问的情况下，每个线程里面返回的是不同的iterator，也就是说expectedModCount是每个线程私有
    - 在使用iterator迭代的时候使用synchronized或者Lock进行同步；
    - 使用并发容器CopyOnWriteArrayList代替ArrayList和Vector。





## LinkedList

- 介绍
  - LinkedList是一种可以在任何位置进行高效地插入和移除操作的有序序列，它是基于双向链表实现的
  - linkedList在执行任何操作的时候，都必须先遍历此列表来靠近通过index查找我们所需要的的值。（顺序存取）（注意和随机存取结构两个概念搞清楚）
  - 非线程安全的(异步)，其中在操作Interator时，如果改变列表结构(add\delete等)，会发生fail-fast
- 在addAll函数中，传入一个集合参数和插入位置，然后将集合转化为数组，然后再遍历数组，挨个添加数组的元素，但是问题来了，为什么要先转化为数组再进行遍历，而不是直接遍历集合呢？
  - 如果直接遍历集合的话，那么在遍历过程中需要插入元素，在堆上分配内存空间，修改指针域，这个过程中就会一直占用着这个集合，考虑正确同步的话，其他线程只能一直等待。
  - 如果转化为数组，只需要遍历集合，而遍历集合过程中不需要额外的操作，所以占用的时间相对是较短的，这样就利于其他线程尽快的使用这个集合。说白了，就是有利于提高多线程访问该集合的效率，尽可能短时间的阻塞。
- arrayList和LinkedList区别
  - arrayList底层是用数组实现的顺序表，是随机存取类型，可自动扩增，并且在初始化时，数组的长度是0，只有在增加元素时，长度才会增加。默认是10，不能无限扩增，有上限，在查询操作的时候性能更好
  - LinkedList底层是用链表来实现的，是一个双向链表，注意这里不是双向循环链表,顺序存取类型。在源码中，似乎没有元素个数的限制。应该能无限增加下去，直到内存满了在进行删除，增加操作时性能更好。
  - 两个都是线程不安全的，在iterator时，会发生fail-fast。

## ArrayList还是LinkedList

- ArrayList
  - 数组是一块连续的内存空间，并且实现了RandomAccess 接口标志，意味着 ArrayList 可以实现快速随机访问，所以 for 循环效率非常高。
  - ArrayList 的实现类源码中对象数组 elementData 使用了transient 修饰，防止对象数组被其他外部方法序列化。
    - 由于 ArrayList 的数组是基于动态扩增的，所以并不是所有被分配的内存空间都存储了数据。
    - 如果采用外部序列化法实现数组的序列化，会序列化整个数组。ArrayList 为了避免这些没有存储数据的内存空间被序列化，内部提供了两个私有方法 writeObject 以及 readObject来自我完成序列化与反序列化，从而在序列化与反序列化数组时节省了空间和时间。
  - 如果我们在初始化时就比较清楚存储数据的大小，就可以在 ArrayList 初始化时指定数组容量大小，并且在添加元素时，只在数组末尾添加元素，那么 ArrayList 在大量新增元素的场景下，性能并不会变差
  - 两个方法也有不同之处，添加元素到任意位置，会导致在该位置后的所有元素都需要重新排列，而将元素添加到数组的末尾，在没有发生扩容的前提下，是不会有元素复制排序过程的。
  - ArrayList 在每一次有效的删除元素操作之后，都要进行数组的重组，并且删除的元素位置越前，数组重组的开销就越大。
- LinkedList
  - LinkedList 的两个重要属性 first/last 属性，其实还有一个 size 属性。我们可以看到这三个属性都被 transient 修饰了，原因很简单，我们在序列化的时候不会只对头尾进行序列化，所以 LinkedList 也是自行实现 readObject 和writeObject 进行序列化与反序列化
  - LinkedList 的获取元素操作实现跟 LinkedList 的删除元素操作基本类似，通过分前后半段来循环查找到对应的元素。但是通过这种方式来查询元素是非常低效的，特别是在 for 循环遍历的情况下，每一次循环都会去遍历半个 List。以使用 iterator 方式迭代循环，直接拿到我们的元素，而不需要通过循环查找 List。
- 在添加元素到尾部的操作中，我们发现，在没有扩容的情况下，ArrayList 的效率要高于LinkedList。这是因为 ArrayList 在添加元素到尾部的时候，不需要复制重排数据，效率非常高。而 LinkedList 虽然也不用循环查找元素，但 LinkedList 中多了 new 对象以及变换指针指向对象的过程，所以效率要低于 ArrayList。
- for迭代遍历
  - 当再次遍历时，会先调用内部类iteator中的hasNext(),再调用next(),在调用next()方法时，会对modCount和expectedModCount进行比较（checkForComodification()），此时两者不一致，就抛出了ConcurrentModificationException异常
  - 在foreach循环中调用list中的remove()方法，会走到fastRemove()方法，该方法不是iterator中的方法，而是ArrayList中的方法，在该方法只做了modCount++，而没有同步到expectedModCount。
  - 在iterator中的remove方法中，删除元素实际上调用的就是list.remove()方法，但是它多了一个操作：expectedModCount = modCount;
- 为什么 ArrayList 不像 HashMap 一样在扩容时需要一个负载因子
  - HashMap有负载因子是既要考虑数组太短，因哈希冲突导致链表过长而导致查询性能下降，也考虑了数组过长，新增数据时性能下降。这个负载因子是综合了数组和链表两者的长度，不能太大也不能太小。而ArrayList不需要这种考虑。

## 集合类的坑

- 使用 Arrays.asList 把数据转换为 List 的三个坑

  - 第一个坑是，不能直接使用 Arrays.asList 来转换基本类型数组

    - ```java
      int[] arr1 = {1, 2, 3};
      List list1 = Arrays.stream(arr1).boxed().collect(Collectors.toList());
      ```

  - 第二个坑，Arrays.asList 返回的 List 不支持增删操作

    - Arrays.asList 返回的 List 并不是我们期望的 java.util.ArrayList，而是 Arrays 的内部类 ArrayList。ArrayList 内部类继承自AbstractList 类，并没有覆写父类的 add 方法，而父类中 add 方法的实现，就是抛出UnsupportedOperationException

  - 第三个坑，对原始数组的修改会影响到我们获得的那个 List

    - 修复方式比较简单，重新 new 一个 ArrayList 初始化 Arrays.asList 返回的 List 即可

- 使用 List.subList 进行切片操作会导致 OOM

  - List.subList 返回的子List 不是一个普通的 ArrayList。这个子 List 可以认为是原始 List 的视图，会和原始 List 相互影响。如果不注意，很可能会因此产生 OOM 问题。
  - 既然 SubList 相当于原始 List 的视图，那么避免相互影响的修复方式有两种
    - 一种是，不直接使用 subList 方法返回的 SubList，而是重新使用 new ArrayList，在构造方法传入 SubList，来构建一个独立的 ArrayList；
    - 另一种是，对于 Java 8 使用 Stream 的 skip 和 limit API 来跳过流中的元素，以及限制流中元素的个数，同样可以达到 SubList 切片的目的。

- 一定要让合适的数据结构做合适的事情

  - 第一个误区是，使用数据结构不考虑平衡时间和空间。
    - ArrayList 在内存占用上性价比很高
    - 要对大 List 进行单值搜索的话，可以考虑使用 HashMap，其中 Key 是要搜索的值，Value 是原始对象，会比使用 ArrayList 有非常明显的性能优势。
  - 第二个误区是，过于迷信教科书的大 O 时间复杂度
    - 对于数组，随机元素访问的时间复杂度是 O(1)，元素插入操作是 O(n)； 
    - 对于链表，随机元素访问的时间复杂度是 O(n)，元素插入操作是 O(1)。 
    - 在大量的元素插入、很少的随机访问的业务场景下，
      - 在随机访问方面，我们看到了 ArrayList 的绝对优势，
      - 随机插入操作居然也是 LinkedList 落败
    - 翻看 LinkedList 源码发现，插入操作的时间复杂度是 O(1) 的前提是，你已经有了那个要插入节点的指针。但，在实现的时候，我们需要先通过循环获取到那个节点的 Node，然后再执行插入操作。前者也是有开销的，不可能只考虑插入操作本身的代价

- 与 ArrayList 在删除元素方面的坑有关

  - 调用类型是 Integer 的 ArrayList 的 remove 方法删除元素，传入一个 Integer 包装类的数字和传入一个 int 基本类型的数字，结果一样吗？
    - remove包装类数字是删除对象，基本类型的int数字是删除下标。



## Hashtable

- 函数都是同步的，意味着它是线程安全的
- Hashtable的主要对外接口
  - 根据entrySet()获取Hashtable的“键值对”的Set集合
  - 根据keySet()获取Hashtable的“键”的Set集合。
  - 根据value()获取Hashtable的“值”的集合。
  - 根据keys()获取Hashtable“键”的Set集合。
  - 根据elements()获取Hashtable“值”的集合。





## HashMap

- 介绍

  - 根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 
  - HashMap最多只允许一条记录的键为null，允许多条记录的值为null。
  - HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。

- HashMap的属性

  - loadFactor加载因子

    - 计算HashMap的实时装载因子的方法为：size/capacity，而不是占用桶的数量去除以capacity
    - 在hashMap中loadFactor的初始值就是0.75，一般情况下不需要更改它

  - 桶

    - 数组中每一个位置上都放有一个桶，每个桶里就是装一个链表，链表中可以有很多个元素(entry)，这就是桶的意思。也就相当于把元素都放在桶中。

  - capacity

    - capacity译为容量代表的数组的容量，也就是数组的长度，同时也是HashMap中桶的个数，默认值是16

  - size

    - size就是在该HashMap的实例中实际存储的元素的个数

  - threshold

    - 当哈希表中条目数超出了当前容量*加载因子threshold = capacity * loadFactor(其实就是HashMap的实际容量)时，则对该哈希表进行rehash操作，将哈希表扩充至两倍的桶数。
    - 什么时候会扩增数组的大小？在put一个元素时先size>=threshold并且还要在对应数组位置上有元素，这才能扩增数组

  - 通过key的值来计算entry的hash值，通过entry的hash值和数组的长度length来计算出entry放在数组中的哪个位置上面

    - 用hash算法求得这个位置(三步：取key的hashCode值、高位运算、取模运算)

    - ```java
      // 方法一：
      // 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的
      static final int hash(Object key) { //jdk1.8 & jdk1.7
      int h;
      // h = key.hashCode() 为第一步 取hashCode值
      // h ^ (h >>> 16) 为第二步 高位参与运算
      return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
      
      }
      
      // 方法二：
      static int indexFor(int h, int length) { //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的
      // 它通过h & (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h& (length-1)运算等价于对length取模，也就是h%length，但是&比%具有更高的效率
          return h & (length-1); //第三步 取模运算
      
      }
      
      
      ```

- hashmap为什么是二倍扩容

  - 容量n为2的幂次方，n-1的二进制会全为1，位运算时可以充分散列，避免不必要的哈希冲突。所以扩容必须2倍就是为了维持容量始终为2的幂次方。

    > /*
    > HashMap的容量为16转化成二进制为10000，length-1得出的二进制为01111 
    >
    > h & (length-1)
    > 哈希值为1111 
    >
    > 
    >
    > 可以得出索引的位置为15
    >
    > 假设 
    >
    > HashMap的容量为15转化成二进制为1111，length-1得出的二进制为1110 
    >
    > h & (length-1)
    > 哈希值为1111和1110 
    >
    > 
    >
    > 那么两个索引的位置都是14，就会造成分布不均匀了，
    >
    > 增加了碰撞的几率，
    >
    > 减慢了查询的效率，
    >
    > 造成空间的浪费。 
    >
    > 总结：
    >
    > 因为2的幂-1都是11111结尾的，所以碰撞几率小。
    >
    > */

- put方法

  ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容；

  ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③；

  ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals；

  ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤；

  ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可；

  ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。

- resize方法

  - resize用于以下两种情况之一
    - 初始化table
      - 初始化HashMap时,  如果构造函数没有指定initialCapacity, 则table大小为16;
      - 如果构造函数指定了initialCapacity, 则table大小为threshold, 即大于指定initialCapacity的最小的2的整数次幂
    - 在table大小超过threshold之后进行扩容
  - resize时的链表拆分
    - 明确三点
      - oldCap一定是2的整数次幂, 这里假设是2^m
      - newCap是oldCap的两倍, 则会是2^(m+1)
      - hash对数组大小取模`(n - 1) & hash` 其实就是取hash的低`m`位
        假设 oldCap = 16, 即 2^4, 
          		则 `(16-1) & hash` 自然就是取hash值的低4位,我们假设它为 `abcd`.
    - oldCap扩大两倍后, 新的index的位置就变成了 `(32-1) & hash`, 其实就是取 hash值的低5位
    - 新旧index是否一致就体现在hash值的第4位(我们把最低为称作第0位)
      - 如果 `(e.hash & oldCap) == 0` 则该节点在新表的下标位置与旧表一致都为 `j`  
      - 如果 `(e.hash & oldCap) == 1` 则该节点在新表的下标位置 `j + oldCap` 
      - 根据这个条件, 我们将原位置的链表拆分成两个链表, 然后一次性将整个链表放到新的Table对应的位置上.

- HashMap 的初始化

  - **当初始化是构造函数指定 1w 时，后续我们立即存入 1w 条数据，是否符合与其不会触发扩容呢**

- 在 HashMap 中，提供了一个指定初始容量的构造方法 `HashMap(int initialCapacity)`，这个方法最终会调用到 HashMap 另一个构造方法，其中的参数 loadFactor 就是默认值 0.75f。

  - 其中的成员变量 `threshold` 就是用来存储，触发 HashMap 扩容的阈值，也就是说，当 HashMap 存储的数据量达到 `threshold` 时，就会触发扩容。
  - 从构造方法的逻辑可以看出，HashMap 并不是直接使用外部传递进来的 `initialCapacity`，而是经过了 `tableSizeFor()` 方法的处理，再赋值到 `threshole` 上。
    - 在 `tableSizeFor()` 方法中，通过逐步位运算，就可以让返回值，保持在 2 的 N 次幂。以方便在扩容的时候，快速计算数据在扩容后的新表中的位置。
    - 那么当我们从外部传递进来 1w 时，实际上经过 `tableSizeFor()` 方法处理之后，就会变成 2 的 14 次幂 16384，再算上负载因子 0.75f，实际在不触发扩容的前提下，可存储的数据容量是 12288（16384 * 0.75f）。
    - 这种场景下，用来存放 1w 条数据，绰绰有余了，并不会触发我们猜想的扩容。
  - 当我们把初始容量，调整到 1000 时
    - 在 HashMap 中，动态扩容的逻辑在 `resize()` 方法中。这个方法不仅仅承担了 table 的扩容，它还承担了 table 的初始化。
      - **table.size == threshold \* loadFactor**
      - 上边的put方法、resize方法。
        - 在 `resize()` 方法中，调整了最终 `threshold` 值，以及完成了 table 的初始化。
      - 虽然 HashMap 初始容量指定为 1000，会被 `tableSizeFor()` 调整为 1024，但是它只是表示 table 数组为 1024，扩容的重要依据扩容阈值会在 `resize()` 中调整为 768（1024 * 0.75）。
      - 它是不足以承载 1000 条数据的，最终在存够 1k 条数据之前，还会触发一次动态扩容。
  - 通常在初始化 HashMap 时，初始容量都是根据业务来的，而不会是一个固定值，为此我们需要有一个特殊处理的方式，就是将预期的初始容量，再除以 HashMap 的装载因子，默认时就是除以 0.75。
    - 例如想要用 HashMap 存放 1k 条数据，应该设置 1000 / 0.75，实际传递进去的值是 1333，然后会被 `tableSizeFor()` 方法调整到 2048，足够存储数据而不会触发扩容。
    - 当想用 HashMap 存放 1w 条数据时，依然设置 10000 / 0.75，实际传递进去的值是 13333，会被调整到 16384，和我们直接传递 10000 效果是一样的。
  - **总结**：
    1. HashMap 构造方法传递的 initialCapacity，虽然在处理后被存入了 loadFactor 中，但它实际表示 table 的容量。
    2. 构造方法传递的 initialCapacity，最终会被 `tableSizeFor()` 方法动态调整为 2 的 N 次幂，以方便在扩容的时候，计算数据在 newTable 中的位置。
    3. 如果设置了 table 的初始容量，会在初始化 table 时，将扩容阈值 threshold 重新调整为 table.size * loadFactor。
    4. **HashMap 是否扩容，由 threshold 决定，而 threshold 又由初始容量和 loadFactor 决定。**
    5. 如果我们预先知道 HashMap 数据量范围，可以预设 HashMap 的容量值来提升效率，但是需要注意要考虑装载因子的影响，才能保证不会触发预期之外的动态扩容。

- 头插法导致的死循环

  - 老版本HashMap扩容代码，其中，重点在于transfer()

  - ```java
    void transfer(Entry[] newTable)
    {
    　　//复制一个原数组src，Entry是一个静态内部类，有K，V，next三个成员变量
        Entry[] src = table;
    　　//数组新容量
        int newCapacity = newTable.length;：
        //  从OldTable里摘一个元素出来，然后放到NewTable中
        for (int j = 0; j < src.length; j++) {
            Entry<K,V> e = src[j];//取出原数组一个元素
            if (e != null) {//判断原数组该位置有元素
                src[j] = null;//原数组位置置为空
                do {//对原数组某一位置下的一串元素进行操作
                    Entry<K,V> next = e.next;//next是当前元素下一个
                    int i = indexFor(e.hash, newCapacity);//i是元素在新数组的位置
                    e.next = newTable[i];//此处体现了头插法，当前元素的下一个是新数组的头元素
                    newTable[i] = e;//将原数组元素加入新数组
                    e = next;//遍历到原数组某一位置下的一串元素的下一个
    　　　　　　} while (e != null); 
    　　　　} 
    　　} 
    }
    ```

    

    - 接下来图示**单线程**情况下，do循环内的情况：

      初始：当前数组容量为2，有三个元素3、7、5，此处的hash算法是简化处理（对容量取模）。因此，3、7、5都在数组索引1对应的链表上。

      　　扩容新容量为2*2=4。

      　　第一步：当前Entry e对应3，next对应7，新位置i为3，然后将3插入新数组对应位置。

      　　第二步：当前Entry e对应7，next对应5，新位置i为3，然后将新数组对应索引处的元素3添加到7的尾巴后（头插），然后将7插入新数组对应位置。

      　　第三步：当前Entry e对应5，next对应null，新位置i为1, 然后将5插入新数组对应位置。

      ![img](https://img2020.cnblogs.com/blog/1384439/202006/1384439-20200606103906885-46047020.png)

       

       

    - 接下来图示多线程情况下死循环场景：初始条件相同。

      如果有两个线程：

      　　　　线程一执行到 Entry<K,V> next = e.next; 便挂起了，即此时Entry e是3，next是7，3是在7前面的。

      　　　　线程二执行完成。

      　　此时如下图所示，线程一的3的next是7，而线程二的7的next是3。（此处是Entry里的next成员变量，在多个线程中相同Entry不冲突）。此时可以看出出现了死循环问题。

      ![img](https://img2020.cnblogs.com/blog/1384439/202006/1384439-20200606111846055-839726988.png)

       

       

       　　如果此时线程一继续往下执行：

       　　第一步：当前Entry e对应3，next对应7，新位置i为3，然后将3插入新数组对应位置。

      　　 第二步：当前Entry e对应7，next对应3（单线程情况下是5），新位置i为3，然后将7插入新数组对应位置。

      　　 第三步：当前Entry e对应3，next对应7，此处死循环，永远不会跳出while循环。

       ![img](https://img2020.cnblogs.com/blog/1384439/202006/1384439-20200606120547060-2090940084.png)

       

       

      总结归纳：多线程情况下，使用头插法会导致链表节点之间的关系混乱，出现倒排现象，例如原本3->7->5变成7->3，其他线程此时再进行扩容是会出现死循环。 

  - 线程A先执行，执行完Entry<K,V> next = e.next;这行代码后挂起，然后线程B完整的执行完整个扩容流程，接着线程A唤醒，继续之前的往下执行，当while循环执行3次后会形成环形链表

  - newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置

  - jdk1.8在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了

    - 我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置
    - 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点
    - 减少哈希冲突，均匀分布元素

- HashMap在jdk1.8之后引入了红黑树的概念，表示若桶中链表元素超过8时，会自动转化成红黑树；若桶中元素小于等于6时，树结构还原成链表形式。

  原因：

  　　红黑树的平均查找长度是log(n)，长度为8，查找长度为log(8)=3，链表的平均查找长度为n/2，当长度为8时，平均查找长度为8/2=4，这才有转换成树的必要；链表长度如果是小于等于6，6/2=3，虽然速度也很快的，但是转化为树结构和生成树的时间并不会太短。

  还有选择6和8的原因是：

  　　中间有个差值7可以防止链表和树之间频繁的转换。假设一下，如果设计成链表个数超过8则链表转换成树结构，链表个数小于8则树结构转换成链表，如果一个HashMap不停的插入、删除元素，链表个数在8左右徘徊，就会频繁的发生树转链表、链表转树，效率会很低。



## TreeMap

- TreeMap实现了SotredMap接口，它是有序的集合。而且是一个红黑树结构，每个key-value都作为一个红黑树的节点

- 如果在调用TreeMap的构造函数时没有指定比较器，则根据key执行自然排序

  - 自然排序：TreeMap的所有key必须实现Comparable接口，所有的key都是同一个类的对象
  - 定制排序：创建TreeMap对象传入了一个Comparator对象，该对象负责对TreeMap中所有的key进行排序，采用定制排序不要求Map的key实现Comparable接口
  - 默认排序方式：对key升序排序。

- **二叉-查找树**

  - 概念
    - 若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值；
    - 若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值；
    - 任意节点的左、右子树也分别为二叉查找树；
    - 没有键值相等的节点。
  - 对二叉查找树进行中序遍历，即可得到有序的数列
  - 删除操作
    - 如果删除的是叶节点，可以直接删除；
    - 如果被删除的元素有一个子节点，可以将子节点直接移到被删除元素的位置；
    - 如果有两个子节点，这时候就采用中序遍历，找到待删除的节点的后继节点，将其与待删除的节点互换，此时待删除节点的位置已经是叶子节点，可以直接删除

- **平衡二叉树**

  - 它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树
  - 在构建一棵平衡二叉树的过程中，当有新的节点要插入时，检查是否因插入后而破坏了树的平衡，如果是，则需要做旋转去改变树的结构
  - 旋转
    - 旋转的目的都是将节点多的一支出让节点给另一个节点少的一支
    - 左旋
      - 将节点的右支往左拉，右子节点变成父节点，并把晋升之后多余的左子节点出让给降级节点的右子节点
    - 右旋
      - 将节点的左支往右拉，左子节点变成了父节点，并把晋升之后多余的右子节点出让给降级节点的左子节点
  - 插入
    - 在节点的左子树的左子树下，有新节点插入
      - 只需要对节点进行右旋即可
    - 在节点的左子树的右子树下，有新节点插入
      - 第一次旋转，将左右先调整成左左
      - 然后再对左左进行调整，从而使得二叉树平衡
      - 先左旋再右旋
    - 左右跟右左互为镜像，左左跟右右也互为镜像

- **平衡二叉树（AVL树）**

- 平衡二叉树(Balance Binary Tree)又称AVL树。它或者是一颗空树，或者是具有下列性质的二叉树：它的左子树和右子树都是平衡二叉树，且左子树和右子树的深度之差的绝对值不超过1。

- **最小生成树**

  - 关于图的几个概念定义：

    - **连通图**：在无向图中，若任意两个顶点vivi与vjvj都有路径相通，则称该无向图为连通图。
    - **强连通图**：在有向图中，若任意两个顶点vivi与vjvj都有路径相通，则称该有向图为强连通图。
    - **连通网**：在连通图中，若图的边具有一定的意义，每一条边都对应着一个数，称为权；权代表着连接连个顶点的代价，称这种连通图叫做连通网。
    - **生成树**：一个连通图的生成树是指一个连通子图，它含有图中全部n个顶点，但只有足以构成一棵树的n-1条边。一颗有n个顶点的生成树有且仅有n-1条边，如果生成树中再添加一条边，则必定成环。
    - **最小生成树**：在连通网的所有生成树中，所有边的代价和最小的生成树，称为最小生成树。
      ![这里写图片描述](https://img-blog.csdn.net/20160714130435508)

    ------

    下面介绍两种求最小生成树算法

    ## **1.Kruskal算法**

    此算法可以称为“加边法”，初始最小生成树边数为0，每迭代一次就选择一条满足条件的最小代价边，加入到最小生成树的边集合里。
    \1. 把图中的所有边按代价从小到大排序；
    \2. 把图中的n个顶点看成独立的n棵树组成的森林；
    \3. 按权值从小到大选择边，所选的边连接的两个顶点ui,viui,vi,应属于两颗不同的树，则成为最小生成树的一条边，并将这两颗树合并作为一颗树。
    \4. 重复(3),直到所有顶点都在一颗树内或者有n-1条边为止。

    ![这里写图片描述](https://img-blog.csdn.net/20160714144315409)

    ## **2.Prim算法**

    此算法可以称为“加点法”，每次迭代选择代价最小的边对应的点，加入到最小生成树中。算法从某一个顶点s开始，逐渐长大覆盖整个连通网的所有顶点。

    1. 图的所有顶点集合为VV；初始令集合u={s},v=V−uu={s},v=V−u;
    2. 在两个集合u,vu,v能够组成的边中，选择一条代价最小的边(u0,v0)(u0,v0)，加入到最小生成树中，并把v0v0并入到集合u中。
    3. 重复上述步骤，直到最小生成树有n-1条边或者n个顶点为止。

    由于不断向集合u中加点，所以最小代价边必须同步更新；需要建立一个辅助数组closedge,用来维护集合v中每个顶点与集合u中最小代价边信息，：

    ```
    struct
    {
      char vertexData   //表示u中顶点信息
      UINT lowestcost   //最小代价
    }closedge[vexCounts]12345
    ```

    ![这里写图片描述](https://img-blog.csdn.net/20160714161107576)

    ------

    ## **3.完整代码**

    ```c
    #include <iostream>
    #include <vector>
    #include <queue>
    #include <algorithm>
    using namespace std;
    #define INFINITE 0xFFFFFFFF   
    #define VertexData unsigned int  //顶点数据
    #define UINT  unsigned int
    #define vexCounts 6  //顶点数量
    char vextex[] = { 'A', 'B', 'C', 'D', 'E', 'F' };
    struct node 
    {
        VertexData data;
        unsigned int lowestcost;
    }closedge[vexCounts]; //Prim算法中的辅助信息
    typedef struct 
    {
        VertexData u;
        VertexData v;
        unsigned int cost;  //边的代价
    }Arc;  //原始图的边信息
    void AdjMatrix(unsigned int adjMat[][vexCounts])  //邻接矩阵表示法
    {
        for (int i = 0; i < vexCounts; i++)   //初始化邻接矩阵
            for (int j = 0; j < vexCounts; j++)
            {
                adjMat[i][j] = INFINITE;
            }
        adjMat[0][1] = 6; adjMat[0][2] = 1; adjMat[0][3] = 5;
        adjMat[1][0] = 6; adjMat[1][2] = 5; adjMat[1][4] = 3;
        adjMat[2][0] = 1; adjMat[2][1] = 5; adjMat[2][3] = 5; adjMat[2][4] = 6; adjMat[2][5] = 4;
        adjMat[3][0] = 5; adjMat[3][2] = 5; adjMat[3][5] = 2;
        adjMat[4][1] = 3; adjMat[4][2] = 6; adjMat[4][5] = 6;
        adjMat[5][2] = 4; adjMat[5][3] = 2; adjMat[5][4] = 6;
    }
    int Minmum(struct node * closedge)  //返回最小代价边
    {
        unsigned int min = INFINITE;
        int index = -1;
        for (int i = 0; i < vexCounts;i++)
        {
            if (closedge[i].lowestcost < min && closedge[i].lowestcost !=0)
            {
                min = closedge[i].lowestcost;
                index = i;
            }
        }
        return index;
    }
    void MiniSpanTree_Prim(unsigned int adjMat[][vexCounts], VertexData s)
    {
        for (int i = 0; i < vexCounts;i++)
        {
            closedge[i].lowestcost = INFINITE;
        }      
        closedge[s].data = s;      //从顶点s开始
        closedge[s].lowestcost = 0;
        for (int i = 0; i < vexCounts;i++)  //初始化辅助数组
        {
            if (i != s)
            {
                closedge[i].data = s;
                closedge[i].lowestcost = adjMat[s][i];
            }
        }
        for (int e = 1; e <= vexCounts -1; e++)  //n-1条边时退出
        {
            int k = Minmum(closedge);  //选择最小代价边
            cout << vextex[closedge[k].data] << "--" << vextex[k] << endl;//加入到最小生成树
            closedge[k].lowestcost = 0; //代价置为0
            for (int i = 0; i < vexCounts;i++)  //更新v中顶点最小代价边信息
            {
                if ( adjMat[k][i] < closedge[i].lowestcost)
                {
                    closedge[i].data = k;
                    closedge[i].lowestcost = adjMat[k][i];
                }
            }
        }
    }
    void ReadArc(unsigned int  adjMat[][vexCounts],vector<Arc> &vertexArc) //保存图的边代价信息
    {
        Arc * temp = NULL;
        for (unsigned int i = 0; i < vexCounts;i++)
        {
            for (unsigned int j = 0; j < i; j++)
            {
                if (adjMat[i][j]!=INFINITE)
                {
                    temp = new Arc;
                    temp->u = i;
                    temp->v = j;
                    temp->cost = adjMat[i][j];
                    vertexArc.push_back(*temp);
                }
            }
        }
    }
    bool compare(Arc  A, Arc  B)
    {
        return A.cost < B.cost ? true : false;
    }
    bool FindTree(VertexData u, VertexData v,vector<vector<VertexData> > &Tree)
    {
        unsigned int index_u = INFINITE;
        unsigned int index_v = INFINITE;
        for (unsigned int i = 0; i < Tree.size();i++)  //检查u,v分别属于哪颗树
        {
            if (find(Tree[i].begin(), Tree[i].end(), u) != Tree[i].end())
                index_u = i;
            if (find(Tree[i].begin(), Tree[i].end(), v) != Tree[i].end())
                index_v = i;
        }
    
        if (index_u != index_v)   //u,v不在一颗树上，合并两颗树
        {
            for (unsigned int i = 0; i < Tree[index_v].size();i++)
            {
                Tree[index_u].push_back(Tree[index_v][i]);
            }
            Tree[index_v].clear();
            return true;
        }
        return false;
    }
    void MiniSpanTree_Kruskal(unsigned int adjMat[][vexCounts])
    {
        vector<Arc> vertexArc;
        ReadArc(adjMat, vertexArc);//读取边信息
        sort(vertexArc.begin(), vertexArc.end(), compare);//边按从小到大排序
        vector<vector<VertexData> > Tree(vexCounts); //6棵独立树
        for (unsigned int i = 0; i < vexCounts; i++)
        {
            Tree[i].push_back(i);  //初始化6棵独立树的信息
        }
        for (unsigned int i = 0; i < vertexArc.size(); i++)//依次从小到大取最小代价边
        {
            VertexData u = vertexArc[i].u;  
            VertexData v = vertexArc[i].v;
            if (FindTree(u, v, Tree))//检查此边的两个顶点是否在一颗树内
            {
                cout << vextex[u] << "---" << vextex[v] << endl;//把此边加入到最小生成树中
            }   
        }
    }
    
    int main()
    {
        unsigned int  adjMat[vexCounts][vexCounts] = { 0 };
        AdjMatrix(adjMat);   //邻接矩阵
        cout << "Prim :" << endl;
        MiniSpanTree_Prim(adjMat,0); //Prim算法，从顶点0开始.
        cout << "-------------" << endl << "Kruskal:" << endl;
        MiniSpanTree_Kruskal(adjMat);//Kruskal算法
        return 0;
    }
    ```





## 从2-3树到 红黑树

- 红黑树的基本思想是用标准的二叉查找树（完全由2-结点构成）和一些额外的信息（替换3-结点）来表示2-3树。
  - 2-结点：含有一个键(及值)和两条链接，左链接指向的2-3树中的键都小于该结点，右链接指向的2-3树中的键都大于该结点。
  - 3-结点：含有两个键(及值)和三条链接，左链接指向的2-3树中的键都小于该结点，中链接指向的2-3树中的键都位于该结点的两个键之间，右链接指向的2-3树中的键都大于该结点。

## 红黑树

- 红黑树之所以难是难在它是自平衡的二叉查找树，在进行插入和删除等可能会破坏树的平衡的操作时，需要重新自处理达到平衡状态。

- 定义和性质

  - 每个节点要么是黑色，要么是红色。
    - 根节点是黑色。
    - 每个叶子节点(Nil)是黑色。
    - 每个红色结点的两个子结点一定都是黑色。
    - 任意一结点到每个叶子结点的路径都包含数量相同的黑结点。
      左子树和右子树的黑结点的层数是相等的
    - 从性质 5 又可以推出：如果一个结点存在黑子结点，那么该结点肯定有两个子结点。
      - ![img](http://s2.51cto.com/oss/201901/22/c7d65c349191f5e5a7efdb6fc33f0ff6.jpg)

- 时间复杂度

- 最坏的情况下也可以保证O(logN)的，这是要好于二叉查找树的。因为二叉查找树最坏情况可以让查找达到O(N)。

  - 红黑树是牺牲了严格的高度平衡的优越条件为代价，它只要求部分地达到平衡要求，降低了对旋转的要求，从而提高了性能。能够以O(log2 n)的时间复杂度进行搜索、插入、删除操作。由于它的设计，任何不平衡都会在三次旋转之内解决

- 红黑树操作

  - 红黑树能自平衡，它靠的是三种操作：
    - 左旋：以某个结点作为支点(旋转结点)，其右子结点变为旋转结点的父结点，右子结点的左子结点变为旋转结点的右子结点，左子结点保持不变。
    - 右旋：以某个结点作为支点(旋转结点)，其左子结点变为旋转结点的父结点，左子结点的右子结点变为旋转结点的左子结点，右子结点保持不变。
    - 变色：结点的颜色由红变黑或由黑变红。
    - 旋转操作不会影响旋转结点的父结点，父结点以上的结构还是保持不变的。
    - 但要保持红黑树的性质，结点不能乱挪，还得靠变色了
  - 红黑树查找
    - ![img](http://s2.51cto.com/oss/201901/22/995e3eb18c0ecd0d25ec4d4f995cadb8.jpg)
    - 简单不代表它效率不好。正由于红黑树总保持黑色完美平衡，所以它的查找最坏时间复杂度为 O(2lgN)，也即整颗树刚好红黑相隔的时候。
    - 能有这么好的查找效率得益于红黑树自平衡的特性，而这背后的付出，红黑树的插入操作功不可没。
  - 红黑树插入
    - 只表示一半的情景，另一半为相反操作
    - ![img](http://s3.51cto.com/oss/201901/22/d3a336a58aad16c18d9a198b367c037d.jpg)
    - 插入操作包括两部分工作：一是查找插入的位置;二是插入后自平衡查找插入的父结点很简单，跟查找操作区别不大：
      - 插入结点红色
        - 红色在父结点(如果存在)为黑色结点时，红黑树的黑色平衡没被破坏，不需要做自平衡操作
      - 如果插入结点是黑色，那么插入位置所在的子树黑色结点总是多 1，必须做自平衡
    - I 表示插入结点，P 表示插入结点的父结点，S 表示插入结点的叔叔结点，PP 表示插入结点的祖父结点。
    - 情景 1：红黑树为空树
      - 把插入结点作为根结点，并把结点设置为黑色。
    - 情景 2：插入结点的 Key 已存在
      - 把 I 设为当前结点的颜色，更新当前结点的值为插入结点的值。
    - 情景 3：插入结点的父结点为黑结点
      - 直接插入
    - 情景 4：插入结点的父结点为红结点
      - 回想下红黑树的性质  2：根结点是黑色。如果插入的父结点为红结点，那么该父结点不可能为根结点，所以插入结点总是存在祖父结点
      - 插入情景 4.1：叔叔结点存在并且为红结点
        - ![img](http://s4.51cto.com/oss/201901/22/ab9a8511442810355d23d47052036c11.jpg)
        - 处理：将 P 和 S 设置为黑色，将 PP 设置为红色，把 PP 设置为当前插入结点。
        - 如果 PP 的父结点是红色，根据性质 4，此时红黑树已不平衡了，所以还需要把 PP 当作新的插入结点，继续做插入操作自平衡处理，直到平衡为止
        - PP 刚好为根结点时，那么根据性质 2，我们必须把 PP 重新设为黑色，那么树的红黑结构变为：黑黑红
          - 从根结点到叶子结点的路径中，黑色结点增加了。这也是唯一一种会增加红黑树黑色结点层数的插入情景
          - 总结出另外一个经验：红黑树的生长是自底向上的。这点不同于普通的二叉查找树，普通的二叉查找树的生长是自顶向下的
      - 插入情景 4.2：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的左子结点。
        - 插入情景 4.2.1：插入结点是其父结点的左子结点。
          - 将 P 设为黑色，将 PP 设为红色，对 PP 进行右旋。
          - ![img](http://s2.51cto.com/oss/201901/22/1babcc6b055fd252e6f553688ec5d770.jpg)
        - 插入情景 4.2.2：插入结点是其父结点的右子结点
          - 对 P 进行左旋，把 P 设置为插入结点，得到情景 4.2.1，进行情景 4.2.1 的处理
          - ![img](http://s2.51cto.com/oss/201901/22/42bacde6f4c67cca3f7c83d356a146ab.jpg)
  - 红黑树删除情景
    - 红黑树的删除操作也包括两部分工作：一是查找目标结点;二是删除后自平衡
    - 前继和后继结点的直观的方法
      - 若删除结点无子结点，直接删除。
      - 若删除结点只有一个子结点，用子结点替换删除结点。
        - 情景 2：删除结点用其唯一的子结点替换，子结点替换为删除结点后，可以认为删除的是子结点，若子结点又有两个子结点，那么相当于转换为情景  3，一直自顶向下转换，总是能转换为情景 1。
      - 若删除结点有两个子结点，用后继结点(大于删除结点的最小结点)替换删除结点。
        - 情景 3：删除结点用后继结点(肯定不存在左结点)，如果后继结点有右子结点，那么相当于转换为情景 2，否则转为情景 1
    - 删除操作结点的叫法约定
      - R  是即将被替换到删除结点的位置的替代结点，在删除前，它还在原来所在位置参与树的子平衡，平衡后再替换到删除结点的位置，才算删除完成
      - ![img](http://s4.51cto.com/oss/201901/22/d36a4e855a65bbdf2d5ec07b9b2dd440.jpg)
    - 删除情景 1：替换结点是红色结点
      - 颜色变为删除结点的颜色。
        - 将 S 设为黑色，将 P 设为红色，对 P 进行左旋，得到情景 2.1.2.3，进行情景 2.1.2.3 的处理
    - 删除情景 2：替换结点是黑结点。
      - 删除情景 2.1：替换结点是其父结点的左子结点
        - 删除情景 2.1.1：替换结点的兄弟结点是红结点
        - 删除情景 2.1.2：替换结点的兄弟结点是黑结点。
          - 父结点和子结点的具体颜色也无法确定
          - 删除情景 2.1.2.1：替换结点的兄弟结点的右子结点是红结点，左子结点任意颜色。
            - 将 S 的颜色设为 P 的颜色，将 P 设为黑色，将 SR 设为黑色，对 P 进行左旋。
            - ![img](http://s2.51cto.com/oss/201901/22/df57bc49670d2b35ef933d6ffb2d04f7.jpg)
          - 删除情景 2.1.2.2：替换结点的兄弟结点的右子结点为黑结点，左子结点为红结点
            - 将 S 设为红色，将 SL 设为黑色，对 S 进行右旋，得到情景 2.1.2.1，进行情景 2.1.2.1 的处理
            - ![img](http://s4.51cto.com/oss/201901/22/39b77c7e02f8824eb4aaa9bb355c7b2f.jpg)
          - 删除情景 2.1.2.3：替换结点的兄弟结点的子结点都为黑结点。
            - 将 S 设为红色，把 P 作为新的替换结点，重新进行删除结点情景处理
            - 这种情景我们把兄弟结点设为红色，再把父结点当作替代结点，自底向上处理，去找父结点的兄弟结点去“借”。
            - 为什么需要把兄弟结点设为红色呢?显然是为了在 P 所在的子树中保证平衡(R  即将删除，少了一个黑色结点，子树也需要少一个)，后续的平衡工作交给父辈们考虑了
            - ![img](http://s5.51cto.com/oss/201901/22/8c230394c67174c0b2a6de0bb8928ca1.jpg)

- 相对于哈希表，在选择使用的时候有什么依据

  - hash查找速度会比map快，而且查找速度基本和数据量大小无关，属于常数级别;而map的查找速度是log(n)级别
  - 如果你考虑效率，特别是在元素达到一定数量级时，考虑考虑hash
  - 希望程序尽可能少消耗内存，那么一定要小心，hash可能会让你陷入尴尬，hash的构造速度较慢
  - 如果数据基本上是静态的，那么让他们待在他们能够插入，并且不影响平衡的地方会具有更好的性能
  - 如果数据完全是静态的，例如，做一个哈希表，性能可能会更好一些。
  - 需要使用动态规则的防火墙系统，使用红黑树而不是散列表被实践证明具有更好的伸缩性
  - Linux内核在管理vm_area_struct时就是采用了红黑树来维护内存块的。

- 为什么一般hashtable的桶数会取一个素数

  - ```java
    设有一个哈希函数
    H( c ) = c % N;
    当N取一个合数时，最简单的例子是取2^n，比如说取2^3=8,这时候
    H( 11100(二进制） ) = H( 28 ) = 4
    H( 10100(二进制) ) = H( 20 ）= 4
    
    这时候c的二进制第4位（从右向左数）就”失效”了，
    
    取质数，基本可以保证c的每一位都参与H( c )的运算，从而在常见应用中减小冲突几率．
    ```

  

  

- **理想情况下二叉搜索树的性能不错, 但是在极端情况下(数据有序, 或者接近有序)二叉搜索树会退化为单支树, 从而导致其操作效率很低, 时间复杂度为O(N),** 于是俄罗斯数学家提出了[平衡二叉搜索树,又称为AVL树](https://blog.csdn.net/weixin_42562387/article/details/106176426). AVL树引入了平衡因子,保证了树的平衡, 从而避免了二叉搜索树会退化为单支树的情况, 进而使得AVL操作时间复杂度为log(N), 但是AVL的应用却比较少, **因为AVL的维护,成本太高,对AVL树进行插入或者删除时, 需要不停地通过左旋,右旋操作来维持其平衡,这些操作的成本太高,甚至抵消了AVL树带来的性能提升**.

**红黑树同时借鉴了AVL树和2-3树的思想, 将AVL的完美平衡改进为局部平衡, 通过改变颜色来区分不同的结点类型, 从而降低了维护平衡的成本和实现的复杂度.**

  总结：实际应用中，若搜索的次数远远大于插入和删除，那么选择AVL，如果搜索，插入删除次数几乎差不多，应该选择RB。

- 对比

  - 都是平衡二叉树。JDK热衷使用红黑树而非AVL树。

    对比：

    1、AVL树是严格平衡的，红黑树非严格平衡，

       这点看查询效率AVL树 略好于 红黑树，但都是O(lon n)数量级

    2、AVL树添加时最多2次旋转操作达到平衡，而删除时，可能删除节点以下的所有节点都需要旋转-> O(lon n)次

       红黑树最多3次旋转可平衡。

       这点看 红黑树效率更好，更平衡

     

    综上所看

    1、如果明确读多写少场景，使用AVL树较优

    2、其他情况从性能和稳定性综合判定，优先使用红黑树

       JDK中TreeMap HashMap也是选择了红黑树而非AVL树

AVL 插入和删除并不像教科书上说的，需要回溯到根节点，两种情况下可以直接退出向上回溯：

1. 插入更新时：如果当前节点的高度没有改变，则停止向上回溯父节点。
2. 删除更新时：如果当前节点的高度没有改变，且平衡值在 [-1, 1] 区间则停止回溯。

最终结论，优化过的 avl 和 linux 的 rbtree 放在一起，avl真的和 rbtree 差不多，avl 也并不总需要回溯到根节点，虽然旋转次数多于 rbtree，但是 rbtree 保持平衡除了旋转外还有重新着色的操作，即便不旋转也在拼命的重新着色，且层数较高，1百万个节点的 rbtree 层数和 1千万个节点的 avl 相同。

所以查询，删除，插入全部放在一起来看，avl 树和 rbtree 差不多

avl树在查询频繁的系统里比红黑树更高效，原因是avl树的平均高度比红黑树更小

算法第四版中说过红黑树等于2-3树，2-节点等价于普通平衡二叉树的节点，3-节点本质上是非平衡的缓存，综合条件下，增删操作差不多时，数据随机性强时，3-节点的非平衡性缓冲效果越明显。因此红黑树的综合性能更优，本质上是用空间换时间。

- B树大量应用在数据库和文件系统当中。
  - 红黑树往往出现由于树的深度过大而造成磁盘IO读写过于频繁，进而导致效率低下的情况在数据较小，可以完全放到内存中时，红黑树的时间复杂度比B树低。
  - 如linux中进程的调度用的是红黑树。
    反之，数据量较大，外存中占主要部分时，B树因其读磁盘次数少，而具有更快的速度。


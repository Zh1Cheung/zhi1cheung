---
title: Spark核心编程进阶（二）
categories:
- Spark
- Big Data
tags:
- Spark



---

## Spark核心编程进阶-yarn模式下日志查看详解


### yarn模式下调试运行中的spark作业

在yarn模式下，spark作业运行相关的executor和ApplicationMaster都是运行在yarn的container中的
一个作业运行完了以后，yarn有两种方式来处理spark作业打印出的日志

### 第一种是聚合日志方式（推荐，比较常用）

这种方式的话，顾名思义，就是说，将散落在集群中各个机器上的日志，最后都给聚合起来，让我们可以统一查看
如果打开了日志聚合的选项，即yarn.log-aggregation-enable，container的日志会拷贝到hdfs上去，并从机器中删除

对于这种情况，可以使用yarn logs -applicationId <app ID>命令，来查看日志
yarn logs命令，会打印出application对应的所有container的日志出来，当然，因为日志是在hdfs上的，我们自然也可以通过hdfs的命令行来直接从hdfs中查看日志
日志在hdfs中的目录，可以通过查看yarn.nodemanager.remote-app-log-dir和yarn.nodemanager.remote-app-log-dir-suffix属性来获知

### 第二种 web ui（如果你有精力的话，可以去配一下）

日志也可以通过spark web ui来查看executor的输出日志
但是此时需要启动History Server，需要让spark history server和mapreduce history server运行着
并且在yarn-site.xml文件中，配置yarn.log.server.url属性
spark history server web ui中的log url，会将你重定向到mapreduce history server上，去查看日志

### 第三种 分散查看（通常不推荐）

如果没有打开聚合日志选项，那么日志默认就是散落在各个机器上的本次磁盘目录中的，在YARN_APP_LOGS_DIR目录下
根据hadoop版本的不同，通常在/tmp/logs目录下，或者$HADOOP_HOME/logs/userlogs目录下
如果你要查看某个container的日志，那么就得登录到那台机器上去，然后到指定的目录下去，找到那个日志文件，然后才能查看


## Spark核心编程进阶-yarn模式相关参数详解


yarn模式运行spark作业所有属性详解

    属性名称                      默认值             含义
    spark.yarn.am.memory                512m              client模式下，YARN Application Master使用的内存总量
    spark.yarn.am.cores                 1               client模式下，Application Master使用的cpu数量
    spark.driver.cores                  1               cluster模式下，driver使用的cpu core数量，driver与Application Master运行在一个进程中，所以也控制了Application Master的cpu数量
    spark.yarn.am.waitTime                100s              cluster模式下，Application Master要等待SparkContext初始化的时长; client模式下，application master等待driver来连接它的时长
    spark.yarn.submit.file.replication          hdfs副本数           作业写到hdfs上的文件的副本数量，比如工程jar，依赖jar，配置文件等，最小一定是1
    spark.yarn.preserve.staging.files         false             如果设置为true，那么在作业运行完之后，会避免工程jar等文件被删除掉
    spark.yarn.scheduler.heartbeat.interval-ms      3000              application master向resourcemanager发送心跳的间隔，单位ms
    spark.yarn.scheduler.initial-allocation.interval  200ms             application master在有pending住的container分配需求时，立即向resourcemanager发送心跳的间隔
    spark.yarn.max.executor.failures          executor数量*2，最小3      整个作业判定为失败之前，executor最大的失败次数
    spark.yarn.historyServer.address          无               spark history server的地址
    spark.yarn.dist.archives              无               每个executor都要获取并放入工作目录的archive
    spark.yarn.dist.files               无               每个executor都要放入的工作目录的文件
    spark.executor.instances              2               默认的executor数量
    spark.yarn.executor.memoryOverhead          executor内存10%         每个executor的堆外内存大小，用来存放诸如常量字符串等东西
    spark.yarn.driver.memoryOverhead          driver内存7%          同上
    spark.yarn.am.memoryOverhead            AM内存7%            同上
    spark.yarn.am.port                  随机              application master端口
    spark.yarn.jar                    无               spark jar文件的位置
    spark.yarn.access.namenodes             无               spark作业能访问的hdfs namenode地址
    spark.yarn.containerLauncherMaxThreads        25                application master能用来启动executor container的最大线程数量
    spark.yarn.am.extraJavaOptions            无               application master的jvm参数
    spark.yarn.am.extraLibraryPath            无               application master的额外库路径
    spark.yarn.maxAppAttempts                             提交spark作业最大的尝试次数
    spark.yarn.submit.waitAppCompletion         true              cluster模式下，client是否等到作业运行完再退出



## Spark核心编程进阶-spark工程打包以及spark-submit详解


### spark工程打包与spark-submit的关系

我们在eclipse编写代码，基于spark api开发自己的大数据计算和处理程序

将我们写好的spark工程打包，比如说java开发环境中，就使用maven assembly插件来打包，将第三方依赖包都打进去

jar包，生产环境中，通常是本机ssh远程连接到部署了spark客户端的linux机器上，使用scp命令将本机的jar包拷贝到远程linux机器上

然后在那个linux机器上，用spark-submit脚本，去将我们的spark工程，作为一次作业/application，提交到集群上去执行



### 打包Spark工程

要使用spark-submit提交spark应用程序，首先就必须将我们的spark工程（java/scala）打包成一个jar包

如果我们的spark工程，依赖了其他一些第三方的组件，那就必须把所有组件jar包都打包到工程中，这样才能将完整的工程代码和第三方依赖都分发到spark集群中去

所以必须创建一个assembly jar来包含你所有的代码和依赖，sbt和maven都有assembly插件的（咱们课程里的java工程，就使用了maven的assembly插件）

配置依赖的时候（比如maven工程的pom.xml），可以把Spark和Hadoop的依赖配置成provided类型的依赖，也就是说仅仅开发和编译时有效，打包时就不将这两种依赖打到jar包里去了，因为集群管理器都会提供这些依赖

打好一个assembly jar包之后（也就是你的spark应用程序工程），就可以使用spark-submit脚本提交jar包中的spark应用程序了

### spark-submit是什么？

在spark安装目录的bin目录中，有一个spark-submit脚本，这个脚本主要就是用来提交我们自己开发的spark应用程序到集群上执行
spark-submit可以通过一个统一的接口，将spark应用程序提交到所有spark支持的集群管理器上（Standalone（Master）、Yarn（ResourceManager）等）
所以我们并不需要为每种集群管理器都做特殊的配置

### --master

1、如果不设置，那么就是local模式

2、如果设置spark://打头的URL，那么就是standalone模式，会提交到指定的URL的Master进程上去

3、如果设置yarn-打头的，那么就是yarn模式，会读取hadoop配置文件，然后连接ResourceManager


## Spark核心编程进阶-spark-submit示例以及基础参数讲解

### 使用spark-submit提交spark应用

将我们的spark工程打包好之后，就可以使用spark-submit脚本提交工程中的spark应用了

spark-submit脚本会设置好spark的classpath环境变量（用于类加载）和相关的依赖，而且还可以支持多种不同的集群管理器和不同的部署模式

以下是一个spark应用提交脚本的示例，以及其基本语法
一般会将执行spark-submit脚本的命令，放置在一个自定义的shell脚本里面，所以说这是比较灵活的一种做法
建议大家还是要熟悉linux操作系统，不需要太熟悉，会基本的操作就可以了

wordcount.sh
    
    /usr/local/spark/bin/spark-submit \
    --class org.leo.spark.study.WordCount \
    --master spark://192.168.0.101:7077 \
    --deploy-mode client \
    --conf <key>=<value> \
    /usr/local/spark-study/spark-study.jar \
    ${1}
 
 
### 以下是上面的spark-submit

--class: spark应用程序对应的主类，也就是spark应用运行的主入口，通常是一个包含了main方法的java类或scala类，需要包含全限定包名，比如org.leo.spark.study.WordCount

--master: spark集群管理器的master URL，standalone模式下，就是ip地址+端口号，比如spark://192.168.0.101:7077，standalone默认端口号就是7077

--deploy-mode: 部署模式，决定了将driver进程在worker节点上启动，还是在当前本地机器上启动；默认是client模式，就是在当前本地机器上启动driver进程，如果是cluster，那么就会在worker上启动

--conf: 配置所有spark支持的配置属性，使用key=value的格式；如果value中包含了空格，那么需要将key=value包裹的双引号中

application-jar: 打包好的spark工程jar包，在当前机器上的全路径名

application-arguments: 传递给主类的main方法的参数; 在shell中用${1}这种格式获取传递给shell的参数；然后在比如java中，可以通过main方法的args[0]等参数获取

## Spark核心编程进阶-spark-submit多个示例以及常用参数详解

### 使用local本地模式，以及8个线程运行

--class 指定要执行的main类

 --master 指定集群模式，local，本地模式，local[8]，进程中用几个线程来模拟集群的执行

    ./bin/spark-submit \
      --class org.leo.spark.study.WordCount \
      --master local[8] \
      /usr/local/spark-study.jar \

###  使用standalone client模式运行
 executor-memory，指定每个executor的内存量，这里每个executor内存是2G

 total-executor-cores，指定所有executor的总cpu core数量，这里所有executor的总cpu core数量是100个

    ./bin/spark-submit \
      --class org.leo.spark.study.WordCount \
      --master spark://192.168.0.101:7077 \
      --executor-memory 2G \
      --total-executor-cores 100 \
      /usr/local/spark-study.jar \

### 使用standalone cluster模式运行

 supervise参数，指定了spark监控driver节点，如果driver挂掉，自动重启driver


    ./bin/spark-submit \
      --class org.leo.spark.study.WordCount \
      --master spark://192.168.0.101:7077 \
      --deploy-mode cluster \
      --supervise \
      --executor-memory 2G \
      --total-executor-cores 100 \
      /usr/local/spark-study.jar \

### 使用yarn-cluster模式运行

 num-executors，指定总共使用多少个executor运行spark应用

    ./bin/spark-submit \
      --class org.leo.spark.study.WordCount \
      --master yarn-cluster \  
      --executor-memory 20G \
      --num-executors 50 \
      /usr/local/spark-study.jar \

### 使用standalone client模式，运行一个python应用


    ./bin/spark-submit \
      --master spark://192.168.0.101:7077 \
      /usr/local/python-spark-wordcount.py \
    
    --class
    application jar
    --master
    --num-executors
    --executor-memory
    --total-executor-cores
    --supervise
    --executor-cores 
    --driver-memory 
    
    ./bin/spark-submit \
      --class org.leo.spark.study.WordCount \
      --master yarn-cluster \
      --num-executors 100 \
      --executor-cores 2 \
      --executor-memory 6G \
      --driver-memory  1G \
      /usr/local/spark-study.jar \
    



## Spark核心编程进阶-SparkConf、spark-submit以及spark-defaults.conf


### 默认的配置属性

spark-submit脚本会自动加载conf/spark-defaults.conf文件中的配置属性，并传递给我们的spark应用程序

加载默认的配置属性，一大好处就在于，我们不需要在spark-submit脚本中设置所有的属性

比如说，默认属性中有一个spark.master属性，所以我们的spark-submit脚本中，就不一定要显式地设置--master，默认就是local

    SparkConf.getOrElse("spark.master", "local")
    
spark配置的优先级如下:

    SparkConf、spark-submit、spark-defaults.conf
    
    spark.default.parallelism
    
    SparkConf.set("spark.default.parallelism", "100")
    spark-submit: --conf spark.default.parallelism=50
    spark-defaults.conf: spark.default.parallelism 10

如果想要了解更多关于配置属性的信息，可以在spark-submit脚本中，使用--verbose，打印详细的调试信息

使用spark-submit设置属性

虽然说SparkConf设置属性的优先级是最高的，但是有的时候咱们可能不希望在代码中硬编码一些配置属性，否则每次修改了参数以后
还得去代码里修改，然后得重新打包应用程序，再部署到生产机器上去，非常得麻烦

对于上述的情况，我们可以在代码中仅仅创建一个空的SparkConf对象，比如: val sc = new SparkContext(new SparkConf())

然后可以在spark-submit脚本中，配置各种属性的值，比如

    ./bin/spark-submit \
      --name "My app" \
      --master local[4] \
      --conf spark.shuffle.spill=false \
      --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" \
      myApp.jar

这里的spark.shuffle.spill属性，我们本来如果是在代码中，SparkConf.set("spark.shuffle.spill", "false")来配置的

此时在spark-submit中配置了，不需要更改代码，就可以更改属性，非常得方便，

尤其是对于spark程序的调优，格外方便，因为调优说白了，就是不断地调整各种各样的参数，然后反复跑反复试的过程

### spark的属性配置方式

spark-shell和spark-submit两个工具，都支持两种加载配置的方式
一种是基于命令行参数，比如上面的--master，spark-submit可以通过--conf参数，接收所有spark属性

另一种是从conf/spark-defaults.conf文件中加载，其中每一行都包括了一个key和value

比如spark.executor.memory 4g

所有在SparkConf、spark-submit和spark-defaults.conf中配置的属性，在运行的时候，都会被综合使用

直接通过SparkConf设置的属性，优先级是最高的，会覆盖其余两种方式设置的属性

其次是spark-submit脚本中通过--conf设置的属性

最后是spark-defaults.conf中设置的属性

通常来说，如果你要对所有的spark作业都生效的配置，放在spark-defaults.conf文件中，只要将spark-defaults.conf.template拷贝成那个文，然后在其中编辑即可

然后呢，对于某个spark作业比较特殊的配置，推荐放在spark-submit脚本中，用--conf配置，比较灵活

SparkConf配置属性，有什么用呢？也有用，在eclipse中用local模式执行运行的时候，那你就只能在SparkConf中设置属性了

这里还有一种特例，就是说，在新的spark版本中，可能会将一些属性的名称改变，那些旧的属性名称就变成过期的了

此时旧的属性名称还是会被接受的，但是新的属性名称会覆盖掉旧的属性名称，并且优先级是比旧属性名称更高的

举例来说

    shuffle reduce read操作的内存缓冲块
    spark 1.3.0: spark.reducer.maxMbInFlight
    spark 1.5.0: spark.reducer.maxSizeInFlight



## Spark核心编程进阶-spark-submit配置第三方依赖


### 高级依赖管理

使用spark-submit脚本提交spark application时，application jar，还有我们使用--jars命令绑定的其他jar，都会自动被发送到集群上去

### spark支持以下几种URL来指定关联的其他jar

file: 是由driver的http文件服务提供支持的，所有的executor都会通过driver的HTTP服务来拉取文件

hdfs:，http:，https:，ftp:，这种文件，就是直接根据URI，从指定的地方去拉取，比如hdfs、或者http链接、或者ftp服务器

local: 这种格式的文件必须在每个worker节点上都要存在，所以不需要通过网络io去拉取文件，这对于特别大的文件或者jar包特别适用，可以提升作业的执行性能

--jars，比如，mysql驱动包，或者是其他的一些包

文件和jar都会被拷贝到每个executor的工作目录中，这就会占用很大一片磁盘空间，因此需要在之后清理掉这些文件
在yarn上运行spark作业时，依赖文件的清理都是自动进行的
适用standalone模式，需要配置spark.worker.cleanup.appDataTtl属性，来开启自动清理依赖文件和jar包

用户还可以通过在spark-submit中，使用--packages，绑定一些maven的依赖包

此外，还可以通过--repositories来绑定过一些额外的仓库
但是说实话，这两种情况还的确不太常见

--files，比如，最典型的就是hive-site.xml配置文件

## Spark核心编程进阶-spark算子的闭包原理详解


Spark中一个非常难以理解的概念，就是在集群中分布式并行运行时操作的算子外部的变量的生命周期

通常来说，这个问题跟在RDD的算子中操作作用域外部的变量有关

所谓RDD算子中，操作作用域外部的变量，指的是，类似下面的语句: val a = 0; rdd.foreach(i -> a += i)

此时，对rdd执行的foreach算子的作用域，其实仅仅是它的内部代码，但是这里却操作了作用域外部的a变量

根据不同的编程语言的语法，这种功能是可以做到的，而这种现象就叫做闭包

闭包简单来说，就是操作的不属于一个作用域范围的变量

如果使用local模式运行spark作业，那么实际只有一个jvm进程在执行这个作业

此时，你所有的RDD算子的代码执行以及它们操作的外部变量，都是在一个进程的内存中，这个进程就是driver进程
此时是没有任何问题的

但是在作业提交到集群执行的模式下（无论是client或cluster模式，作业都是在集群中运行的）

为了分布式并行执行你的作业，spark会将你的RDD算子操作，分散成多个task，放到集群中的多个节点上的executor进程中去执行

每个task执行的是相同的代码，但是却是处理不同的数据

在提交作业的task到集群去执行之前，spark会先在driver端处理闭包

spark中的闭包，特指那些，不在算子的作用域内部，但是在作用域外部却被算子处理和操作了的变量

而算子代码的执行也需要这些变量才能顺利执行

此时，这些闭包变量会被序列化成多个副本，然后每个副本都发送到各个executor进程中，供那个executor进程运行的task执行代码时使用

对于上面说的闭包变量处理机制

对于local模式，没有任何特别的影响，毕竟都在一个jvm进程中，变量发送到executor，也不过就是进程中的一个线程而已

但是对于集群运行模式来说，每个executor进程，都会得到一个闭包变量的副本，这个时候，就会出问题

因此闭包变量发送到executor进程中之后，就变成了一个一个独立的变量副本了，这就是最关键的一点

此时在executor进程中，执行task和算子代码时，访问的闭包变量，也仅仅只是当前executor进程中的一个变量副本而已了

此时虽然在driver进程中，也有一个变量副本，但是却完全跟各个executor进程中的变量副本不是一个东西

此时，各个executor进程对于自己内存中的变量副本进行操作，即使改变了变量副本的值，但是对于driver端的程序，是完全感知不到的

driver端的变量没有被进行任何操作

因此综上所述，在你使用集群模式运行作业的时候，切忌不要在算子内部，对作用域外面的闭包变量进行改变其值的操作

因为那没有任何意义，算子仅仅会在executor进程中，改变变量副本的值

对于driver端的变量没有任何影响，我们也获取不到executor端的变量副本的值

如果希望在集群模式下，对某个driver端的变量，进行分布式并行地全局性的修改

可以使用Spark提供的Accumulator，全局累加器

后面我们会讲解一个Accumulator的高级用法，自定义Accumulator，实现任意机制和算法的全局计算器



![](http://i2.51cto.com/images/blog/201810/04/c8a66d8e81d8fe588d1e422b80725d34.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



## Spark核心编程进阶-mapPartitions以及学生成绩查询案例

代码：

    MapPartitions.java



## Spark核心编程进阶-mapPartitionsWithIndex以开学分班案例


    MapPartitionsWithIndex.java



## Spark核心编程进阶-sample以及公司年会抽奖案例


代码：

    Sample.java


## Spark核心编程进阶-union以及公司部门合并案例

代码：

    Union.java


## Spark核心编程进阶-intersection以及公司跨多项目人员查询案例

代码：

    Intersection.java

## Spark核心编程进阶-distinct以及网站uv统计案例

代码：

    Distinct.java



## Spark核心编程进阶-aggregateByKey以及单词计数案例



![](http://i2.51cto.com/images/blog/201810/04/90835d740919f421cd46b20360b8261e.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)




代码：

    AggregateByKey.java


## Spark核心编程进阶-cartesian以及服装搭配案例

代码：

    Cartesian.java


## Spark核心编程进阶-coalesce以及公司部门整合案例

代码：

    Coalesce.java



## Spark核心编程进阶-repartition以及公司新增部门案例


代码：

    Repartition.java


## Spark核心编程进阶-takeSampled以及公司年会抽奖案例


代码：

    TakeSampled.java



## Spark核心编程进阶-shuffle操作原理详解

shuffle操作，是spark中一些特殊的算子操作会触发的一种操作
shuffle操作，会导致大量的数据在不同的机器和节点之间进行传输，因此也是spark中最复杂、最消耗性能的一种操作

我们可以通过reduceByKey操作作为一个例子，来理解shuffle操作

reduceByKey算子会将上一个RDD中的每个key对应的所有value都聚合成一个value，然后生成一个新的RDD

新的RDD的元素类型就是<key,value>对的格式，每个key对应一个聚合起来的value

这里最大的问题就在于，对于上一个RDD来说，并不是一个key对应的所有value都是在一个partition中的，也更不太可能说key的所有value都在一台机器上

所以对于这种情况来说，就必须在整个集群中，将各个节点上，同一个key对应的values，统一传输到一个节点上来聚合处理

这个过程中就会发生大量的网络数据的传输

在进行一个key对应的values的聚合时

首先，上一个stage的每个map task就必须保证将自己处理的当前分区中的数据，相同的key写入一个分区文件中，可能会写多个不同的分区文件

接着下一个stage的reduce task就必须从上一个stage所有task所在的机器上，将各个task写入的多个分区文件中，找到属于自己的那个分区文件

接着将属于自己的分区数据，拉取过来，这样就可以保证每个key对应的所有values都汇聚到一个节点上去处理和聚合
这个过程就称之为shuffle

shuffle是分为shuffle write和shuffle read两个部分的
是在两个不同的stage中进行的



![](http://i2.51cto.com/images/blog/201810/04/3992493bd121ef454221c72c42e23f07.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

通过key hash，决定要写入的文件
所以说，不同的map side task，一定保证一个key
是写入对应下游同一个task的分区文件的


reduce task，一定会将所有节点上，自己的那个分区文件的数据拉取过来



最终保证，集群中，相同的一个key，最后一定是到一个reduce task上面去进行处理




## Spark核心编程进阶-shuffle操作过程中进行数据排序

默认情况下，shuffle操作是不会对每个分区中的数据进行排序的

如果想要对每个分区中的数据进行排序，那么可以使用以下三种方法：

1、使用mapPartitions算子处理每个partition，对每个partition中的数据进行排序

2、使用repartitionAndSortWithinPartitions，对RDD进行重分区，在重分区的过程中同时就进行分区内数据的排序

3、使用sortByKey对数据进行全局排序

上述三种方法中，相对来说，mapPartitions的代价比较小，因为不需要进行额外的shuffle操作

repartitionAndSortWithinPartitions和sortByKey可能会进行额外的shuffle操作的，因此性能并不是很高
    
    val rdd2 = rdd1.reduceByKey(_ + _)
    
    rdd2.mapPartitions(tuples.sort)
    
    rdd2.repartitionAndSortWithinPartitions()，重分区，重分区的过程中，
    就进行分区内的key的排序，重分区的原理和repartition一样
    
    rdd2.sortByKey，直接对rdd按照key进行全局性的排序


## Spark核心编程进阶-会触发shuffle操作的算子


spark中会导致shuffle操作的有以下几种算子

1、repartition类的操作：比如repartition、repartitionAndSortWithinPartitions、coalesce等

2、byKey类的操作：比如reduceByKey、groupByKey、sortByKey等

3、join类的操作：比如join、cogroup等

重分区: 一般会shuffle，因为需要在整个集群中，对之前所有的分区的数据进行随机，均匀的打乱，然后把数据放入下游新的指定数量的分区内

byKey类的操作：因为你要对一个key，进行聚合操作，那么肯定要保证集群中，所有节点上的，相同的key，一定是到同一个节点上进行处理

join类的操作：两个rdd进行join，就必须将相同join key的数据，shuffle到同一个节点上，然后进行相同key的两个rdd数据的笛卡尔乘积

提醒一下

所以对于上述的操作

首先第一原则，就是，能不用shuffle操作，就尽量不用shuffle操作，尽量使用不shuffle的操作

第二原则，就是，如果使用了shuffle操作，那么肯定要进行shuffle的调优，甚至是解决碰到的数据倾斜的问题


## Spark核心编程进阶-shuffle操作对性能消耗的原理详解

![](http://i2.51cto.com/images/blog/201810/04/fbc6a36429146986b74c80cb5ba6a05f.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

shuffle操作是spark中唯一最最消耗性能的地方

因此也就成了最需要进行性能调优的地方，最需要解决线上报错的地方，也是唯一可能出现数据倾斜的地方

因为shuffle过程中，会产生大量的磁盘IO、数据序列化和反序列化、网络IO

为了实施shuffle操作

spark中才有了stage的概念，在发生shuffle操作的算子中，进行stage的拆分

shuffle操作的前半部分，是上一个stage来进行，也称之为map task，shuffle操作的后半部分，是下一个stage来进行，也称之为reduce task

其中map task负责数据的组织，也就是将同一个key对应的value都写入同一个下游task对应的分区文件中

其中reduce task负责数据的聚合，也就是将上一个stage的task所在节点上，将属于自己的各个分区文件，都拉取过来聚合

这种模型，是参考和模拟了MapReduce的shuffle过程来的

map task会将数据先保存在内存中，如果内存不够时，就溢写到磁盘文件中去

reduce task会读取各个节点上属于自己的分区磁盘文件，到自己节点的内存中，并进行聚合

shuffle操作会消耗大量的内存，因为无论是网络传输数据之前，还是之后，都会使用大量的内存中数据结构来实施聚合操作

比如reduceByKey和aggregateByKey操作，会在map side使用内存中的数据结构进行预先聚合

其他的byKey类的操作，都是在reduce side，使用内存数据结构进行聚合

在聚合过程中，如果内存不够，只能溢写到磁盘文件中去，此时就会发生大量的磁盘IO，降低性能

此外，shuffle过程中，还会产生大量的中间文件，也就是map side写入的大量分区文件

比如Spark 1.3版本，这些中间文件会一致保留着，直到RDD不再被使用，而且被垃圾回收掉了，才会去清理中间文件

这主要是为了，如果要重新计算shuffle后的RDD，那么map side不需要重新做一次磁盘写操作

但是这种情况下，如果我们的应用程序中，一直保持着对RDD的引用，导致很长时间以后才会进行RDD垃圾回收操作

保存中间文件的目录，由spark.local.dir属性指定

内存的消耗、磁盘IO、网络数据传输（IO）



## Spark核心编程进阶-shuffle操作所有相关参数详解以及性能调优

我们可以通过对一系列的参数进行调优，来优化shuffle的性能

spark 1.5.2版本

    属性名称                    默认值     属性说明
    spark.reducer.maxSizeInFlight         48m       reduce task的buffer缓冲，代表了每个reduce task每次能够拉取的map side数据最大大小，如果内存充足，可以考虑加大大小，从而减少网络传输次数，提升性能
    spark.shuffle.blockTransferService        netty     shuffle过程中，传输数据的方式，两种选项，netty或nio，spark 1.2开始，默认就是netty，比较简单而且性能较高，spark 1.5开始nio就是过期的了，而且spark 1.6中会去除掉
    spark.shuffle.compress              true      是否对map side输出的文件进行压缩，默认是启用压缩的，压缩器是由spark.io.compression.codec属性指定的，默认是snappy压缩器，该压缩器强调的是压缩速度，而不是压缩率
    spark.shuffle.consolidateFiles          false     默认为false，如果设置为true，那么就会合并map side输出文件，对于reduce task数量特别的情况下，可以极大减少磁盘IO开销，提升性能
    spark.shuffle.file.buffer           32k       map side task的内存buffer大小，写数据到磁盘文件之前，会先保存在缓冲中，如果内存充足，可以适当加大大小，从而减少map side磁盘IO次数，提升性能
    spark.shuffle.io.maxRetries           3       网络传输数据过程中，如果出现了网络IO异常，重试拉取数据的次数，默认是3次，对于耗时的shuffle操作，建议加大次数，以避免full gc或者网络不通常导致的数据拉取失败，进而导致task lost，增加shuffle操作的稳定性
    spark.shuffle.io.retryWait            5s        每次重试拉取数据的等待间隔，默认是5s，建议加大时长，理由同上，保证shuffle操作的稳定性
    spark.shuffle.io.numConnectionsPerPeer      1       机器之间的可以重用的网络连接，主要用于在大型集群中减小网络连接的建立开销，如果一个集群的机器并不多，可以考虑增加这个值
    spark.shuffle.io.preferDirectBufs       true      启用堆外内存，可以避免shuffle过程的频繁gc，如果堆外内存非常紧张，则可以考虑关闭这个选项
    spark.shuffle.manager             sort      ShuffleManager，Spark 1.5以后，有三种可选的，hash、sort和tungsten-sort，sort-based ShuffleManager会更高效实用内存，并且避免产生大量的map side磁盘文件，从Spark 1.2开始就是默认的选项，tungsten-sort与sort类似，但是内存性能更高
    spark.shuffle.memoryFraction          0.2       如果spark.shuffle.spill属性为true，那么该选项生效，代表了executor内存中，用于进行shuffle reduce side聚合的内存比例，默认是20%，如果内存充足，建议调高这个比例，给reduce聚合更多内存，避免内存不足频繁读写磁盘
    spark.shuffle.service.enabled         false     启用外部shuffle服务，这个服务会安全地保存shuffle过程中，executor写的磁盘文件，因此executor即使挂掉也不要紧，必须配合spark.dynamicAllocation.enabled属性设置为true，才能生效，而且外部shuffle服务必须进行安装和启动，才能启用这个属性
    spark.shuffle.service.port            7337      外部shuffle服务的端口号，具体解释同上
    spark.shuffle.sort.bypassMergeThreshold     200       对于sort-based ShuffleManager，如果没有进行map side聚合，而且reduce task数量少于这个值，那么就不会进行排序，如果你使用sort ShuffleManager，而且不需要排序，那么可以考虑将这个值加大，直到比你指定的所有task数量都打，以避免进行额外的sort，从而提升性能
    spark.shuffle.spill               true      当reduce side的聚合内存使用量超过了spark.shuffle.memoryFraction指定的比例时，就进行磁盘的溢写操作
    spark.shuffle.spill.compress          true      同上，进行磁盘溢写时，是否进行文件压缩，使用spark.io.compression.codec属性指定的压缩器，默认是snappy，速度优先




## Spark核心编程进阶-综合案例1：移动端app访问流量日志分析


如果你是在一个互联网公司，然后你的公司现在也在做移动互联网，做了一个手机app

那么你的手机app的用户，每次进行点击，或者是一些搜索操作的时候，都会跟你的远程的后端服务器做一次交互

也就是说，你的手机app，首先会往后端服务器发送一个请求，然后你的后端服务器会给你的手机app返回一个响应，响应的内容可能是图片、或者文字、或者json

此时，就完成了一次你的移动端app和后端服务器之间的交互过程

通常来说，在你的移动端app访问你的后端服务器的时候，你的后端服务器会记录一条日志

这个日志，也就是你的移动端app访问流量的相关日志，但是也可以根据你自己的需要，移动端发送一条日志过来，服务器端的web系统保存日志

我们这里做的就是最基本的，记录你的移动端app和服务器之间的上行数据包和下行数据包，上行流量和下行流量

我们要来计算，就是说，你的每个移动端，唯一的一个标识是你的deviceID

然后呢，每条日志，都会有这一次请求和响应的上行流量和下行流量的记录，这里呢，上行流量指的是手机app向服务器发送的请求数据的流量

下行流量，认为是服务器端给手机app返回的数据（比如说图片、文字、json）的流量

每个设备（deviceID），总上行流量和总下行流量，计算之后，要根据上行流量和下行流量进行排序，需要进行倒序排序

获取流量最大的前10个设备

难点：根据上行流量和下行流量进行排序的时候，不是简单的排序，优先根据上行流量进行排序，如果上行流量相等，那么根据下行流量排序
二次排序





## Spark核心编程进阶-综合案例1：日志文件格式分析

文件：

    access.log


## Spark核心编程进阶-综合案例1：读取日志文件并创建RDD

代码：
    
    AppLogSpark.java
    DataGenerator.java


## Spark核心编程进阶-综合案例1：创建自定义的可序列化类

代码：
    
    AccessLogInfo.java


## Spark核心编程进阶-综合案例1：将RDD映射为key-value格式

代码：
    
    AppLogSpark.java


## Spark核心编程进阶-综合案例1：基于deviceID进行聚合操作

代码：
        
    AppLogSpark.java
    
    
    
## Spark核心编程进阶-综合案例1：自定义二次排序key类

代码：
       
    AccessLogSortKey.java



## Spark核心编程进阶-综合案例1：将二次排序key映射为RDD的key
 
 代码：
     
     AppLogSpark.java
     
     

## Spark核心编程进阶-综合案例1：执行二次排序以及获取top10数据


 代码：
     
     AppLogSpark.java
     






































































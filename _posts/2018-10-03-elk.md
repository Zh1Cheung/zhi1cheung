---
title: elasticsearch高级篇(五)
categories:
- ELK
tags:
- elasticsearch


---



### elasticsearch性能调优之慢查询日志


es里面的操作，主要分为两种，一种写入（增删改），另一种是查询（搜索）

我们分别要识别出来，哪些写入操作性能比较慢，哪些查询操作性能比较慢，先要识别出来有性能问题的这些慢查询，慢写入，然后才能去考虑如何优化写入的性能，如何优化搜索的性能

搜索慢查询日志

无论是慢查询日志，还是慢写入日志，都是针对shard级别的，因为大家应该知道，无论你是执行增删改，还是执行搜索，都是对某个数据执行写入或者是搜索，其实都是到某个shard上面去执行的

shard上面执行的慢的写入或者是搜索，都会记录在针对这个shard的日志中

阈值的意思，就是说，什么叫做慢？搜索，5s叫做慢？还是10s叫做慢？或者是1s叫做慢？

比如说，你设置一个阈值，5s就是搜索的阈值，5s就叫做慢，那么一旦一个搜索请求超过了5s之后，就会记录一条慢搜索日志到日志文件中

shard level的搜索慢查询日志，辉将搜索性能较慢的查询写入一个专门的日志文件中。可以针对query phase和fetch phase单独设置慢查询的阈值，而具体的慢查询阈值设置如下所示：

在elasticsearch.yml中，设置

    index.search.slowlog.threshold.query.warn: 10s
    index.search.slowlog.threshold.query.info: 5s
    index.search.slowlog.threshold.query.debug: 2s
    index.search.slowlog.threshold.query.trace: 500ms
    
    index.search.slowlog.threshold.fetch.warn: 1s
    index.search.slowlog.threshold.fetch.info: 800ms
    index.search.slowlog.threshold.fetch.debug: 500ms
    index.search.slowlog.threshold.fetch.trace: 200ms

而慢查询日志具体的格式，都是在log4j2.properties中配置的，比如下面的配置：

    appender.index_search_slowlog_rolling.type = RollingFile
    appender.index_search_slowlog_rolling.name = index_search_slowlog_rolling
    appender.index_search_slowlog_rolling.fileName = ${sys:es.logs}_index_search_slowlog.log
    appender.index_search_slowlog_rolling.layout.type = PatternLayout
    appender.index_search_slowlog_rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %.10000m%n
    appender.index_search_slowlog_rolling.filePattern = ${sys:es.logs}_index_search_slowlog-%d{yyyy-MM-dd}.log
    appender.index_search_slowlog_rolling.policies.type = Policies
    appender.index_search_slowlog_rolling.policies.time.type = TimeBasedTriggeringPolicy
    appender.index_search_slowlog_rolling.policies.time.interval = 1
    appender.index_search_slowlog_rolling.policies.time.modulate = true
    
    logger.index_search_slowlog_rolling.name = index.search.slowlog
    logger.index_search_slowlog_rolling.level = trace
    logger.index_search_slowlog_rolling.appenderRef.index_search_slowlog_rolling.ref = index_search_slowlog_rolling
    logger.index_search_slowlog_rolling.additivity = false

索引慢写入日志

可以用如下的配置来设置索引写入慢日志的阈值：

    index.indexing.slowlog.threshold.index.warn: 10s
    index.indexing.slowlog.threshold.index.info: 5s
    index.indexing.slowlog.threshold.index.debug: 2s
    index.indexing.slowlog.threshold.index.trace: 500ms
    index.indexing.slowlog.level: info
    index.indexing.slowlog.source: 1000

用下面的log4j.properties配置就可以设置索引慢写入日志的格式：

    appender.index_indexing_slowlog_rolling.type = RollingFile
    appender.index_indexing_slowlog_rolling.name = index_indexing_slowlog_rolling
    appender.index_indexing_slowlog_rolling.fileName = ${sys:es.logs}_index_indexing_slowlog.log
    appender.index_indexing_slowlog_rolling.layout.type = PatternLayout
    appender.index_indexing_slowlog_rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %marker%.10000m%n
    appender.index_indexing_slowlog_rolling.filePattern = ${sys:es.logs}_index_indexing_slowlog-%d{yyyy-MM-dd}.log
    appender.index_indexing_slowlog_rolling.policies.type = Policies
    appender.index_indexing_slowlog_rolling.policies.time.type = TimeBasedTriggeringPolicy
    appender.index_indexing_slowlog_rolling.policies.time.interval = 1
    appender.index_indexing_slowlog_rolling.policies.time.modulate = true
    
    logger.index_indexing_slowlog.name = index.indexing.slowlog.index
    logger.index_indexing_slowlog.level = trace
    logger.index_indexing_slowlog.appenderRef.index_indexing_slowlog_rolling.ref = index_indexing_slowlog_rolling
    logger.index_indexing_slowlog.additivity = false

把这个东西记录好了以后，就可以什么呢？es集群运维管理员，就需要经常每天去看看，正常情况下，慢查询应该是比较少数的。所以比如你每天检查一次，如果发现某个查询特别慢，就要通知写这个查询的RD，开发人员，让他们去优化一下性能。


### elasticsearch性能调优之基本优化建议



对es的性能优化的一种常识性的知识，以后做es的时候，就是要按照这个最基本的规范来干

1、搜索结果不要返回过大的结果集

es是一个搜索引擎，所以如果用这个搜索引擎对大量的数据进行搜索，并且返回搜索结果中排在最前面的少数结果，是非常合适的。然而，如果要做成类似数据库的东西，每次都进行大批量的查询，是很不合适的。如果真的要做大批量结果的查询，记得考虑用scroll api。

2、避免超大的document

http.max_context_length的默认值是100mb，意味着你一次document写入时，document的内容不能超过100mb，否则es就会拒绝写入。也许你可以将这个参数设置的更大，从而让你的超大的documdent可以写入es，但是es底层的lucene引擎还是有一个2gb的最大限制。

即使我们不考虑引擎层的限制，超大的document在实际生产环境中是很不好的。超大document会耗费更多的网络资源，内存资源和磁盘资源，甚至对那些不要求获取_source的请求，也是一样，因为es需要从_source中提取_id字段，对于超大document这个获取_id字段的过程的资源开销也是很大的。而将这种超大document写入es也会使用大量的内存，占用内存空间的大小甚至会是documdent本身大小的数倍。近似匹配的搜索，比如phrase query，以及高亮显示，对超大document的资源开销会更大，因为这些操作的性能开销直接跟document的大小成正比。

因此对于超大document，我们需要考虑一下，我们到底需要其中的哪些部分。举例来说，如果我们要对一些书进行搜索，那么我们并不需要将整本书的内容就放入es中吧。我们可以仅仅使用每一篇章或者一个段落作为一个document，然后给一个field标识出来这些document属于哪本书，这样每个document的大小不就变小了么。这就可以避免超大document导致的各种开销，同时可以优化搜索的体验。比如说，如果一个用户要搜索两个单词，foo和bar，如果在两个不同的段落中分别匹配了一个单词，肯定匹配效果要比，一个段落中匹配了两个单词，要差。

3、避免稀疏的数据

lucene的内核结构，跟稠密的数据配合起来，性能会更好，举个例子，什么叫稀疏的数据，什么叫稠密的数据？比如有100个document，每个document都有20个field，20个field都有值，这就是稠密的数据。但是如果100个document，每个document的field都不一样，有的document有2个field，有的document有50个field，这就是稀疏的数据。

原因就是，lucene在内部会通过doc id来唯一标识一个document，这个doc id是integer类型，范围在0到索引中含有的document数量之间。这些doc id是用来在lucene内部的api之间进行通信的，比如说，对一个term用一个match query来进行搜索，就会产生一个doc id集合，然后这些doc id会用来获取对应的norm值，以用来计算每个doc的相关度分数。而根据doc id查找norm的过程，是通过每个document的每个field保留一个字节来进行的一个算法，这个过程叫做norm查找，norm就是每个document的每个field保留的一个字节。对于每个doc id对应的那个norm值，可以通过读取es一个内置索引，叫做doc_id的索引，中的一个字节来获取。这个过程是性能很高的，而且可以帮助lucene快速的定位到每个document的norm值，但是同时这样的话document本身就不需要存储这一个字节的norm值了。

在实际运行过程中，这就意味着，如果一个索引有100个document，对于每个field，就需要100个字节来存储norm值，即使100个document中只有10个document含有某个field，但是对那个field来说，还是要100个字节来存储norm值。这就会对存储产生更大的开销，存储空间被浪费的一个问题，而且也会影响读写性能。

下面有一些避免稀疏数据的办法：

（1）避免将没有任何关联性的数据写入同一个索引

我们必须避免将结构完全不一样的数据写入同一个索引中，因为结构完全不一样的数据，field是完全不一样的，会导致index数据非常稀疏。最好将这种数据写入不同的索引中，如果这种索引数据量比较少，那么可以考虑给其很少的primary shard，比如1个，避免资源浪费。

（2）对document的结构进行规范化/标准化

即使我们真的要将不同类型的document写入相同的索引中，还是有办法可以避免稀疏性，那就是对不同类型的document进行标准化。比如说，如果所有的document都有一个时间戳field，不过有的叫做timestamp，有的叫做creation_date，那么可以将不同document的这个field重命名为相同的字段，尽量让documment的结构相同。另外一个，就是比如有的document有一个字段，叫做goods_type，但是有的document没有这个字段，此时可以对没有这个字段的document，补充一个goods_type给一个默认值，比如default。

（3）避免使用多个types存储不一样结构的document

很多人会很喜欢在一个index中放很多个types来存储不同类型的数据。但是其实不是这样的，最好不要这么干，如果你在一个index中有多个type，但是这些type的数据结构不太一样，那么这些type实际上底层都是写到这个索引中的，还是会导致稀疏性。如果多个type的结构不太一样，最好放入不同的索引中，不要写入一个索引中。

（4）对稀疏的field禁用norms和doc_values

如果上面的步骤都没法做，那么只能对那种稀疏的field，禁止norms和doc_values字段，因为这两个字段的存储机制类似，都是每个field有一个全量的存储，对存储浪费很大。如果一个field不需要考虑其相关度分数，那么可以禁用norms，如果不需要对一个field进行排序或者聚合，那么可以禁用doc_values字段。

### elasticsearch性能调优之索引写入性能优化


1和2，适合的是，你的es java client程序，可以采取批量写的场景
3，比较通用的，比较合适的是，你对于写入数据到可以读到能够接受比较大的延迟
4，一次性批量导入数据的场景
5/6/7/8/9，通用型，尽量都去做到

1/2/3/4，都是有各自适用的场景，如果场景合适，就尽量用，因为对性能的提升都是很明显的

5/6/7/8/9，其中通用，尽量去优化你的集群，但是其中最重要的，就是3块，filesystem cache更大的内存，给index buffer最充足的内存，一个是用SSD固态硬盘

1、用bulk批量写入

你如果要往es里面灌入数据的话，那么根据你的业务场景来，如果你的业务场景可以支持，可以做到，让你将一批数据聚合起来，一次性写入es，那么就尽量采用bulk的方式，每次批量写个几百条这样子。

bulk批量写入的性能比你一条一条写入大量的document的性能要好很多。但是如果要知道一个bulk请求最佳的大小，需要对单个es node的单个shard做压测。先bulk写入100个document，然后200个，400个，以此类推，每次都将bulk size加倍一次。如果bulk写入性能开始变平缓的时候，那么这个就是最佳的bulk大小。并不是bulk size越大越好，而是根据你的集群等环境具体要测试出来的，因为越大的bulk size会导致内存压力过大，因此最好一个请求不要发送超过10mb的数据量。

之前有es学员就是在公司里测试这个bulk写入，上来就是多线程并发写bulk，但是这里面就有一个问题，刚开始，你先确定一个是bulk size，此时就尽量是用你的程序，单线程，一个es node，一个shard，测试。看看单线程最多一次性写多少条数据，性能是比较好的。

2、使用多线程将数据写入es

单线程发送bulk请求是无法最大化es集群写入的吞吐量的。如果要利用集群的所有资源，就需要使用多线程并发将数据bulk写入集群中。为了更好的利用集群的资源，这样多线程并发写入，可以减少每次底层磁盘fsync的次数和开销。一样，可以对单个es节点的单个shard做压测，比如说，先是2个线程，然后是4个线程，然后是8个线程，16个，每次线程数量倍增。一旦发现es返回了TOO_MANY_REQUESTS的错误，JavaClient也就是EsRejectedExecutionException，之前有学员就是做多线程的bulk写入的时候，就发生了。此时那么就说明es是说已经到了一个并发写入的最大瓶颈了，此时我们就知道最多只能支撑这么高的并发写入了。

3、增加refresh间隔

默认的refresh间隔是1s，用index.refresh_interval参数可以设置，这样会其强迫es每秒中都将内存中的数据写入磁盘中，创建一个新的segment file。正是这个间隔，让我们每次写入数据后，1s以后才能看到。但是如果我们将这个间隔调大，比如30s，可以接受写入的数据30s后才看到，那么我们就可以获取更大的写入吞吐量，因为30s内都是写内存的，每隔30s才会创建一个segment file。

4、禁止refresh和replia

如果我们要一次性加载大批量的数据进es，可以先禁止refresh和replia复制，将index.refresh_interval设置为-1，将index.number_of_replicas设置为0即可。这可能会导致我们的数据丢失，因为没有refresh和replica机制了。但是不需要创建segment file，也不需要将数据replica复制到其他的replica shasrd上面去。此时写入的速度会非常快，一旦写完之后，可以将refresh和replica修改回正常的状态。

5、禁止swapping交换内存

之前讲解果，可以将swapping禁止掉，有的时候，如果要将es jvm内存交换到磁盘，再交换回内存，大量磁盘IO，性能很差

6、给filesystem cache更多的内存

filesystem cache被用来执行更多的IO操作，如果我们能给filesystem cache更多的内存资源，那么es的写入性能会好很多。

7、使用自动生成的id

如果我们要手动给es document设置一个id，那么es需要每次都去确认一下那个id是否存在，这个过程是比较耗费时间的。如果我们使用自动生成的id，那么es就可以跳过这个步骤，写入性能会更好。对于你的业务中的表id，可以作为es document的一个field。

8、用性能更好的硬件

我们可以给filesystem cache更多的内存，也可以使用SSD替代机械硬盘，避免使用NAS等网络存储，考虑使用RAID 0来条带化存储提升磁盘并行读写效率，等等。

9、index buffer

如果我们要进行非常重的高并发写入操作，那么最好将index buffer调大一些，indices.memory.index_buffer_size，这个可以调节大一些，设置的这个index buffer大小，是所有的shard公用的，但是如果除以shard数量以后，算出来平均每个shard可以使用的内存大小，一般建议，但是对于每个shard来说，最多给512mb，因为再大性能就没什么提升了。es会将这个设置作为每个shard共享的index buffer，那些特别活跃的shard会更多的使用这个buffer。默认这个参数的值是10%，也就是jvm heap的10%，如果我们给jvm heap分配10gb内存，那么这个index buffer就有1gb，对于两个shard共享来说，是足够的了。





### elasticsearch性能调优之搜索性能优化


magic，如果真的要优化搜索性能的话，就是以下几种办法

1/5，配合起来，就是搜索性能优化的杀手锏

3/4，配合起来，解决各种复杂的搜索需求的性能

1、给filesysgtem cache更多的内存

es的搜索引擎严重依赖于底层的filesystem cache，你如果给filesystem cache更多的内存，尽量让内存可以容纳所有的indx segment file索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。

之前有个学员，一直在问我，说他的搜索性能，聚合性能，倒排索引，正排索引，磁盘文件，十几秒。。。。

比如说，你，es节点有3台机器，每台机器，看起来内存很多，64G，总内存，64 * 3

每台机器给es jvm heap是32G，那么剩下来留给filesystem cache的就是每台机器才32g，总共集群里给filesystem cache的就是32 * 3 = 96gb内存

如果你此时，你整个，磁盘上索引数据文件，在3台机器上，一共占用了1T的磁盘容量，你的es数据量是1t

你觉得你的性能能好吗？filesystem cache的内存才100g，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差

归根结底，你要让es性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半

比如说，你一共要在es中存储1T的数据，那么你的多台机器留个filesystem cache的内存加起来综合，至少要到512G，至少半数的情况下，搜索是走内存的，性能一般可以到几秒钟，2秒，3秒，5秒

如果最佳的情况下，我们自己的生产环境实践经验，最好是用es就存少量的数据，就是你要用来搜索的那些索引，内存留给filesystem cache的，就100G，那么你就控制在100gb以内，相当于是，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在1秒以内

所以之前有些学员也是问，我也是跟他们说，尽量在es里，就存储必须用来搜索的数据，比如说你现在有一份数据，有100个字段，其实用来搜索的只有10个字段，建议是将10个字段的数据，存入es，剩下90个字段的数据，可以放mysql，hadoop hbase，都可以

这样的话，es数据量很少，10个字段的数据，都可以放内存，就用来搜索，搜索出来一些id，通过id去mysql，hbase里面去查询明细的数据

2、用更快的硬件资源

（1）给filesystem cache更多的内存资源

（2）用SSD固态硬盘

（3）使用本地存储系统，不要用NFS等网络存储系统

（4）给更多的CPU资源

3、document模型设计

document模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es能支持的操作就是那么多，不要考虑用es做一些它不好操作的事情。如果真的有那种操作，尽量在document模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如join，nested，parent-child搜索都要尽量避免，性能都很差的。

很多同学在问我，很多复杂的乱七八糟的一些操作，如何执行

两个思路，在搜索/查询的时候，要执行一些业务强相关的特别复杂的操作：

（1）在写入数据的时候，就设计好模型，加几个字段，把处理好的数据写入加的字段里面
（2）自己用java程序封装，es能做的，用es来做，搜索出来的数据，在java程序里面去做，比如说我们，基于es，用java封装一些特别复杂的操作

4、预先index data

为了性能，提前优化data index时的数据模型，比如说document有一个price field，然后大多数查询都对一个固定的范围，对这个field使用range范围查询，那么可以提前将这个price的范围处理出来，写入一个字段中。比如下面这样：
    
    PUT index/type/1
    {
      "designation": "spoon",
      "price": 13
    }
    
    GET index/_search
    {
      "aggs": {
        "price_ranges": {
          "range": {
            "field": "price",
            "ranges": [
              { "to": 10 },
              { "from": 10, "to": 100 },
              { "from": 100 }
            ]
          }
        }
      }
    }

我们完全可以增加一个price_range字段：
    
    PUT index
    {
      "mappings": {
        "type": {
          "properties": {
            "price_range": {
              "type": "keyword"
            }
          }
        }
      }
    }

然后写入的时候，直接计算出来这个range：

    PUT index/type/1
    {
      "designation": "spoon",
      "price": 13,
      "price_range": "10-100"
    }

然后搜索的时候，就可以直接用term查询了，性能非常高：

    GET index/_search
    {
      "aggs": {
        "price_ranges": {
          "terms": {
            "field": "price_range"
          }
        }
      }
    }

5、预热filesystem cache

如果我们重启了es，那么filesystem cache是空壳的，就需要不断的查询才能重新让filesystem cache热起来，我们可以先说动对一些数据进行查询。

比如说，你本来一个查询，要用户点击以后才执行，才能从磁盘加载到filesystem cache里，第一次执行要10s，以后每次就几百毫秒

你完全可以，自己早上的时候，就程序执行那个查询，预热，数据就加载到filesystem cahce，程序执行的时候是10s，以后用户真的来看的时候就才几百毫秒

6、避免使用script脚本

说实话，一般是避免使用es script的，实际生产中更是少用，性能不高，尽量不要使用

7、使用固定范围的日期查询

尽量不要使用now这种内置函数来执行日期查询，因为默认now是到毫秒级的，是无法缓存结果，尽量使用一个阶段范围，比如now/m，就是到分钟级

那么如果一分钟内，都执行这个查询，是可以取用查询缓存的

    PUT index/type/1
    {
      "my_date": "2016-05-11T16:30:55.328Z"
    }
    
    GET index/_search
    {
      "query": {
        "constant_score": {
          "filter": {
            "range": {
              "my_date": {
                "gte": "now-1h",
                "lte": "now"
              }
            }
          }
        }
      }
    }

完全可以替换为：

    GET index/_search
    {
      "query": {
        "constant_score": {
          "filter": {
            "range": {
              "my_date": {
                "gte": "now-1h/m",
                "lte": "now/m"
              }
            }
          }
        }
      }
    }


### elasticsearch性能调优之磁盘读写性能优化



优化磁盘空间的占用，减少磁盘空间的占用，更多的数据可以进入filesystem cache

比如说你原来，磁盘空间占用一共是1T，内存只有512G，现在优化了磁盘空间占用之后，减少了数据量，可能数据量就只有512G了，那么就可以全部进入内存

1、禁用不需要的功能

聚合，搜索，评分，近似匹配

聚合：doc values

搜索：倒排索引，index

评分：norms

近似匹配：index_options（freqs）

任何一个功能不需要，就把对应的存储的数据给干掉，这样可以节约磁盘空间的占用，也可以优化磁盘的读写性能

默认情况下，es在写入document到索引的时候，都会给大多数的field增加一份doc values，就是正排索引，用来进行聚合或者排序的。比如说，如果我们有一个叫做foo的数字类型field，我们要对这个字段运行histograms aggr聚合操作，但是可能我们并不需要对这个字段进行搜索，那么就可以禁止为这个字段生成倒排索引，只需要doc value正排索引即可。禁用倒排索引：
    
    PUT index
    {
      "mappings": {
        "type": {
          "properties": {
            "foo": {
              "type": "integer",
              "index": false
            }
          }
        }
      }
    }

text类型的field会存储norm值，用来计算doc的相关度分数，如果我们需要对一个text field进行搜索，但是不关心这个field的分数，那么可以禁用norm值

    PUT index
    {
      "mappings": {
        "type": {
          "properties": {
            "foo": {
              "type": "text",
              "norms": false
            }
          }
        }
      }
    }

text field还会存储出现频率以及位置，出现频率也是用来计算相关度分数的，位置是用来进行phrase query这种近似匹配操作的，如果我们不需要执行phrase query近似匹配，那么可以禁用位置这个属性：

    PUT index
    {
      "mappings": {
        "type": {
          "properties": {
            "foo": {
              "type": "text",
              "index_options": "freqs"
            }
          }
        }
      }
    }

此外，如果我们不关心相关度频分，我们可以配置es仅仅为每个term索引对应的document，我们可以对这个field进行搜索，但是phrase query这种近似匹配会报错，而且相关度评分会不准确：

    PUT index
    {
      "mappings": {
        "type": {
          "properties": {
            "foo": {
              "type": "text",
              "norms": false,
              "index_options": "freqs"
            }
          }
        }
      }
    }

2、不要用默认的动态string类型映射

默认的动态string类型映射会将string类型的field同时映射为text类型以及keyword类型，这会浪费磁盘空间，因为我们不一定两种都需要。通常来说，id field这种字段可能只需要keyword映射，而body field可能只需要text field。

映射一个content，content: text，content.内置字段: keyword

可以通过手动设置mappings映射来避免字符串类型的field被自动映射为text和keyword：

    PUT index
    {
      "mappings": {
        "type": {
          "dynamic_templates": [
            {
              "strings": {
                "match_mapping_type": "string",
                "mapping": {
                  "type": "keyword"
                }
              }
            }
          ]
        }
      }
    }

3、禁止_all field

_all field会将document中所有field的值都合并在一起进行索引，很耗费空空间，如果不需要一次性对所有的field都进行搜索，那么最好禁用_all field。

4、使用best_compression

_source field和其他field都很耗费磁盘空间，最好是对其使用best_compression进行压缩。用elasticsearch.yml中的index.codec来设置，将其设置为best_compression即可。

5、用最小的最合适的数字类型

es支持4种数字类型，byte，short，integer，long。如果最小的类型就合适，那么就用最小的类型。



### es生产集群监控之基于cat API进行监控



es集群监控，最好是自己干吧，因为官方出了那种非常棒的x-pack做权限认证，监控，等等，做的都非常好，但是。。。是收费的。。。

自己做es集群监控，就是根据es的一些api，自己写一个java web的应用，自己做前端界面，程序里不断的每隔几秒钟，调用一次后端的接口，获取到各种监控信息，然后用前端页面显示出来，开发开发一个可视化的es集群的监控的工作台

在es老版本，有一个很好用的插件，叫做head，但是5.x之后都收口了，不让做这种插件了，主推自己的x-pack，要收费，要盈利，赚钱

1、GET /_cat/aliases?v

看到集群中有哪些索引别名

    alias  index filter routing.index routing.search
    alias1 test1 -      -            -
    alias2 test1 *      -            -
    alias3 test1 -      1            1
    alias4 test1 -      2            1,2

2、GET /_cat/allocation?v

    看到每个节点分配了几个shard，对磁盘的占用空间大小，使用率，等等
    
    shards disk.indices disk.used disk.avail disk.total disk.percent host      ip        node
         5         260b    47.3gb     43.4gb    100.7gb           46 127.0.0.1 127.0.0.1 CSUXak2
    	 
3、GET /_cat/count?v

    看每个索引的document数量
    
    epoch      timestamp count
    1475868259 15:24:20  120

4、GET /_cat/fielddata?v

看每个节点的jvm heap内存中的fielddata内存占用情况（对分词的field进行聚合/排序要用jvm heap中的正排索引，fielddata）
    
    id                     host      ip        node    field   size
    Nqk-6inXQq-OxUfOUI8jNQ 127.0.0.1 127.0.0.1 Nqk-6in body    544b
    Nqk-6inXQq-OxUfOUI8jNQ 127.0.0.1 127.0.0.1 Nqk-6in soul    480b

5、GET /_cat/health?v

比较全面的看一个es集群的整体健康状况，主要是看是green，yellow，red

    epoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
    1475871424 16:17:04  elasticsearch green           1         1      5   5    0    0        0             0                  -                100.0%

6、GET /_cat/indices?v

每个索引的具体的情况，比如有几个shard，多少个document，被删除的document有多少，占用了多少磁盘空间

    health status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
    yellow open   twitter  u8FNjxh8Rfy_awN11oDKYQ   1   1       1200            0     88.1kb         88.1kb

7、GET /_cat/master?v

看master node当前的具体的情况，哪个node是当前的master node

    id                     host      ip        node
    YzWoH_2BT-6UjVGDyPdqYg 127.0.0.1 127.0.0.1 YzWoH_2

8、GET /_cat/nodes?v

看每个node的具体的情况，就比如jvm heap内存使用率，内存使用率，cpu load，是什么角色

    ip        heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
    127.0.0.1           65          99  42    3.07                  mdi       *      mJw06l1

9、GET /_cat/pending_tasks?v

看当前pending没执行完的task的具体情况，执行的是什么操作

    insertOrder timeInQueue priority source
           1685       855ms HIGH     update-mapping [foo][t]
           1686       843ms HIGH     update-mapping [foo][t]
           1693       753ms HIGH     refresh-mapping [foo][[t]]
           1688       816ms HIGH     update-mapping [foo][t]
           1689       802ms HIGH     update-mapping [foo][t]
           1690       787ms HIGH     update-mapping [foo][t]
           1691       773ms HIGH     update-mapping [foo][t]
	   
10、GET /_cat/plugins?v&s=component&h=name,component,version,description

看当前集群安装了哪些插件

    name    component               version   description
    U7321H6 analysis-icu            5.5.1 The ICU Analysis plugin integrates Lucene ICU module into elasticsearch, adding ICU relates analysis components.
    U7321H6 analysis-kuromoji       5.5.1 The Japanese (kuromoji) Analysis plugin integrates Lucene kuromoji analysis module into elasticsearch.
    U7321H6 analysis-phonetic       5.5.1 The Phonetic Analysis plugin integrates phonetic token filter analysis with elasticsearch.
    U7321H6 analysis-smartcn        5.5.1 Smart Chinese Analysis plugin integrates Lucene Smart Chinese analysis module into elasticsearch.
    U7321H6 analysis-stempel        5.5.1 The Stempel (Polish) Analysis plugin integrates Lucene stempel (polish) analysis module into elasticsearch.
    U7321H6 analysis-ukrainian        5.5.1 The Ukrainian Analysis plugin integrates the Lucene UkrainianMorfologikAnalyzer into elasticsearch.
    U7321H6 discovery-azure-classic 5.5.1 The Azure Classic Discovery plugin allows to use Azure Classic API for the unicast discovery mechanism
    U7321H6 discovery-ec2           5.5.1 The EC2 discovery plugin allows to use AWS API for the unicast discovery mechanism.
    U7321H6 discovery-file          5.5.1 Discovery file plugin enables unicast discovery from hosts stored in a file.
    U7321H6 discovery-gce           5.5.1 The Google Compute Engine (GCE) Discovery plugin allows to use GCE API for the unicast discovery mechanism.
    U7321H6 ingest-attachment       5.5.1 Ingest processor that uses Apache Tika to extract contents
    U7321H6 ingest-geoip            5.5.1 Ingest processor that uses looksup geo data based on ip adresses using the Maxmind geo database
    U7321H6 ingest-user-agent       5.5.1 Ingest processor that extracts information from a user agent
    U7321H6 jvm-example             5.5.1 Demonstrates all the pluggable Java entry points in Elasticsearch
    U7321H6 lang-javascript         5.5.1 The JavaScript language plugin allows to have javascript as the language of scripts to execute.
    U7321H6 lang-python             5.5.1 The Python language plugin allows to have python as the language of scripts to execute.
    U7321H6 mapper-attachments      5.5.1 The mapper attachments plugin adds the attachment type to Elasticsearch using Apache Tika.
    U7321H6 mapper-murmur3          5.5.1 The Mapper Murmur3 plugin allows to compute hashes of a field's values at index-time and to store them in the index.
    U7321H6 mapper-size             5.5.1 The Mapper Size plugin allows document to record their uncompressed size at index time.
    U7321H6 store-smb               5.5.1 The Store SMB plugin adds support for SMB stores.

11、GET _cat/recovery?v

看shard recovery恢复的一个过程的具体情况
    
    index   shard time type  stage source_host source_node target_host target_node repository snapshot files files_recovered files_percent files_total bytes bytes_recovered bytes_percent bytes_total translog_ops translog_ops_recovered translog_ops_percent
    
    twitter 0     13ms store done  n/a         n/a         node0       node-0      n/a        n/a      0     0               100%          13          0     0               100%          9928        0            0                      100.0%

12、GET /_cat/repositories?v

查看用于snapshotting的repository有哪些
    
    id    type
    repo1   fs
    repo2   s3

13、GET /_cat/thread_pool?v

看每个线程池的具体的情况
    
    Z6MkIvC bulk                0 0 0
    Z6MkIvC fetch_shard_started 0 0 0
    Z6MkIvC fetch_shard_store   0 0 0
    Z6MkIvC flush               0 0 0
    Z6MkIvC force_merge         0 0 0
    Z6MkIvC generic             0 0 0
    Z6MkIvC get                 0 0 0
    Z6MkIvC index               0 0 0
    Z6MkIvC listener            0 0 0
    Z6MkIvC management          1 0 0
    Z6MkIvC refresh             0 0 0
    Z6MkIvC search              0 0 0
    Z6MkIvC snapshot            0 0 0
    Z6MkIvC warmer              0 0 0

14、GET _cat/shards?v

看每个shard的具体的情况
    
    twitter 0 p STARTED 3014 31.1mb 192.168.56.10 H5dfFeA
    twitter 0 r UNASSIGNED

15、GET /_cat/segments?v

看每个segement，索引segment文件的情况，在哪个node上，有多少个document，占用了多少磁盘空间，有多少数据在内存中，是否可以搜索

    index shard prirep ip        segment generation docs.count docs.deleted size size.memory committed searchable version compound
    test  3     p      127.0.0.1 _0               0          1            0  3kb        2042 false     true       6.5.1   true
    test1 3     p      127.0.0.1 _0               0          1            0  3kb        2042 false     true       6.5.1   true

16、GET /_cat/snapshots?v&s=id

看当前执行的snapshot的操作

    id     status start_epoch start_time end_epoch  end_time duration indices successful_shards failed_shards total_shards
    snap1  FAILED 1445616705  18:11:45   1445616978 18:16:18     4.6m       1                 4             1            5
    snap2 SUCCESS 1445634298  23:04:58   1445634672 23:11:12     6.2m       2                10             0           10

17、GET /_cat/templates?v&s=name

看当前有的那些tempalte，具体的情况是什么

    name      template order version
    template0 te*      0
    template1 tea*     1
    template2 teak*    2     7



### es生产集群监控之基于cluster API进行监控



1、GET _cluster/health
    
    {
      "cluster_name" : "testcluster",
      "status" : "yellow",
      "timed_out" : false,
      "number_of_nodes" : 1,
      "number_of_data_nodes" : 1,
      "active_primary_shards" : 5,
      "active_shards" : 5,
      "relocating_shards" : 0,
      "initializing_shards" : 0,
      "unassigned_shards" : 5,
      "delayed_unassigned_shards": 0,
      "number_of_pending_tasks" : 0,
      "number_of_in_flight_fetch": 0,
      "task_max_waiting_in_queue_millis": 0,
      "active_shards_percent_as_number": 50.0
    }
    
2、GET _cluster/stats?human&pretty
    
    {
       "timestamp": 1459427693515,
       "cluster_name": "elasticsearch",
       "status": "green",
       "indices": {
          "count": 2,
          "shards": {
             "total": 10,
             "primaries": 10,
             "replication": 0,
             "index": {
                "shards": {
                   "min": 5,
                   "max": 5,
                   "avg": 5
                },
                "primaries": {
                   "min": 5,
                   "max": 5,
                   "avg": 5
                },
                "replication": {
                   "min": 0,
                   "max": 0,
                   "avg": 0
                }
             }
          },
          "docs": {
             "count": 10,
             "deleted": 0
          },
          "store": {
             "size": "16.2kb",
             "size_in_bytes": 16684,
             "throttle_time": "0s",
             "throttle_time_in_millis": 0
          },
          "fielddata": {
             "memory_size": "0b",
             "memory_size_in_bytes": 0,
             "evictions": 0
          },
          "query_cache": {
             "memory_size": "0b",
             "memory_size_in_bytes": 0,
             "total_count": 0,
             "hit_count": 0,
             "miss_count": 0,
             "cache_size": 0,
             "cache_count": 0,
             "evictions": 0
          },
          "completion": {
             "size": "0b",
             "size_in_bytes": 0
          },
          "segments": {
             "count": 4,
             "memory": "8.6kb",
             "memory_in_bytes": 8898,
             "terms_memory": "6.3kb",
             "terms_memory_in_bytes": 6522,
             "stored_fields_memory": "1.2kb",
             "stored_fields_memory_in_bytes": 1248,
             "term_vectors_memory": "0b",
             "term_vectors_memory_in_bytes": 0,
             "norms_memory": "384b",
             "norms_memory_in_bytes": 384,
             "doc_values_memory": "744b",
             "doc_values_memory_in_bytes": 744,
             "index_writer_memory": "0b",
             "index_writer_memory_in_bytes": 0,
             "version_map_memory": "0b",
             "version_map_memory_in_bytes": 0,
             "fixed_bit_set": "0b",
             "fixed_bit_set_memory_in_bytes": 0,
             "file_sizes": {}
          },
          "percolator": {
             "num_queries": 0
          }
       },
       "nodes": {
          "count": {
             "total": 1,
             "data": 1,
             "coordinating_only": 0,
             "master": 1,
             "ingest": 1
          },
          "versions": [
             "5.5.1"
          ],
          "os": {
             "available_processors": 8,
             "allocated_processors": 8,
             "names": [
                {
                   "name": "Mac OS X",
                   "count": 1
                }
             ],
             "mem" : {
                "total" : "16gb",
                "total_in_bytes" : 17179869184,
                "free" : "78.1mb",
                "free_in_bytes" : 81960960,
                "used" : "15.9gb",
                "used_in_bytes" : 17097908224,
                "free_percent" : 0,
                "used_percent" : 100
             }
          },
          "process": {
             "cpu": {
                "percent": 9
             },
             "open_file_descriptors": {
                "min": 268,
                "max": 268,
                "avg": 268
             }
          },
          "jvm": {
             "max_uptime": "13.7s",
             "max_uptime_in_millis": 13737,
             "versions": [
                {
                   "version": "1.8.0_74",
                   "vm_name": "Java HotSpot(TM) 64-Bit Server VM",
                   "vm_version": "25.74-b02",
                   "vm_vendor": "Oracle Corporation",
                   "count": 1
                }
             ],
             "mem": {
                "heap_used": "57.5mb",
                "heap_used_in_bytes": 60312664,
                "heap_max": "989.8mb",
                "heap_max_in_bytes": 1037959168
             },
             "threads": 90
          },
          "fs": {
             "total": "200.6gb",
             "total_in_bytes": 215429193728,
             "free": "32.6gb",
             "free_in_bytes": 35064553472,
             "available": "32.4gb",
             "available_in_bytes": 34802409472
          },
          "plugins": [
            {
              "name": "analysis-icu",
              "version": "5.5.1",
              "description": "The ICU Analysis plugin integrates Lucene ICU module into elasticsearch, adding ICU relates analysis components.",
              "classname": "org.elasticsearch.plugin.analysis.icu.AnalysisICUPlugin",
              "has_native_controller": false
            },
            {
              "name": "ingest-geoip",
              "version": "5.5.1",
              "description": "Ingest processor that uses looksup geo data based on ip adresses using the Maxmind geo database",
              "classname": "org.elasticsearch.ingest.geoip.IngestGeoIpPlugin",
              "has_native_controller": false
            },
            {
              "name": "ingest-user-agent",
              "version": "5.5.1",
              "description": "Ingest processor that extracts information from a user agent",
              "classname": "org.elasticsearch.ingest.useragent.IngestUserAgentPlugin",
              "has_native_controller": false
            }
          ]
       }
    }

3、GET _cluster/pending_tasks
    
    {
       "tasks": [
          {
             "insert_order": 101,
             "priority": "URGENT",
             "source": "create-index [foo_9], cause [api]",
             "time_in_queue_millis": 86,
             "time_in_queue": "86ms"
          },
          {
             "insert_order": 46,
             "priority": "HIGH",
             "source": "shard-started ([foo_2][1], node[tMTocMvQQgGCkj7QDHl3OA], [P], s[INITIALIZING]), reason [after recovery from shard_store]",
             "time_in_queue_millis": 842,
             "time_in_queue": "842ms"
          },
          {
             "insert_order": 45,
             "priority": "HIGH",
             "source": "shard-started ([foo_2][0], node[tMTocMvQQgGCkj7QDHl3OA], [P], s[INITIALIZING]), reason [after recovery from shard_store]",
             "time_in_queue_millis": 858,
             "time_in_queue": "858ms"
          }
      ]
    }








